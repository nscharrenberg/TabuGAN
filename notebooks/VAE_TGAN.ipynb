{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1FGcBJtfwDs",
        "outputId": "ce4a60cd-dea6-431b-c9e1-e793fece3e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Categorical:  ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'income']\n",
            "Continuous:  ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "X = pd.read_csv(\"censusData.csv\")\n",
        "drive.mount('/content/drive')\n",
        "categorical_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country','income']\n",
        "print(\"Categorical: \",categorical_columns)\n",
        "continuous_columns = [col for col in X.columns if col not in categorical_columns]\n",
        "print(\"Continuous: \",continuous_columns)\n",
        "# Precompute an encoder to handle later conversions efficiently\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "X_encoded_cat = encoder.fit_transform(X[categorical_columns])\n",
        "scaler = StandardScaler()\n",
        "X_continuous = scaler.fit_transform(X[continuous_columns].values)\n",
        "X_encoded = torch.cat((torch.tensor(X_encoded_cat),torch.tensor(X_continuous)),dim=1)\n",
        "dataset = list(X_encoded)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OPXETRV3bnOp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RadialBasisFunction(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        grid_min: float = -1.5,\n",
        "        grid_max: float = 1.5,\n",
        "        num_grids: int = 8,\n",
        "        denominator: float = None,  # larger denominators lead to smoother basis\n",
        "    ):\n",
        "        super().__init__()\n",
        "        grid = torch.linspace(grid_min, grid_max, num_grids)\n",
        "        self.grid = torch.nn.Parameter(grid, requires_grad=False)\n",
        "        self.denominator = denominator or (grid_max - grid_min) / (num_grids - 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.exp(-((x[..., None] - self.grid) / self.denominator) ** 2)\n",
        "\n",
        "\n",
        "class BSRBF_KANLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        grid_size = 5,\n",
        "        spline_order = 3,\n",
        "        base_activation = torch.nn.SiLU,\n",
        "        grid_range=[-1.5, 1.5],\n",
        "\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.layernorm = nn.LayerNorm(input_dim)\n",
        "        self.spline_order = spline_order\n",
        "        self.grid_size = grid_size\n",
        "        self.output_dim = output_dim\n",
        "        self.base_activation = base_activation()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.base_weight = torch.nn.Parameter(torch.Tensor(self.output_dim, self.input_dim))\n",
        "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5))\n",
        "\n",
        "        self.spline_weight = torch.nn.Parameter(torch.Tensor(self.output_dim, self.input_dim*(grid_size+spline_order)))\n",
        "        torch.nn.init.kaiming_uniform_(self.spline_weight, a=math.sqrt(5))\n",
        "\n",
        "        self.rbf = RadialBasisFunction(grid_range[0], grid_range[1], grid_size+spline_order)\n",
        "\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size # 0.45, 0.5\n",
        "        grid = (\n",
        "            (\n",
        "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
        "                + grid_range[0]\n",
        "            )\n",
        "            .expand(self.input_dim, -1)\n",
        "            .contiguous()\n",
        "        )\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "        #self.linear = nn.Linear(self.input_dim*(grid_size+spline_order), self.output_dim)\n",
        "\n",
        "        #self.drop = nn.Dropout(p=0.01) # dropout\n",
        "\n",
        "    def b_splines(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Compute the B-spline bases for the given input tensor.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "        Returns:\n",
        "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2 and x.size(1) == self.input_dim\n",
        "\n",
        "        grid: torch.Tensor = (\n",
        "            self.grid\n",
        "        )  # (input_dim, grid_size + 2 * spline_order + 1)\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            #print('-- k: ', k)\n",
        "            bases = (\n",
        "                (x - grid[:, : -(k + 1)])\n",
        "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
        "                * bases[:, :, :-1]\n",
        "            ) + (\n",
        "                (grid[:, k + 1 :] - x)\n",
        "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
        "                * bases[:, :, 1:]\n",
        "            )\n",
        "\n",
        "        assert bases.size() == (\n",
        "            x.size(0),\n",
        "            self.input_dim,\n",
        "            self.grid_size + self.spline_order,\n",
        "        )\n",
        "        return bases.contiguous()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # layer normalization\n",
        "        device = x.device\n",
        "\n",
        "        x = self.layernorm(x)\n",
        "        #x = self.drop(x)\n",
        "\n",
        "        # base\n",
        "        #bias = torch.randn(self.output_dim)\n",
        "        #base_output = F.linear(self.base_activation(x), self.base_weight, bias)\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "\n",
        "        # b_splines\n",
        "        bs_output = self.b_splines(x).view(x.size(0), -1)\n",
        "\n",
        "        # rbf\n",
        "        rbf_output = self.rbf(x).view(x.size(0), -1)\n",
        "        #rbf_output = self.rbf(x)\n",
        "        #rbf_output = torch.reshape(rbf_output, (rbf_output.shape[0], -1))\n",
        "\n",
        "        # combine\n",
        "        bsrbf_output = bs_output + rbf_output\n",
        "        bsrbf_output = F.linear(bsrbf_output, self.spline_weight)\n",
        "\n",
        "        return base_output + bsrbf_output\n",
        "\n",
        "class BSRBF_KAN(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers_hidden,\n",
        "        grid_size=5,\n",
        "        spline_order=3,\n",
        "        base_activation=torch.nn.SiLU,\n",
        "    ):\n",
        "        super(BSRBF_KAN, self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        #self.drop = torch.nn.Dropout(p=0.1) # dropout\n",
        "\n",
        "        for input_dim, output_dim in zip(layers_hidden, layers_hidden[1:]):\n",
        "            self.layers.append(\n",
        "                BSRBF_KANLayer(\n",
        "                    input_dim,\n",
        "                    output_dim,\n",
        "                    grid_size=grid_size,\n",
        "                    spline_order=spline_order,\n",
        "                    base_activation=base_activation,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        #x = self.drop(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aJWvNSnBlXWp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Bernoulli\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "attr = torch.load('census_attr_final.pt').to(device)\n",
        "index = torch.load('census_index_final.pt').to(device).int()\n",
        "\n",
        "\n",
        "class DeepLinear(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.layers = BSRBF_KAN([input_dim, hidden_dim//2,hidden_dim//2,output_dim], spline_order = 4,grid_size = 8).to(device)\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x.to(device))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class DeepConv(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        hidden_dim=64\n",
        "        self.num_cat_variables = 107\n",
        "        self.num_cont_variables = 5\n",
        "        self.num_variables = self.num_cat_variables + self.num_cont_variables\n",
        "        self.e_features = attr.shape[1]\n",
        "        self.input_embedding = DeepLinear(self.num_variables,hidden_dim, hidden_dim)\n",
        "        self.attr_embedding = nn.ModuleList([nn.Linear(self.num_variables+self.e_features, hidden_dim) for _ in range(attr.shape[0])])\n",
        "        self.e_scoring_network = DeepLinear(hidden_dim * 2, hidden_dim, output_dim)\n",
        "        self.attribute_feature_extract = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.attribute_scoring_network = DeepLinear(attr.shape[0]*16,hidden_dim,hidden_dim)\n",
        "\n",
        "    def forward(self, X_encoded):\n",
        "        # X_encoded is of shape\n",
        "        batch_size = X_encoded.shape[0]\n",
        "\n",
        "        # Embed variables\n",
        "        variable_embedded = self.input_embedding(X_encoded)\n",
        "        # Embed attributes (each row separately)\n",
        "        index_matrix = torch.zeros(attr.shape[0], self.num_variables).to(device)\n",
        "        for i in range(attr.shape[0]):\n",
        "            index_matrix[i, index[i][0]] = 1\n",
        "            index_matrix[i, index[i][1]] = 1\n",
        "        attr_input = torch.cat((index_matrix, attr), dim=1)\n",
        "        attr_embedded = torch.stack([self.attr_embedding[i](attr_input[i]) for i in range(attr.shape[0])])\n",
        "        attr_features = self.attribute_feature_extract(attr_embedded)\n",
        "        attr_features = torch.flatten(attr_features).unsqueeze(0)\n",
        "        attr_scores = self.attribute_scoring_network(attr_features)\n",
        "        attr_scores = attr_scores.expand(batch_size,-1)\n",
        "        combined_embeddings = torch.cat([variable_embedded, attr_scores], dim=-1)\n",
        "\n",
        "        # Scoring\n",
        "        scores = self.e_scoring_network(combined_embeddings)\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pGIG2eEiRQPJ"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Bernoulli, Independent\n",
        "\n",
        "class DBM(nn.Module):\n",
        "    def __init__(self, nc, nh=None, L=2):\n",
        "        super().__init__()\n",
        "\n",
        "        nv = nc\n",
        "        if nh is None:\n",
        "            nh = nv\n",
        "        self.weight = nn.ParameterList([nn.Parameter(torch.Tensor(nh, nv))])\n",
        "        self.weight.extend([nn.Parameter(torch.Tensor(nh, nh)) for _ in range(L-1)])\n",
        "        self.bias = nn.ParameterList([nn.Parameter(torch.Tensor(nv))])\n",
        "        self.bias.extend([nn.Parameter(torch.Tensor(nh)) for _ in range(L)])\n",
        "\n",
        "        self.nv = nv\n",
        "        self.nh = nh\n",
        "        self.nc = nc\n",
        "        self.L = L\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for w in self.weight:\n",
        "            nn.init.orthogonal_(w)\n",
        "\n",
        "        for b in self.bias:\n",
        "            nn.init.zeros_(b)\n",
        "\n",
        "    def forward(self, v):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "\n",
        "        # Positive phase\n",
        "        if self.L == 1:\n",
        "            if self.marginal:\n",
        "                energy_pos = self.marginal_energy(v, None, True)\n",
        "            else:\n",
        "                v, h = self.gibbs_step(v, None, True,\n",
        "                                       torch.ones(N, device=device))\n",
        "                energy_pos = self.energy(v, h)\n",
        "        else:\n",
        "            h = []\n",
        "            for _ in range(self.L):\n",
        "                h_i = torch.empty(N, self.nh, device=device).bernoulli_()\n",
        "                h.append(h_i)\n",
        "\n",
        "            v, h = self.local_search(v, h, True)\n",
        "            v, h = self.gibbs_step(v, h, True)\n",
        "\n",
        "            energy_pos, v, h = self.coupling(v, h, True)\n",
        "\n",
        "        # Negative phase\n",
        "        v = torch.empty_like(v).bernoulli_()\n",
        "\n",
        "        h = []\n",
        "        for _ in range(self.L):\n",
        "            h_i = torch.empty(N, self.nh, device=device).bernoulli_()\n",
        "            h.append(h_i)\n",
        "\n",
        "        v, h = self.local_search(v, h)\n",
        "        v, h = self.gibbs_step(v, h)\n",
        "\n",
        "        energy_neg, v, h = self.coupling(v, h)\n",
        "\n",
        "        loss = energy_pos - energy_neg\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def local_search(self, v, h, fix_v=False):\n",
        "        N = v.size(0)\n",
        "        device= v.device\n",
        "\n",
        "        rand_u = torch.rand(N, device=device)\n",
        "        _v, _h = deepcopy((v, h))\n",
        "        v, h = self.gibbs_step(v, h, fix_v, rand_u=rand_u, T=0)\n",
        "\n",
        "        converged = torch.ones(N, dtype=torch.bool, device=device) if fix_v \\\n",
        "                    else torch.all(v == _v, 1)\n",
        "        for i in range(self.L):\n",
        "            converged = converged.logical_and(torch.all(h[i] == _h[i], 1))\n",
        "\n",
        "        while not converged.all():\n",
        "            not_converged = converged.logical_not()\n",
        "            _v = v[not_converged]\n",
        "            _h = [h[i][not_converged] for i in range(self.L)]\n",
        "            M = _v.size(0)\n",
        "\n",
        "            v_, h_ = self.gibbs_step(_v, _h, fix_v,\n",
        "                                     rand_u=rand_u[not_converged], T=0)\n",
        "\n",
        "            if fix_v:\n",
        "                converged_ = torch.ones(M, dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                converged_ = torch.all(v_ == _v, 1)\n",
        "                v[not_converged] = v_\n",
        "\n",
        "            for i in range(self.L):\n",
        "                converged_ = converged_.logical_and(torch.all(h_[i] == _h[i], 1))\n",
        "                h[i][not_converged] = h_[i]\n",
        "\n",
        "            converged[not_converged] = converged_\n",
        "\n",
        "        return v, h\n",
        "\n",
        "    def coupling(self, v, h, fix_v=False):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "        _v, _h = deepcopy((v, h))\n",
        "\n",
        "        v, h = self.mh_step(v, h, fix_v)\n",
        "        energy = self.energy(v, h)\n",
        "\n",
        "        converged = torch.ones(N, dtype=torch.bool, device=device) if fix_v \\\n",
        "                    else torch.all(v == _v, 1)\n",
        "        for i in range(self.L):\n",
        "            converged = converged.logical_and(torch.all(h[i] == _h[i], 1))\n",
        "\n",
        "        while not converged.all():\n",
        "            not_converged = converged.logical_not()\n",
        "            _v = v[not_converged]\n",
        "            _h = [h[i][not_converged] for i in range(self.L)]\n",
        "            M = _v.size(0)\n",
        "\n",
        "            rand_v = None if fix_v else torch.rand_like(_v)\n",
        "            rand_h = [torch.rand_like(_h[i]) for i in range(self.L)]\n",
        "            rand_u = torch.rand(M, device=device)\n",
        "\n",
        "            v_, h_ = self.mh_step(_v, _h, fix_v, rand_v, rand_h, rand_u)\n",
        "            energy[not_converged] += self.energy(v_, h_) - self.energy(_v, _h)\n",
        "\n",
        "            if fix_v:\n",
        "                converged_ = torch.ones(M, dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                converged_ = torch.all(v_ == _v, 1)\n",
        "                v[not_converged] = v_\n",
        "\n",
        "            for i in range(self.L):\n",
        "                converged_ = converged_.logical_and(torch.all(h_[i] == _h[i], 1))\n",
        "                h[i][not_converged] = h_[i]\n",
        "\n",
        "            converged[not_converged] = converged_\n",
        "\n",
        "        return energy, v, h\n",
        "\n",
        "    def energy(self, v, h):\n",
        "        energy = - torch.sum(v * self.bias[0].unsqueeze(0), 1)\n",
        "\n",
        "        for i in range(self.L):\n",
        "            logits = F.linear(v if i==0 else h[i-1],\n",
        "                              self.weight[i], self.bias[i+1])\n",
        "\n",
        "            energy -= torch.sum(h[i] * logits, 1)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def gibbs_step(self, v, h, fix_v=False,\n",
        "                   rand_v=None, rand_h=None, rand_u=None, rand_z=None, T=1):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "\n",
        "        v_, h_ = deepcopy((v, h))\n",
        "\n",
        "        if rand_u is None:\n",
        "            rand_u = torch.rand(N, device=device)\n",
        "\n",
        "        even = rand_u < 0.5\n",
        "        odd = even.logical_not()\n",
        "\n",
        "        if even.sum() > 0:\n",
        "            if not fix_v:\n",
        "                logits = F.linear(h_[0][even],\n",
        "                                  self.weight[0].t(), self.bias[0])\n",
        "\n",
        "                if T == 0:\n",
        "                    v_[even] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits /= T\n",
        "\n",
        "                    if rand_v is None:\n",
        "                        v_[even] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        v_[even] = (rand_v[even] < logits.sigmoid()).float()\n",
        "\n",
        "            for i in range(1, len(h), 2):\n",
        "                logits = F.linear(h_[i-1][even],\n",
        "                                  self.weight[i], self.bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits += F.linear(h_[i+1][even],\n",
        "                                       self.weight[i+1].t(), None)\n",
        "\n",
        "                if T == 0:\n",
        "                    h_[i][even] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits /= T\n",
        "\n",
        "                    if rand_h is None:\n",
        "                        h_[i][even] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        h_[i][even] = (rand_h[i][even] < logits.sigmoid()).float()\n",
        "\n",
        "            for i in range(0, len(h), 2):\n",
        "                logits = F.linear(v_[even] if i==0 else h_[i-1][even],\n",
        "                                  self.weight[i], self.bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits += F.linear(h_[i+1][even],\n",
        "                                       self.weight[i+1].t(), None)\n",
        "\n",
        "                if T == 0:\n",
        "                    h_[i][even] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits /= T\n",
        "\n",
        "                    if rand_h is None:\n",
        "                        h_[i][even] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        h_[i][even] = (rand_h[i][even] < logits.sigmoid()).float()\n",
        "\n",
        "        if odd.sum() > 0:\n",
        "            for i in range(0, len(h), 2):\n",
        "                logits = F.linear(v_[odd] if i==0 else h_[i-1][odd],\n",
        "                                  self.weight[i], self.bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits += F.linear(h_[i+1][odd],\n",
        "                                       self.weight[i+1].t(), None)\n",
        "\n",
        "                if T == 0:\n",
        "                    h_[i][odd] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits /= T\n",
        "\n",
        "                    if rand_h is None:\n",
        "                        h_[i][odd] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        h_[i][odd] = (rand_h[i][odd] < logits.sigmoid()).float()\n",
        "\n",
        "            if not fix_v:\n",
        "                logits = F.linear(h_[0][odd],\n",
        "                                  self.weight[0].t(), self.bias[0])\n",
        "\n",
        "                if T == 0:\n",
        "                    v_[odd] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits /= T\n",
        "\n",
        "                    if rand_v is None:\n",
        "                        v_[odd] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        v_[odd] = (rand_v[odd] < logits.sigmoid()).float()\n",
        "\n",
        "            for i in range(1, len(h), 2):\n",
        "                logits = F.linear(h_[i-1][odd],\n",
        "                                  self.weight[i], self.bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits += F.linear(h_[i+1][odd],\n",
        "                                       self.weight[i+1].t(), None)\n",
        "\n",
        "                if T == 0:\n",
        "                    h_[i][odd] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits /= T\n",
        "\n",
        "                    if rand_h is None:\n",
        "                        h_[i][odd] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        h_[i][odd] = (rand_h[i][odd] < logits.sigmoid()).float()\n",
        "\n",
        "        return v_, h_\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def mh_step(self, v, h, fix_v=False,\n",
        "                rand_v=None, rand_h=None, rand_u=None):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "\n",
        "        if fix_v:\n",
        "            v_ = v\n",
        "        else:\n",
        "            if rand_v is None:\n",
        "                v_ = torch.empty_like(v).bernoulli_()\n",
        "            else:\n",
        "                v_ = (rand_v < 0.5).float()\n",
        "\n",
        "        if rand_h is None:\n",
        "            h_ = [torch.empty_like(h[i]).bernoulli_() for i in range(self.L)]\n",
        "        else:\n",
        "            h_ = [(rand_h[i] < 0.5).float() for i in range(self.L)]\n",
        "\n",
        "        log_ratio = self.energy(v, h) - self.energy(v_, h_)\n",
        "\n",
        "        if rand_u is None:\n",
        "            accepted = log_ratio.exp().clamp(0, 1).bernoulli().bool()\n",
        "        else:\n",
        "            accepted = rand_u < log_ratio.exp()\n",
        "\n",
        "        if not fix_v:\n",
        "            v = torch.where(accepted.unsqueeze(1), v_, v)\n",
        "        h = [torch.where(accepted.unsqueeze(1), h_[i], h[i]) for i in range(self.L)]\n",
        "\n",
        "        return v, h\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, N):\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        v = torch.empty(N, self.nv, device=device).bernoulli_()\n",
        "        h = [torch.empty(N, self.nh,\n",
        "                         device=device).bernoulli_() for _ in range(self.L)]\n",
        "\n",
        "        v_mode, h_mode = self.local_search(v, h)\n",
        "        v_rand, h_rand = self.gibbs_step(v_mode, h_mode)\n",
        "\n",
        "        return v_mode, v_rand\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def reconstruct(self, v):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "\n",
        "        h = [torch.empty(N, self.nh, device=device).bernoulli_() for _ in range(self.L)]\n",
        "\n",
        "        v, h = self.local_search(v, h, True)\n",
        "        v_mode, h_mode = self.gibbs_step(v, h, T=0)\n",
        "        v_rand, h_rand = self.gibbs_step(v, h)\n",
        "\n",
        "        return v_mode, v_rand"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "def from_numpy(array, device, dtype=np.float32):\n",
        "    return torch.from_numpy(array.astype(dtype)).to(device)\n",
        "\n",
        "\n",
        "def to_numpy(tensor, device):\n",
        "    if device.type == 'cuda':\n",
        "        tensor = tensor.cpu()\n",
        "\n",
        "    return tensor.data.numpy()\n",
        "\n",
        "\n",
        "def make_data_loader(array, device, batch_size):\n",
        "    return DataLoader(\n",
        "        TensorDataset(from_numpy(array, device)),\n",
        "        batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "def reparameterize(mu, ln_var):\n",
        "    std = torch.exp(0.5 * ln_var)\n",
        "    eps = torch.randn_like(std)\n",
        "    z = mu + std * eps\n",
        "    return z\n",
        "\n",
        "\n",
        "def gaussian_nll(x, mu, ln_var, dim=1):\n",
        "    prec = torch.exp(-1 * ln_var)\n",
        "    x_diff = x - mu\n",
        "    x_power = (x_diff * x_diff) * prec * -0.5\n",
        "    return torch.sum((ln_var + math.log(2 * math.pi)) * 0.5 - x_power, dim=dim)\n",
        "\n",
        "\n",
        "def standard_gaussian_nll(x, dim=1):\n",
        "    return torch.sum(0.5 * math.log(2 * math.pi) + 0.5 * x * x, dim=dim)\n",
        "\n",
        "\n",
        "def bernoulli_nll(x, logits, dim=1):\n",
        "    return torch.sum(F.softplus(logits) - x * logits, dim=dim)\n",
        "\n",
        "\n",
        "def gaussian_kl_divergence(mu, ln_var, dim=1):\n",
        "    return torch.sum(-0.5 * (1 + ln_var - mu.pow(2) - torch.exp(ln_var)), dim=dim)\n",
        "\n",
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class GaussianNetwork(nn.Module):\n",
        "    def __init__(self, n_in, n_latent, n_h):\n",
        "        super(GaussianNetwork, self).__init__()\n",
        "        self.input_layer = DeepConv(128,512)\n",
        "        n_in = 512\n",
        "        self.n_in = n_in\n",
        "        self.n_latent = n_latent\n",
        "        self.n_h = n_h\n",
        "\n",
        "        # Encoder\n",
        "        self.le1 = nn.Sequential(\n",
        "            nn.Linear(n_in, n_h), nn.Tanh(),\n",
        "            nn.Linear(n_h, n_h), nn.Tanh(),\n",
        "            nn.Linear(n_h, n_h), nn.Tanh(),\n",
        "        )\n",
        "        self.le2_mu = nn.Linear(n_h, n_latent)\n",
        "        self.le2_ln_var = nn.Linear(n_h, n_latent)\n",
        "\n",
        "        # Decoder\n",
        "        self.ld1 = nn.Sequential(\n",
        "            nn.Linear(n_latent, n_h), nn.Tanh(),\n",
        "            nn.Linear(n_h, n_h), nn.Tanh(),\n",
        "            nn.Linear(n_h, n_h), nn.Tanh(),\n",
        "        )\n",
        "        self.ld2_mu = nn.Linear(n_h, 5)\n",
        "        self.ld2_ln_var = nn.Linear(n_h, 5)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        h = self.le1(x)\n",
        "        return self.le2_mu(h), self.le2_ln_var(h)\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.ld1(z)\n",
        "        return self.ld2_mu(h), self.ld2_ln_var(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, ln_var = self.encode(x)\n",
        "        return reparameterize(mu=mu, ln_var=ln_var)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_latent, n_h):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.n_latent = n_latent\n",
        "        self.n_h = n_h\n",
        "\n",
        "        # Layer\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(n_latent, n_h), nn.Tanh(),\n",
        "            nn.Linear(n_h, n_h), nn.Tanh(),\n",
        "            nn.Linear(n_h, n_h), nn.Tanh(), nn.Dropout(),\n",
        "            nn.Linear(n_h, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.layers(z).squeeze()\n",
        "\n",
        "\n",
        "class GaussianVAEIOP:\n",
        "\n",
        "    def __init__(self, n_in, n_latent, n_h):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.network = GaussianNetwork(n_in=32, n_latent=n_latent, n_h=n_h).to(self.device)\n",
        "        self.discriminator = Discriminator(n_latent=n_latent, n_h=n_h).to(self.device)\n",
        "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.train_times = []\n",
        "        self.reconstruction_errors = []\n",
        "        self.kl_divergences = []\n",
        "        self.valid_losses = []\n",
        "        self.min_valid_loss = float(\"inf\")\n",
        "\n",
        "    def _compute_RE_and_KL(self, x, k=1):\n",
        "        mu_enc, ln_var_enc = self.network.encode(x)\n",
        "\n",
        "        RE = 0\n",
        "        density_ratio = 0\n",
        "        for i in range(k):\n",
        "            z = reparameterize(mu=mu_enc, ln_var=ln_var_enc)\n",
        "            mu_dec, ln_var_dec = self.network.decode(z)\n",
        "            RE += gaussian_nll(x[:, 107:], mu=mu_dec, ln_var=ln_var_dec) / k\n",
        "            density_ratio += self.discriminator(z) / k\n",
        "\n",
        "        KL = gaussian_kl_divergence(mu=mu_enc, ln_var=ln_var_enc) - density_ratio\n",
        "        return RE, KL\n",
        "\n",
        "    def _evidence_lower_bound(self, x, k=1):\n",
        "        RE, KL = self._compute_RE_and_KL(x, k=k)\n",
        "        return -1 * (RE + KL)\n",
        "\n",
        "    def _importance_sampling(self, x, k=1):\n",
        "        mu_enc, ln_var_enc = self.network.encode(x1)\n",
        "        lls = []\n",
        "        for i in range(k):\n",
        "            z = reparameterize(mu=mu_enc, ln_var=ln_var_enc)\n",
        "            mu_dec, ln_var_dec = self.network.decode(z)\n",
        "            ll = -1 * gaussian_nll(x, mu=mu_dec, ln_var=ln_var_dec, dim=1)\n",
        "            ll -= standard_gaussian_nll(z, dim=1)\n",
        "            ll += gaussian_nll(z, mu=mu_enc, ln_var=ln_var_enc, dim=1)\n",
        "            ll += self.discriminator(z)\n",
        "            lls.append(ll[:, None])\n",
        "\n",
        "        return torch.cat(lls, dim=1).logsumexp(dim=1) - math.log(k)\n",
        "\n",
        "    def _loss_VAE(self, x, k=1, beta=1):\n",
        "        RE, KL = self._compute_RE_and_KL(x, k=k)\n",
        "        # RE_sum = RE.sum()\n",
        "        # KL_sum = KL.sum()\n",
        "        RE_sum = torch.sum(RE)\n",
        "        KL_sum = torch.sum(KL)\n",
        "        loss = RE_sum + beta * KL_sum\n",
        "        return loss, RE_sum, KL_sum\n",
        "\n",
        "    def _loss_DRE(self, x):\n",
        "        z_inferred = self.network(x).detach()\n",
        "        z_sampled = torch.randn_like(z_inferred)\n",
        "        logits_inferred = self.discriminator(z_inferred, use_dropout=True)\n",
        "        logits_sampled = self.discriminator(z_sampled, use_dropout=True)\n",
        "        loss = self.criterion(logits_inferred, torch.ones_like(logits_inferred))\n",
        "        loss += self.criterion(logits_sampled, torch.zeros_like(logits_sampled))\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X, k=1, batch_size=500,\n",
        "            n_epoch_primal=500, n_epoch_dual=10,\n",
        "            learning_rate_primal=1e-4, learning_rate_dual=1e-3,\n",
        "            dynamic_binarization=False, warm_up=False, warm_up_epoch=3,\n",
        "            is_stoppable=False, X_valid=None, path=None):\n",
        "\n",
        "        self.network.train()\n",
        "        self.discriminator.train()\n",
        "        N = X.shape[0]\n",
        "        data_loader = make_data_loader(X, device=self.device, batch_size=batch_size)\n",
        "        optimizer_primal = torch.optim.Adam(self.network.parameters(), lr=learning_rate_primal)\n",
        "        optimizer_dual = torch.optim.Adam(self.discriminator.parameters(), lr=learning_rate_dual)\n",
        "\n",
        "        if is_stoppable:\n",
        "            X_valid = from_numpy(X_valid, self.device)\n",
        "\n",
        "        for epoch_primal in range(n_epoch_primal):\n",
        "            start = time.time()\n",
        "\n",
        "            # warm-up\n",
        "            beta = 1 * epoch_primal / warm_up_epoch if warm_up and epoch_primal <= warm_up_epoch else 1\n",
        "\n",
        "            mean_loss = 0\n",
        "            mean_RE = 0\n",
        "            mean_KL = 0\n",
        "\n",
        "            # Training VAE\n",
        "            self.discriminator.eval()\n",
        "            for _, batch in enumerate(data_loader):\n",
        "                self.network.zero_grad()\n",
        "                xs = torch.bernoulli(batch[0]) if dynamic_binarization else batch[0]\n",
        "                loss_VAE, RE, KL = self._loss_VAE(xs, k=k, beta=beta)\n",
        "                loss_VAE.backward()\n",
        "                mean_loss += loss_VAE.item() / N\n",
        "                mean_RE += RE.item() / N\n",
        "                mean_KL += KL.item() / N\n",
        "                optimizer_primal.step()\n",
        "\n",
        "            # Training DRE\n",
        "            self.discriminator.train()\n",
        "            for epoch_dual in range(n_epoch_dual):\n",
        "                sum_loss_DRE = 0\n",
        "                for _, batch in enumerate(data_loader):\n",
        "                    self.discriminator.zero_grad()\n",
        "                    xs = torch.bernoulli(batch[0]) if dynamic_binarization else batch[0]\n",
        "                    mu_enc, ln_var_enc = self.network.encode(x=xs)\n",
        "                    z_inferred = reparameterize(mu_enc, ln_var_enc).detach()\n",
        "                    z_sampled = torch.randn_like(z_inferred)\n",
        "                    logits_inferred = self.discriminator(z_inferred)\n",
        "                    logits_sampled = self.discriminator(z_sampled)\n",
        "                    loss_DRE = self.criterion(logits_inferred, torch.ones_like(logits_inferred))\n",
        "                    loss_DRE += self.criterion(logits_sampled, torch.zeros_like(logits_sampled))\n",
        "                    loss_DRE.backward()\n",
        "                    sum_loss_DRE += loss_DRE.item()\n",
        "                    optimizer_dual.step()\n",
        "\n",
        "                print(f\"\\tDRE epoch: {epoch_dual} / Train: {sum_loss_DRE / N:f}\")\n",
        "\n",
        "            end = time.time()\n",
        "            self.train_losses.append(mean_loss)\n",
        "            self.train_times.append(end - start)\n",
        "            self.reconstruction_errors.append(mean_RE)\n",
        "            self.kl_divergences.append(mean_KL)\n",
        "\n",
        "            print(\n",
        "                f\"VAE epoch: {epoch_primal} / Train: {mean_loss:0.3f} / RE: {mean_RE:0.3f} / KL: {mean_KL:0.3f}\",\n",
        "                end=''\n",
        "            )\n",
        "\n",
        "            if warm_up and epoch_primal < warm_up_epoch:\n",
        "                print(\" / Warm-up\", end='')\n",
        "            elif is_stoppable:\n",
        "                valid_loss, _, _ = self._loss_VAE(X_valid, k=k, beta=1)\n",
        "                valid_loss = valid_loss.item() / X_valid.shape[0]\n",
        "                print(f\" / Valid: {valid_loss:0.3f}\", end='')\n",
        "                self.valid_losses.append(valid_loss)\n",
        "                self._early_stopping(valid_loss, path)\n",
        "\n",
        "            print('')\n",
        "\n",
        "        if is_stoppable:\n",
        "            self.network.load_state_dict(torch.load(path + \"_network\"))\n",
        "            self.discriminator.load_state_dict(torch.load(path + \"_discriminator\"))\n",
        "\n",
        "        self.network.eval()\n",
        "        self.discriminator.eval()\n",
        "\n",
        "    def _early_stopping(self, valid_loss, path):\n",
        "        if valid_loss < self.min_valid_loss:\n",
        "            self.min_valid_loss = valid_loss\n",
        "            torch.save(self.network.state_dict(), path + \"_network\")\n",
        "            torch.save(self.discriminator.state_dict(), path + \"_discriminator\")\n",
        "            print(\" / Save\", end='')\n",
        "\n",
        "    def encode(self, X):\n",
        "        mu, ln_var = self.network.encode(from_numpy(X, self.device))\n",
        "        return to_numpy(mu, self.device), to_numpy(ln_var, self.device)\n",
        "\n",
        "    def decode(self, Z):\n",
        "        mu, ln_var = self.network.decode(from_numpy(Z, self.device))\n",
        "        return to_numpy(mu, self.device), to_numpy(ln_var, self.device)\n",
        "\n",
        "    def evidence_lower_bound(self, X, k=1):\n",
        "        return to_numpy(self._evidence_lower_bound(from_numpy(X, self.device), k=k), self.device)\n",
        "\n",
        "    def importance_sampling(self, X, k=1):\n",
        "        return to_numpy(self._importance_sampling(from_numpy(X, self.device), k=k), self.device)\n",
        "    def save_models(self, path):\n",
        "        \"\"\"Saves the state dictionaries of the network and discriminator models.\n",
        "\n",
        "        Args:\n",
        "            path (str): The path where the models will be saved (should have a .pth extension).\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            'network_state_dict': self.network.state_dict(),\n",
        "            'discriminator_state_dict': self.discriminator.state_dict()\n",
        "        }, path)\n",
        "    def load_models(self, path):\n",
        "        \"\"\"Loads the state dictionaries of the network and discriminator models.\n",
        "\n",
        "        Args:\n",
        "            path (str): The path to the saved model file (with .pth extension).\n",
        "        \"\"\"\n",
        "        loaded_data = torch.load(path, map_location=self.device)  # Load on the correct device\n",
        "        self.network.load_state_dict(loaded_data['network_state_dict'])\n",
        "        self.discriminator.load_state_dict(loaded_data['discriminator_state_dict'])"
      ],
      "metadata": {
        "id": "a30dzjXwKEaM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "final_vae_model = GaussianVAEIOP(5, 32, 1000)\n",
        "final_vae_model.load_models('VAEIOPv4.pth')\n",
        "#final_vae_model = torch.load('VAEIOP1.pth')\n",
        "final_vae_model.fit(np.array(X_encoded),warm_up=False)"
      ],
      "metadata": {
        "id": "H7VeWd5HB8lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897f81e7-4fbb-476a-dd9a-fc5f25d38d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tDRE epoch: 0 / Train: 0.000136\n",
            "\tDRE epoch: 1 / Train: 0.000139\n",
            "\tDRE epoch: 2 / Train: 0.000137\n",
            "\tDRE epoch: 3 / Train: 0.000138\n",
            "\tDRE epoch: 4 / Train: 0.000140\n",
            "\tDRE epoch: 5 / Train: 0.000138\n",
            "\tDRE epoch: 6 / Train: 0.000137\n",
            "\tDRE epoch: 7 / Train: 0.000134\n",
            "\tDRE epoch: 8 / Train: 0.000138\n",
            "\tDRE epoch: 9 / Train: 0.000134\n",
            "VAE epoch: 0 / Train: 4.864 / RE: 3.342 / KL: 1.522\n",
            "\tDRE epoch: 0 / Train: 0.000086\n",
            "\tDRE epoch: 1 / Train: 0.000081\n",
            "\tDRE epoch: 2 / Train: 0.000080\n",
            "\tDRE epoch: 3 / Train: 0.000079\n",
            "\tDRE epoch: 4 / Train: 0.000080\n",
            "\tDRE epoch: 5 / Train: 0.000080\n",
            "\tDRE epoch: 6 / Train: 0.000077\n",
            "\tDRE epoch: 7 / Train: 0.000079\n",
            "\tDRE epoch: 8 / Train: 0.000077\n",
            "\tDRE epoch: 9 / Train: 0.000078\n",
            "VAE epoch: 1 / Train: 0.782 / RE: 0.267 / KL: 0.515\n",
            "\tDRE epoch: 0 / Train: 0.000092\n",
            "\tDRE epoch: 1 / Train: 0.000092\n",
            "\tDRE epoch: 2 / Train: 0.000094\n",
            "\tDRE epoch: 3 / Train: 0.000094\n",
            "\tDRE epoch: 4 / Train: 0.000094\n",
            "\tDRE epoch: 5 / Train: 0.000088\n",
            "\tDRE epoch: 6 / Train: 0.000090\n",
            "\tDRE epoch: 7 / Train: 0.000092\n",
            "\tDRE epoch: 8 / Train: 0.000093\n",
            "\tDRE epoch: 9 / Train: 0.000090\n",
            "VAE epoch: 2 / Train: 3.373 / RE: 1.418 / KL: 1.956\n",
            "\tDRE epoch: 0 / Train: 0.000088\n",
            "\tDRE epoch: 1 / Train: 0.000086\n",
            "\tDRE epoch: 2 / Train: 0.000082\n",
            "\tDRE epoch: 3 / Train: 0.000084\n",
            "\tDRE epoch: 4 / Train: 0.000087\n",
            "\tDRE epoch: 5 / Train: 0.000089\n",
            "\tDRE epoch: 6 / Train: 0.000086\n",
            "\tDRE epoch: 7 / Train: 0.000079\n",
            "\tDRE epoch: 8 / Train: 0.000084\n",
            "\tDRE epoch: 9 / Train: 0.000081\n",
            "VAE epoch: 3 / Train: 4.315 / RE: 2.181 / KL: 2.134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_models(model, path):\n",
        "        \"\"\"Saves the state dictionaries of the network and discriminator models.\n",
        "\n",
        "        Args:\n",
        "            path (str): The path where the models will be saved (should have a .pth extension).\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            'network_state_dict': model.network.state_dict(),\n",
        "            'discriminator_state_dict': model.discriminator.state_dict()\n",
        "        }, path)\n",
        "\n",
        "\n",
        "save_models(final_vae_model,'VAEIOPv4.pth')\n",
        "%cp VAEIOPv4.pth /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "XWLS3QkMbasN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(GaussianVAEIOP,'VAEIOP2.pth')\n",
        "\n",
        "%cp VAEIOP2.pth /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "CFr41YlvF4su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JyUngbZz2D5"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "\n",
        "def train_DBM(dataset, model, num_epochs, learning_rate, batch_size=500):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Data loader for efficient batching\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        total_energy_loss = 0.0\n",
        "        total_vae_rc_loss = 0.0\n",
        "        total_kl_loss = 0.0\n",
        "        total_val_cat = 0.0\n",
        "        total_val_cat_rand = 0.0\n",
        "        i=0\n",
        "        for data in train_loader:  # Iterate over batches\n",
        "            i+=1\n",
        "            # Forward pass\n",
        "            data = data.float().to(device)\n",
        "            energy_loss, recon, recon_rand = model(data)\n",
        "            energy_loss = torch.mean(energy_loss)\n",
        "            total_energy_loss+=energy_loss.item()\n",
        "            total_val_cat += nn.BCELoss()(recon,data[:,:model.num_cat])\n",
        "            total_val_cat_rand += nn.BCELoss()(recon_rand,data[:,:model.num_cat])\n",
        "            # Loss\n",
        "            loss = energy_loss\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_energy_loss /= len(train_loader)\n",
        "    #    total_vae_rc_loss /= len(train_loader)\n",
        "    #    total_kl_loss /= len(train_loader)\n",
        "        total_val_cat /= len(train_loader)\n",
        "        total_val_cat_rand /= len(train_loader)\n",
        "      #  print(f\"Epoch {epoch} Energy Loss: {total_energy_loss:.4f} VAE rc loss: {total_vae_rc_loss:.4f} KL Loss: {total_kl_loss:.4f} val_cat {total_val_cat:.4f} Val_cat_rand loss {total_val_cat_rand:.4f} Time: {end_time-start_time:.4f}\")\n",
        "        print(f\"Epoch {epoch} Energy Loss: {total_energy_loss:.4f} val_cat {total_val_cat:.4f} Val_cat_rand loss {total_val_cat_rand:.4f} Time: {end_time-start_time:.4f}\")\n",
        "\n",
        "\n",
        "#final_model = VAEDBM(107, 5, 32, 10, 128, 8).to(device)\n",
        "final_model = torch.load('/content/drive/MyDrive/32_10_dbm_model1.pth')\n",
        "\n",
        "train_DBM(dataset, final_model, 1000, 0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLzceZt2QYdd"
      },
      "source": [
        "Consider using the convolutional layer as some sort of attention mechanism / conditional whatever for the VAE, and leave the DBM to have the pure input.\n",
        "\n",
        "Trained VAE separately with convolutional attention/ conditional batch. Train DBM separately. Make converter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09SRE488UDKr"
      },
      "outputs": [],
      "source": [
        "torch.save(final_model,'32_10_dbm_model1.pth')\n",
        "\n",
        "%cp 32_10_dbm_model1.pth /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Bernoulli\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "class Converter(nn.Module):\n",
        "    def __init__(self, hidden_dim, VAE_model):\n",
        "        super().__init__()\n",
        "        self.num_cat_variables = 107\n",
        "        self.VAE = VAE_model\n",
        "        self.convert = DeepLinear(self.num_variables,hidden_dim, self.VAE.network.n_latent)\n",
        "\n",
        "\n",
        "    def forward(self, X_encoded):\n",
        "        # X_encoded is of shape\n",
        "        gaussianParameters = self.convert(X_encoded[:,:self.num_cat_variables])\n",
        "        return gaussianParameters\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "\n",
        "def train_converter(dataset, model, num_epochs, learning_rate, batch_size=2048):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Data loader for efficient batching\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        total_loss = 0.0\n",
        "        i=0\n",
        "        for data in train_loader:\n",
        "            i+=1\n",
        "            # Forward pass]\n",
        "            data = data.float().to(device)\n",
        "            target = model.VAE.encode(data[:,model.num_cat_variables:])\n",
        "            est_parameters = model(data)\n",
        "            loss = nn.MSELoss()(est_parameters,target)\n",
        "            total_loss+=loss.item()\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_loss /= len(train_loader)\n",
        "        print(f\"Epoch {epoch} Loss: {total_loss:.4f} Time: {end_time-start_time:.4f}\")\n",
        "\n",
        "\n",
        "final_vae_model = GaussianVAEIOP(5, 32, 1000)\n",
        "final_vae_model.load_models('/content/drive/MyDrive/VAEIOPv5.pth')\n",
        "converter_model = Converter(128, final_vae_model)\n",
        "train_converter(dataset, converter_model, 1000, 0.001)"
      ],
      "metadata": {
        "id": "gOoFEas7Hzir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMD9zbgMzVv1"
      },
      "source": [
        "For generating fresh sample, the VAEDBM can at best generate 2 disconnected samples (cat and cont).\n",
        "\n",
        "A converter that takes in the categorical input and outputs its equivalent point in the VAE latent space. Trained from taking each row in the data, the categorical variables are the input, and the target value is the continuous variables after being fed through the trained encoder. Use KAN for non-linear relationship. Then to generate a sample, generate the categorical sample, then feed it through the converter, and then decode the output."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}