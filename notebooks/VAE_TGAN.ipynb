{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa4nnkdUKwWj",
        "outputId": "bad863ce-b610-46fe-a2ee-71fb3de379ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.0.3)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install ucimlrepo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1FGcBJtfwDs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "X = pd.read_csv(\"censusData.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DhCnFMeKw_4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, attention_hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(attention_hidden_size, attention_hidden_size).to(device)\n",
        "\n",
        "    def forward(self, encoder_outputs):\n",
        "      # Transform x using a linear layer; output shape will be (sq, b, hidden_size)\n",
        "      x_transformed = self.linear(encoder_outputs)\n",
        "      # Step 2: Compute attention scores using softmax across the sequence dimension (sq)\n",
        "      # Attention scores shape: (sq, b, hidden_size) -> (b, sq, hidden_size) for softmax\n",
        "      x_transposed = x_transformed.transpose(0, 1)  # Transposing for softmax operation\n",
        "      attention_scores = F.softmax(x_transposed, dim=1)  # Applying softmax; shape remains (b, sq, hidden_size)\n",
        "      # Step 3: Apply attention scores to the original input tensor\n",
        "      # For weighted sum, first transpose x back: (sq, b, hidden_size) -> (b, sq, hidden_size)\n",
        "      x = encoder_outputs.transpose(0, 1)  # Transposing x to match attention_scores shape\n",
        "      # Compute the context vector as the weighted sum of the input vectors\n",
        "      # (b, sq, hidden_size) * (b, sq, hidden_size) -> (b, hidden_size) after summing over sq dimension\n",
        "      context_vector = torch.sum(attention_scores * x, dim=1)\n",
        "      return context_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size,only_z=False):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear4 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear5 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear6 = nn.Linear(hidden_size, latent_size)\n",
        "        self.linear_mu = nn.Linear(latent_size, latent_size)\n",
        "        self.linear_logvar = nn.Linear(latent_size, latent_size)\n",
        "        self.only_z = only_z\n",
        "        self.relu1 = nn.GELU()\n",
        "        self.relu2 = nn.GELU()\n",
        "        self.relu3 = nn.GELU()\n",
        "        self.relu4 = nn.GELU()\n",
        "        self.relu5 = nn.GELU()\n",
        "        self.relu6 = nn.GELU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu1(self.linear1(x))\n",
        "        out = self.relu2(self.linear2(out))\n",
        "        out = self.relu3(self.linear3(out))\n",
        "        out = self.relu4(self.linear4(out))\n",
        "        out = self.relu5(self.linear5(out))\n",
        "        out = self.relu6(self.linear6(out))\n",
        "        mu = self.linear_mu(out)\n",
        "        logvar = self.linear_logvar(out)\n",
        "\n",
        "        # Reparameterization trick (as before)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = eps.mul(std).add_(mu)\n",
        "        if self.only_z:\n",
        "          return z\n",
        "        return z, mu, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(latent_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "        self.relu1 = nn.GELU()\n",
        "        self.relu2 = nn.GELU()\n",
        "        self.relu3 = nn.GELU()\n",
        "        self.relu4 = nn.GELU()\n",
        "\n",
        "    def forward(self, z, sig=False):\n",
        "        out = self.relu1(self.linear1(z))\n",
        "        out = self.relu2(self.linear2(out))\n",
        "        out = self.relu3(self.linear3(out))\n",
        "        out = torch.sigmoid(self.output_layer(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size, cat=False):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(input_size, hidden_size, latent_size).to(device)\n",
        "        self.decoder = Decoder(latent_size, hidden_size, input_size).to(device)\n",
        "        self.cat=cat\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.cat:\n",
        "          z, logits = self.encoder(x,True)\n",
        "          recon = self.decoder(z,True)\n",
        "          return recon, logits\n",
        "        else:\n",
        "          z, mu, logvar = self.encoder(x)\n",
        "          recon = self.decoder(z)\n",
        "          return recon, mu, logvar\n",
        "\n",
        "class ConditionalBatchNorm1d(nn.Module):\n",
        "    def __init__(self, num_features, num_conditions):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "\n",
        "        self.gamma_layer = nn.Linear(num_conditions, num_features)\n",
        "        self.beta_layer = nn.Linear(num_conditions, num_features)\n",
        "\n",
        "    def forward(self, input, condition):\n",
        "\n",
        "        out = F.batch_norm(input, None, None, training=True).to(device)  # Standard batch normalization\n",
        "        gamma = self.gamma_layer(condition).to(device)\n",
        "        beta = self.beta_layer(condition).to(device)\n",
        "\n",
        "        out = gamma * out + beta\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVNvFy0Opu_x",
        "outputId": "73b7595e-9dd2-432e-d23e-b16ce9cead8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48842, 107)\n",
            "torch.Size([2, 5671])\n",
            "torch.Size([5671, 131])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "rbf_hsic_matrix = torch.load('rbf_hsic_matrix_updated.pt')\n",
        "linear_hsic_matrix = torch.load('linear_hsic_matrix_updated.pt')\n",
        "mutual_information_matrix = torch.load('mutual_information_matrix.pt')\n",
        "distance_correlation_matrix = torch.load('distance_correlation_matrix.pt')\n",
        "chi2_matrix = torch.load('chi2_matrix.pt')\n",
        "theils_u_matrix = torch.load('theils_u_matrix.pt')\n",
        "cramers_v_matrix = torch.load('cramers_v_matrix.pt')\n",
        "\n",
        "def load_measure_matrix(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data['matrix'], data['feature_names']\n",
        "\n",
        "agreement_matrix, agreement_feature_names = load_measure_matrix('agreement_matrix.pkl')\n",
        "binary_matrix, binary_feature_names = load_measure_matrix('binary_matrix.pkl')\n",
        "categorical_matrix, categorical_feature_names = load_measure_matrix('categorical_matrix.pkl')\n",
        "confusion_matrix, confusion_feature_names = load_measure_matrix('confusion_matrix.pkl')\n",
        "\n",
        "num_features = rbf_hsic_matrix.shape[0]\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categorical_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country','income']\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "X_encoded = encoder.fit_transform(X[categorical_columns])\n",
        "feature_names = encoder.get_feature_names_out(categorical_columns)\n",
        "\n",
        "column_mapping = {}\n",
        "start_index = 0\n",
        "for col in categorical_columns:\n",
        "    column_mapping[col] = start_index\n",
        "    start_index += len(encoder.categories_[categorical_columns.index(col)])\n",
        "\n",
        "print(X_encoded.shape)\n",
        "\n",
        "index = []\n",
        "attr = []\n",
        "\n",
        "for i in range(num_features):\n",
        "    for j in range(i + 1, num_features):\n",
        "        index.append([i, j])\n",
        "\n",
        "        # Find the categorical columns associated with features i and j\n",
        "        col_i = next(col for col, start_idx in column_mapping.items() if start_idx <= i < start_idx + len(encoder.categories_[categorical_columns.index(col)]))\n",
        "        col_j = next(col for col, start_idx in column_mapping.items() if start_idx <= j < start_idx + len(encoder.categories_[categorical_columns.index(col)]))\n",
        "\n",
        "        # Create the categorical column vector (1 for the corresponding column, 0 otherwise)\n",
        "        categorical_col_vec = np.zeros(len(categorical_columns))\n",
        "        categorical_col_vec[categorical_columns.index(col_i)] = 1\n",
        "        categorical_col_vec[categorical_columns.index(col_j)] = 1\n",
        "\n",
        "        list1 = [linear_hsic_matrix[i, j],\n",
        "            rbf_hsic_matrix[i, j],\n",
        "            mutual_information_matrix[i, j],\n",
        "            distance_correlation_matrix[i, j],\n",
        "            chi2_matrix[i, j],\n",
        "            theils_u_matrix[i, j],\n",
        "            cramers_v_matrix[i, j]]\n",
        "        for measure in agreement_matrix[i][j].keys():\n",
        "          list1.append(agreement_matrix[i][j][measure])\n",
        "        for measure in binary_matrix[i][j].keys():\n",
        "          if measure == 'mcnemar_test':\n",
        "            list1.append(binary_matrix[i][j][measure][0])\n",
        "          else:\n",
        "            list1.append(binary_matrix[i][j][measure])\n",
        "        for measure in categorical_matrix[i][j].keys():\n",
        "          list1.append(categorical_matrix[i][j][measure])\n",
        "        for measure in confusion_matrix[i][j].keys():\n",
        "          list1.append(confusion_matrix[i][j][measure])\n",
        "        list1.extend(categorical_col_vec)\n",
        "        attr.append(list1)\n",
        "\n",
        "\n",
        "index = torch.tensor(index, dtype=torch.long).t().contiguous()\n",
        "attr = torch.tensor(attr, dtype=torch.float).to(device)\n",
        "print(index.shape)\n",
        "print(attr.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IvuOPYTwsqo"
      },
      "source": [
        "Remake the convolutional layer to be a DBM, then have a complete DBM model, keep deepConv for VAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV8ZZHK56qhs",
        "outputId": "ba7e0644-e0b3-4968-b322-fd74aae41b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([48842, 5671])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch\n",
        "index1 = index[0]\n",
        "index2 = index[1]\n",
        "\n",
        "X_encoded = torch.tensor(X_encoded)\n",
        "# Optimization using torch.expand and torch.gather\n",
        "index1_expanded = index1.expand(X_encoded.shape[0], -1)\n",
        "index2_expanded = index2.expand(X_encoded.shape[0], -1)\n",
        "print(index1_expanded.shape)\n",
        "# Efficiently gather feature pairs using indexing\n",
        "features1 = torch.gather(X_encoded, dim=1, index=index1_expanded)\n",
        "features2 = torch.gather(X_encoded, dim=1, index=index2_expanded)\n",
        "X_train = torch.stack([features1, features2], dim=2)\n",
        "Y_train = X_encoded\n",
        "dataset = list(zip(X_train,Y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLjGwJ9jYgU5"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "neighborhoods = defaultdict(list)\n",
        "for i in range(index.shape[1]):\n",
        "    n1_index = index1[i].item()\n",
        "    n2_index = index2[i].item()\n",
        "    neighborhoods[n1_index].append(i)\n",
        "    neighborhoods[n2_index].append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJWvNSnBlXWp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Bernoulli\n",
        "import random\n",
        "\n",
        "class DeepLinear(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, x, cond=None):\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DeepConv(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_output_shape, neighbourhoods):\n",
        "        super().__init__()\n",
        "        self.num_variables = 107\n",
        "        self.e_features = attr.shape[1]\n",
        "        self.input_embedding = nn.Linear(self.num_variables, hidden_dim)\n",
        "        self.attr_embedding = nn.Linear(self.e_features, hidden_dim)\n",
        "        self.e_scoring_network = DeepLinear(2 * hidden_dim, hidden_dim, output_dim=1)\n",
        "        self.neighborhood_agg_network = DeepLinear(len(neighbourhoods[0]),hidden_dim, n_output_shape)\n",
        "        self.output_dim = n_output_shape * len(neighbourhoods)\n",
        "        max_neighborhood_size = max(len(v) for v in neighbourhoods.values())\n",
        "        neighborhood_edge_indices = torch.zeros((len(neighbourhoods), max_neighborhood_size), dtype=torch.long)\n",
        "        for node_index, edge_list in neighbourhoods.items():\n",
        "          neighborhood_edge_indices[node_index] = torch.tensor(edge_list)\n",
        "        self.neighbourhoods = neighborhood_edge_indices.to(device)\n",
        "        # Dropout layers\n",
        "        self.dropout_e_scoring = nn.Dropout(0.25)\n",
        "        self.dropout_neighborhood_agg = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x.requires_grad=True\n",
        "        batch_size = x.shape[0]\n",
        "        original_x0 = x[:, :, 0]\n",
        "        original_x1 = x[:, :, 1]\n",
        "        x_mod = torch.zeros(batch_size, x.shape[1], self.num_variables,requires_grad=True).to(device)\n",
        "        index1_expanded = index1.unsqueeze(0).expand(x_mod.shape[0], -1).to(device)\n",
        "        index2_expanded = index2.unsqueeze(0).expand(x_mod.shape[0], -1).to(device)\n",
        "        x_mod = torch.scatter(x_mod, 2, index1_expanded.unsqueeze(2), original_x0.unsqueeze(2))\n",
        "        x_mod = torch.scatter(x_mod, 2, index2_expanded.unsqueeze(2), original_x1.unsqueeze(2))\n",
        "        broadcasted_attr = attr.unsqueeze(0).expand(x_mod.shape[0], -1, -1)\n",
        "        x_mod_embedded = self.input_embedding(x_mod.to(device))\n",
        "        attr_embedded = self.attr_embedding(broadcasted_attr.to(device))\n",
        "        final_tensor = torch.cat([x_mod_embedded, attr_embedded], dim=2)\n",
        "        all_edge_scores = self.e_scoring_network(final_tensor).squeeze()\n",
        "        all_edge_scores = self.dropout_e_scoring(all_edge_scores)\n",
        "        neighborhood_edge_indices = self.neighbourhoods[None, :, :]\n",
        "        neighborhood_edge_indices=neighborhood_edge_indices.expand(all_edge_scores.shape[0],-1,-1)\n",
        "        batch_size, v, r = neighborhood_edge_indices.shape\n",
        "        e = all_edge_scores.shape[1]\n",
        "        vector1_expanded = all_edge_scores.unsqueeze(1).expand(-1, v, -1)\n",
        "        neighborhood_scores = torch.gather(vector1_expanded, 2, neighborhood_edge_indices)\n",
        "        neighborhood_outputs = self.neighborhood_agg_network(neighborhood_scores)\n",
        "        neighborhood_outputs = self.dropout_neighborhood_agg(neighborhood_outputs)\n",
        "        return neighborhood_outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "\n",
        "epochs = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "conv_model = DeepConv(16,10,neighborhoods).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(conv_model.parameters(), lr=0.0001)\n",
        "dataloader = DataLoader(dataset, batch_size=3000, shuffle=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    i=0\n",
        "    start_time = time.time()\n",
        "    for (data, target) in dataloader:\n",
        "        i+=1\n",
        "        target[target==1]=10\n",
        "        data = data.to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = conv_model(data.float().to(device))\n",
        "        # Calculate the loss\n",
        "        loss = 0\n",
        "        for r in range(outputs.shape[2]):\n",
        "          loss += criterion(outputs[:,:,r], target.float().to(device))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch [{epoch+1}/{epochs}] Loss: {running_loss / i:.4f} Elapsed time (s): {end_time-start_time}')\n",
        "torch.save(conv_model,\"conv_trained.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PJuHOkyXg2Xi",
        "outputId": "573935b7-ff6f-4a51-e56b-ae0380703e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] Loss: 552698.5037 Elapsed time (s): 135.67110323905945\n",
            "Epoch [2/100] Loss: 154742.2537 Elapsed time (s): 135.07784223556519\n",
            "Epoch [3/100] Loss: 55404.1769 Elapsed time (s): 136.54048371315002\n",
            "Epoch [4/100] Loss: 22837.1646 Elapsed time (s): 134.8882405757904\n",
            "Epoch [5/100] Loss: 10137.8808 Elapsed time (s): 135.31834936141968\n",
            "Epoch [6/100] Loss: 4568.3458 Elapsed time (s): 135.51753973960876\n",
            "Epoch [7/100] Loss: 2136.1282 Elapsed time (s): 136.06355047225952\n",
            "Epoch [8/100] Loss: 1156.2320 Elapsed time (s): 134.78057885169983\n",
            "Epoch [9/100] Loss: 745.4657 Elapsed time (s): 135.4370150566101\n",
            "Epoch [10/100] Loss: 511.4358 Elapsed time (s): 134.4221911430359\n",
            "Epoch [11/100] Loss: 373.0150 Elapsed time (s): 137.2895634174347\n",
            "Epoch [12/100] Loss: 289.7714 Elapsed time (s): 136.4621982574463\n",
            "Epoch [13/100] Loss: 237.4207 Elapsed time (s): 135.18896579742432\n",
            "Epoch [14/100] Loss: 203.0696 Elapsed time (s): 136.36238980293274\n",
            "Epoch [15/100] Loss: 179.3115 Elapsed time (s): 135.54934930801392\n",
            "Epoch [16/100] Loss: 161.9497 Elapsed time (s): 136.42995643615723\n",
            "Epoch [17/100] Loss: 149.0969 Elapsed time (s): 134.84194469451904\n",
            "Epoch [18/100] Loss: 139.2626 Elapsed time (s): 134.43960237503052\n",
            "Epoch [19/100] Loss: 131.4088 Elapsed time (s): 135.14751815795898\n",
            "Epoch [20/100] Loss: 125.1270 Elapsed time (s): 135.52113962173462\n",
            "Epoch [21/100] Loss: 119.9880 Elapsed time (s): 136.12111902236938\n",
            "Epoch [22/100] Loss: 115.8105 Elapsed time (s): 136.16662526130676\n",
            "Epoch [23/100] Loss: 112.3276 Elapsed time (s): 136.2406668663025\n",
            "Epoch [24/100] Loss: 109.5718 Elapsed time (s): 136.60511374473572\n",
            "Epoch [25/100] Loss: 106.9574 Elapsed time (s): 136.13354516029358\n",
            "Epoch [26/100] Loss: 104.8281 Elapsed time (s): 135.77906370162964\n",
            "Epoch [27/100] Loss: 103.0564 Elapsed time (s): 135.15892457962036\n",
            "Epoch [28/100] Loss: 101.3789 Elapsed time (s): 137.49756360054016\n",
            "Epoch [29/100] Loss: 100.0387 Elapsed time (s): 135.72782588005066\n",
            "Epoch [30/100] Loss: 98.8299 Elapsed time (s): 136.85894346237183\n",
            "Epoch [31/100] Loss: 97.7019 Elapsed time (s): 135.55243682861328\n",
            "Epoch [32/100] Loss: 96.7878 Elapsed time (s): 137.20130920410156\n",
            "Epoch [33/100] Loss: 95.9527 Elapsed time (s): 135.86292147636414\n",
            "Epoch [34/100] Loss: 95.1616 Elapsed time (s): 135.82616138458252\n",
            "Epoch [35/100] Loss: 94.4580 Elapsed time (s): 135.4784607887268\n",
            "Epoch [36/100] Loss: 93.8463 Elapsed time (s): 135.4735143184662\n",
            "Epoch [37/100] Loss: 93.2557 Elapsed time (s): 135.74275302886963\n",
            "Epoch [38/100] Loss: 92.7446 Elapsed time (s): 137.00155639648438\n",
            "Epoch [39/100] Loss: 92.2327 Elapsed time (s): 135.11060309410095\n",
            "Epoch [40/100] Loss: 91.8085 Elapsed time (s): 135.8455822467804\n",
            "Epoch [41/100] Loss: 91.4098 Elapsed time (s): 136.676335811615\n",
            "Epoch [42/100] Loss: 91.0443 Elapsed time (s): 134.95127153396606\n",
            "Epoch [43/100] Loss: 90.7350 Elapsed time (s): 135.7829611301422\n",
            "Epoch [44/100] Loss: 90.4122 Elapsed time (s): 137.2195749282837\n",
            "Epoch [45/100] Loss: 90.0848 Elapsed time (s): 135.61410999298096\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-18724fb5858f>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "optimizer = optim.Adam(conv_model.parameters(), lr=0.0001)\n",
        "conv_model = torch.load(\"conv_trained (2).pth\")\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    i=0\n",
        "    start_time = time.time()\n",
        "    for (data, target) in dataloader:\n",
        "        i+=1\n",
        "        data = data.to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = conv_model(data.float().to(device))\n",
        "        # Calculate the loss\n",
        "        loss = 0\n",
        "        for r in range(outputs.shape[2]):\n",
        "          loss += criterion(outputs[:,:,r], target.float().to(device))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch [{epoch+1}/{epochs}] Loss: {running_loss / i:.4f} Elapsed time (s): {end_time-start_time}')\n",
        "torch.save(conv_model,\"conv_trained.pth\")"
      ],
      "metadata": {
        "id": "wvfaLom0diAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d64030-403d-49ef-f8f1-a591d2a3c819"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200] Loss: 1.5997 Elapsed time (s): 135.9868836402893\n",
            "Epoch [2/200] Loss: 1.5833 Elapsed time (s): 136.734454870224\n",
            "Epoch [3/200] Loss: 1.5935 Elapsed time (s): 135.1617739200592\n",
            "Epoch [4/200] Loss: 1.5999 Elapsed time (s): 135.95690083503723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGIG2eEiRQPJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "hidden_layers = [2048,1024,1024,2048]\n",
        "L = len(hidden_layers)\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn.utils as nn_utils\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "class GreaterThanFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, a, b):\n",
        "        return (a>b).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "      toReturn1= grad_output\n",
        "      toReturn2 = -1 * grad_output\n",
        "      return toReturn1, toReturn2\n",
        "\n",
        "\n",
        "from torch import autograd, nn\n",
        "\n",
        "def energy(v, *params):\n",
        "  h = params[:L]\n",
        "  weight = params[L:L*2]\n",
        "  bias = params[L*2:]\n",
        "  energy = - torch.sum(v * bias[0].unsqueeze(0), 1)\n",
        "  for i in range(L):\n",
        "      logits = F.linear(v if i==0 else h[i-1], weight[i], bias[i+1])\n",
        "      energy -= torch.sum(h[i] * logits, 1)\n",
        "  return energy\n",
        "\n",
        "\n",
        "class MHStepFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, v, fix_v, rand_v, rand_h, rand_u, *params):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "        h = params[:L]\n",
        "        weight = params[L:L*2]\n",
        "        bias = params[L*2:]\n",
        "        fix_v = fix_v==1.0\n",
        "        temp = []\n",
        "        for x in [rand_v,rand_h,rand_u]:\n",
        "          if isinstance(x, torch.Tensor) and x.numel() == 1 and x.item() == 0.0:\n",
        "            temp.append(None)\n",
        "          else:\n",
        "            temp.append(x)\n",
        "        rand_v = temp[0]\n",
        "        rand_h = temp[1]\n",
        "        rand_u = temp[2]\n",
        "        ctxs = []\n",
        "        if fix_v:\n",
        "            v_ = v\n",
        "        else:\n",
        "            if rand_v is None:\n",
        "                v_ = torch.empty_like(v).bernoulli_()\n",
        "            else:\n",
        "                v_ = (rand_v < 0.5).float()\n",
        "\n",
        "        if rand_h is None:\n",
        "            h_ = [torch.empty_like(h[i]).bernoulli_() for i in range(L)]\n",
        "        else:\n",
        "            h_ = [(rand_h[i] < 0.5).float() for i in range(L)]\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "            params.append(tensor)\n",
        "        for parameter in weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in bias:\n",
        "            params.append(parameter)\n",
        "        energy1 = energy(v,*params)\n",
        "        energy1CTX = tuple((v, *params))\n",
        "        ctxs.append(energy1CTX)\n",
        "        params = []\n",
        "        for tensor in h_:\n",
        "            params.append(tensor)\n",
        "        for parameter in weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in bias:\n",
        "            params.append(parameter)\n",
        "        energy2 = energy(v_,*params)\n",
        "        energy2CTX = tuple((v_, *params))\n",
        "        ctxs.append(energy2CTX)\n",
        "        log_ratio = energy1 - energy2\n",
        "        toSave = []\n",
        "        toSave.append(log_ratio)\n",
        "        if rand_u is None:\n",
        "            input1 = torch.clamp(log_ratio.exp().unsqueeze(1),0,1)\n",
        "            random_numbers = torch.rand(input1.shape,device=device).float()\n",
        "            accepted = (input1>=random_numbers)\n",
        "        else:\n",
        "            accepted = (log_ratio.exp().unsqueeze(1)>=rand_u.unsqueeze(1))\n",
        "        if not fix_v:\n",
        "            toSave.append(v_)\n",
        "            toSave.append(v)\n",
        "        for i in range(L):\n",
        "          toSave.append(h_[i])\n",
        "          toSave.append(h[i])\n",
        "\n",
        "        params = []\n",
        "        for parameter in weight:\n",
        "          params.append(parameter)\n",
        "        for parameter in bias:\n",
        "          params.append(parameter)\n",
        "        for sav in toSave:\n",
        "          params.append(sav)\n",
        "        for lis in ctxs:\n",
        "          params.append(torch.tensor(len(lis)))\n",
        "          params.extend(lis)\n",
        "        savLength = torch.tensor(len(toSave))\n",
        "        if not fix_v:\n",
        "            v = torch.where(accepted, v_, v)\n",
        "        h = [torch.where(accepted, h_[i], h[i]) for i in range(L)]\n",
        "        if rand_u is None:\n",
        "          rand_u = torch.tensor(0.0)\n",
        "        fix_v = torch.tensor(float(fix_v))\n",
        "        accepted = accepted.float()\n",
        "        ctx.save_for_backward(accepted, fix_v, rand_u,savLength, *params)\n",
        "        return v, *h\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_v, *grad_h):\n",
        "        grad_v = torch.clamp(grad_v, -10, 10)\n",
        "        for gr_h in grad_h:\n",
        "          gr_h = torch.clamp(gr_h, -10, 10)\n",
        "        accepted, fix_v, rand_u,savLength, *params = ctx.saved_tensors\n",
        "        if len(rand_u.shape)==0 :\n",
        "          rand_u = None\n",
        "        fix_v = fix_v==1.0\n",
        "        accepted = accepted.bool()\n",
        "        weight = params[:L]\n",
        "        bias = params[L:L*2+1]\n",
        "        toSave = params[L*2+1:L*2+1+savLength.item()]\n",
        "        ctxTensors = params[L*2+1+savLength.item():]\n",
        "        ctxTuples = []\n",
        "        i = 0\n",
        "        while i < len(ctxTensors):\n",
        "            tuple_length = ctxTensors[i].item()\n",
        "            start = i + 1  # Start index of tuple elements\n",
        "            end = start + tuple_length\n",
        "            ctxTuples.append(tuple(ctxTensors[start:end]))\n",
        "            i = end\n",
        "        ctx1 = ctxTuples[0]\n",
        "        ctx2 = ctxTuples[1]\n",
        "        grad_h = list(grad_h)\n",
        "        grad_weight = [torch.zeros_like(w) for w in weight]\n",
        "        grad_bias = [torch.zeros_like(b) for b in bias]\n",
        "        toSave = list(toSave)\n",
        "        if not fix_v:\n",
        "          d_accepted = torch.sum((toSave[1]-toSave[2]) * grad_v, dim=1, keepdim=True)\n",
        "          for i in range(len(grad_h)):\n",
        "            d_accepted = d_accepted + torch.sum((toSave[3+i*2]-toSave[4+(i*2)]) * grad_h[i], dim=1, keepdim=True)\n",
        "        else:\n",
        "          d_accepted = torch.sum((toSave[1]-toSave[2]) * grad_h[0], dim=1, keepdim=True)\n",
        "          for i in range(1,len(grad_h)):\n",
        "            d_accepted = d_accepted +torch.sum((toSave[1+i*2]-toSave[2+(i*2)]) * grad_h[i], dim=1, keepdim=True)\n",
        "        log_ratio = toSave[0].detach().requires_grad_()\n",
        "        if rand_u is None:\n",
        "            d_log_ratio_exp = d_accepted\n",
        "        else:\n",
        "            d_log_ratio_exp = d_accepted\n",
        "        log_ratio_exp= log_ratio.exp().unsqueeze(1).detach().requires_grad_()\n",
        "        log_ratio_exp = torch.clamp(log_ratio_exp, -100, 100)\n",
        "        d_log_ratio = d_log_ratio_exp * log_ratio_exp\n",
        "        d_log_ratio = torch.clamp(d_log_ratio, -10, 10)\n",
        "        with torch.enable_grad():\n",
        "            v1 = ctx1[0].detach().requires_grad_()\n",
        "            params1 = ctx1[1:]\n",
        "            params1 = [item.detach().requires_grad_() for item in params1]\n",
        "            input = (v1, *params1)\n",
        "            energy8 = energy(*input)\n",
        "            if d_log_ratio.shape[0]==1:\n",
        "              d_log_ratio= d_log_ratio.squeeze(1)\n",
        "            else:\n",
        "              d_log_ratio=d_log_ratio.squeeze()\n",
        "            v1, *params1 = autograd.grad(energy8, input, d_log_ratio)\n",
        "        params1 = list(params1)\n",
        "        h1 = params1[:L]\n",
        "        weight1 = params1[L:L*2]\n",
        "        bias1 = params1[L*2:]\n",
        "        with torch.enable_grad():\n",
        "            v2 = ctx2[0].detach().requires_grad_()\n",
        "            params2 = ctx2[1:]\n",
        "            params2 = [item.detach().requires_grad_() for item in params2]\n",
        "            input = (v2, *params2)\n",
        "            energy8 = energy(*input)\n",
        "            v2, *params2 = autograd.grad(energy8, input, -1*d_log_ratio)\n",
        "        params2 = list(params2)\n",
        "        weight2 = params2[L:L*2]\n",
        "        bias2 = params2[L*2:]\n",
        "        if not fix_v:\n",
        "            grad_v = torch.where(accepted,0,grad_v)\n",
        "        for i in range(len(grad_h)):\n",
        "            grad_h[i] = torch.where(accepted,0,grad_h[i])\n",
        "        grad_v += v1\n",
        "        for i in range(len(grad_h)):\n",
        "          grad_h[i]+=h1[i]\n",
        "        for i in range(len(grad_weight)):\n",
        "          grad_weight[i] += (weight1[i]-weight2[i])\n",
        "        for i in range(len(grad_bias)):\n",
        "          grad_bias[i] += (bias1[i]-bias2[i])\n",
        "        grads = []\n",
        "        grad_v = torch.clamp(grad_v, -10, 10)\n",
        "        for gr_h in grad_h:\n",
        "          gr_h = torch.clamp(gr_h, -10, 10)\n",
        "        for tensor in grad_h:\n",
        "            grads.append(tensor)\n",
        "        for parameter in grad_weight:\n",
        "          grads.append(parameter)\n",
        "        for parameter in grad_bias:\n",
        "          grads.append(parameter)\n",
        "\n",
        "        return grad_v, None, None, None, None, *grads\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "from torch.autograd import Function\n",
        "\n",
        "class GibbsStepFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, v,fix_v, rand_v, rand_h, rand_u, rand_z, T, *params):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "        fix_v= fix_v==1.0\n",
        "        params = list(params)\n",
        "        h = params[:L]\n",
        "        weight = params[L:L*2]\n",
        "        bias = params[L*2:]\n",
        "        temp = []\n",
        "        for x in [rand_v,rand_h,rand_u,rand_z]:\n",
        "          if isinstance(x, torch.Tensor) and x.numel() == 1 and x.item() == 0.0:\n",
        "            temp.append(None)\n",
        "          else:\n",
        "            temp.append(x.to(device))\n",
        "        rand_v = temp[0]\n",
        "        rand_h = temp[1]\n",
        "        rand_u = temp[2]\n",
        "        rand_z = temp[3]\n",
        "        if rand_u is None:\n",
        "            rand_u = torch.rand(N, device=device)\n",
        "        even = rand_u < 0.5\n",
        "        odd = even.logical_not()\n",
        "        toSave = []\n",
        "        toSaveID = []\n",
        "        if even.sum() > 0:\n",
        "            if not fix_v:\n",
        "                logits = F.linear(h[0][even],\n",
        "                                  weight[0].t(), bias[0])\n",
        "                toSaveID.append(15)\n",
        "                toSave.append(h[0][even])\n",
        "\n",
        "                if T == 0:\n",
        "                    sample = (logits >= 0).float()\n",
        "                    v = torch.scatter(v, 0, even.nonzero().repeat(1,v.shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(14)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(logits)\n",
        "                    if rand_v is None:\n",
        "                        random_numbers = torch.rand( sigLogits.shape,device=device).float()\n",
        "                        sample = (sigLogits>=random_numbers).float()\n",
        "                        v =  torch.scatter(v,0,even.nonzero().repeat(1,v.shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = (sigLogits>=rand_v[even]).float()\n",
        "                        v = torch.scatter(v,0,even.nonzero().repeat(1,v.shape[1]),sample)\n",
        "\n",
        "            for i in range(1, len(h), 2):\n",
        "                logits = F.linear(h[i-1][even], weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][even], weight[i+1].t(), None)\n",
        "                    toSaveID.append(12)\n",
        "                    toSave.append(h[i+1][even])\n",
        "\n",
        "                toSaveID.append(13)\n",
        "                toSave.append(h[i-1][even])\n",
        "                if T == 0:\n",
        "                    sample = (logits>=0).float()\n",
        "                    h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(11)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(logits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.rand( sigLogits.shape,device=device).float()\n",
        "                        sample = (sigLogits>=random_numbers).float()\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = (sigLogits>=rand_h[i][even]).float()\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "\n",
        "            for i in range(0, len(h), 2):\n",
        "                logits = F.linear(v[even] if i==0 else h[i-1][even],\n",
        "                                  weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][even], weight[i+1].t(), None)\n",
        "                    toSaveID.append(9)\n",
        "                    toSave.append(h[i+1][even])\n",
        "\n",
        "                toSaveID.append(10)\n",
        "                toSave.append(v[even] if i==0 else h[i-1][even])\n",
        "                if T == 0:\n",
        "                    sample = (logits>=0).float()\n",
        "                    h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSaveID.append(8)\n",
        "                    toSave.append(logits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.rand(sigLogits.shape,device=device).float()\n",
        "                        sample = (sigLogits>=random_numbers).float()\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = (sigLogits>=rand_h[i][even]).float()\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "        if odd.sum() > 0:\n",
        "            for i in range(0, len(h), 2):\n",
        "                logits = F.linear(v[odd] if i==0 else h[i-1][odd], weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][odd], weight[i+1].t(), None)\n",
        "                    toSaveID.append(6)\n",
        "                    toSave.append(h[i+1][odd])\n",
        "\n",
        "                toSaveID.append(7)\n",
        "                toSave.append(v[odd] if i==0 else h[i-1][odd])\n",
        "                if T == 0:\n",
        "                    sample = (logits>=0).float()\n",
        "                    h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(5)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(logits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.rand( sigLogits.shape,device=device).float()\n",
        "                        sample = (sigLogits>=random_numbers).float()\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = (sigLogits>=rand_h[i][odd]).float()\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "\n",
        "            if not fix_v:\n",
        "                logits = F.linear(h[0][odd], weight[0].t(), bias[0])\n",
        "                toSaveID.append(4)\n",
        "                toSave.append(h[0][odd])\n",
        "                if T == 0:\n",
        "                    sample = (logits>=0.00).float()\n",
        "                    v = torch.scatter(v, 0, odd.nonzero().repeat(1,v.shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(3)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(logits)\n",
        "                    if rand_v is None:\n",
        "                        random_numbers = torch.rand( sigLogits.shape,device=device).float()\n",
        "                        sample = (sigLogits>=random_numbers).float()\n",
        "                        v = torch.scatter(v,0,odd.nonzero().repeat(1,v.shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = (sigLogits>=rand_v[odd]).float()\n",
        "                        v = torch.scatter(v,0,odd.nonzero().repeat(1,v.shape[1]),sample)\n",
        "\n",
        "            for i in range(1, len(h), 2):\n",
        "                logits = F.linear(h[i-1][odd], weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][odd], weight[i+1].t(), None)\n",
        "                    toSaveID.append(1)\n",
        "                    toSave.append(h[i+1][odd])\n",
        "                toSaveID.append(2)\n",
        "                toSave.append(h[i-1][odd])\n",
        "                if T == 0:\n",
        "                    sample = (logits>=0).float()\n",
        "                    h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSaveID.append(0)\n",
        "                    toSave.append(logits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.rand( sigLogits.shape,device=device).float()\n",
        "                        sample = (sigLogits>=random_numbers).float()\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                    else:\n",
        "                        sample = (sigLogits>=rand_h[i][odd]).float()\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "          params.append(tensor)\n",
        "        for parameter in weight:\n",
        "          params.append(parameter)\n",
        "        for parameter in bias:\n",
        "          params.append(parameter)\n",
        "        for sav in toSave:\n",
        "          params.append(sav)\n",
        "        toSaveID = torch.tensor(toSaveID)\n",
        "        ctx.save_for_backward(v,even, odd,torch.tensor(fix_v), rand_v, rand_h, rand_u, rand_z, torch.tensor(T),toSaveID, *params)\n",
        "        return v,  *h\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_v, *grad_h):\n",
        "        grad_v = torch.clamp(grad_v, -10, 10)\n",
        "        for gr_h in grad_h:\n",
        "          gr_h = torch.clamp(gr_h, -10, 10)\n",
        "        v,even, odd,fix_v, rand_v, rand_h, rand_u, rand_z, T, toSaveID, *params = ctx.saved_tensors\n",
        "        params = list(params)\n",
        "        h = params[:L]\n",
        "        weight = params[L:L*2]\n",
        "        bias = params[L*2:L*3+1]\n",
        "        toSave = params[L*3+1:]\n",
        "        grad_v2return = torch.zeros_like(grad_v)\n",
        "        h2return = []\n",
        "        for h5 in grad_h:\n",
        "          h2return.append(torch.zeros_like(h5))\n",
        "        grad_weight = []\n",
        "        grad_bias = []\n",
        "        for i in range(L):\n",
        "            grad_weight.append(torch.zeros_like(weight[i]))\n",
        "        for i in range(L+1):\n",
        "            grad_bias.append(torch.zeros_like(bias[i]))\n",
        "        even_v = v[even]\n",
        "        odd_v = v[odd]\n",
        "        even_h = []\n",
        "        odd_h = []\n",
        "        grad_h = list(grad_h)\n",
        "        for gh in grad_h:\n",
        "          even_h.append(gh[even])\n",
        "        for gh in grad_h:\n",
        "          odd_h.append(gh[odd])\n",
        "        h = list(h)\n",
        "        toSaveID2 = []\n",
        "        for tsID in list(toSaveID):\n",
        "          toSaveID2.append(int(tsID))\n",
        "        toSaveID = toSaveID2\n",
        "\n",
        "\n",
        "        toSave = reversed(toSave)\n",
        "        toSaveID = reversed(toSaveID)\n",
        "        save_queues = defaultdict(deque)\n",
        "        for obj, category_id in zip(toSave, toSaveID):\n",
        "                save_queues[category_id].appendleft(obj)\n",
        "\n",
        "        if odd.sum() > 0:\n",
        "          for i in reversed(range(1, len(h), 2)):\n",
        "            if T==0:\n",
        "              d_logits = odd_h[i]\n",
        "            else:\n",
        "              d_logitsSig =  odd_h[i]\n",
        "              input = save_queues[0].pop().detach().requires_grad_()\n",
        "              d_logits = d_logitsSig * torch.sigmoid(input)*(1-torch.sigmoid(input))\n",
        "              d_logits = d_logits/T\n",
        "            d_logits = torch.clamp(d_logits, -10, 10)\n",
        "            if i+1<len(h):\n",
        "              input1 = save_queues[1].pop()\n",
        "              grad_weight[i+1] += (d_logits.t() @ input1).t()\n",
        "              odd_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            grad_weight[i] += d_logits.t() @ save_queues[2].pop()\n",
        "            odd_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "          if not fix_v:\n",
        "            if T==0:\n",
        "              d_logits = odd_v\n",
        "            else:\n",
        "              d_logitsSig =  odd_v\n",
        "              input = save_queues[3].pop()\n",
        "              d_logits = d_logitsSig * torch.sigmoid(input)*(1-torch.sigmoid(input))\n",
        "              d_logits = d_logits/T\n",
        "            d_logits = torch.clamp(d_logits, -10, 10)\n",
        "            grad_weight[0] += (d_logits.t() @ save_queues[4].pop()).t()\n",
        "            odd_h[0] += d_logits @ weight[0].t()\n",
        "            grad_bias[0] += d_logits.sum(0)\n",
        "          for i in reversed(range(0,len(h),2)):\n",
        "            if T==0:\n",
        "              d_logits =  odd_h[i]\n",
        "            else:\n",
        "              d_logitsSig = odd_h[i]\n",
        "              input = save_queues[5].pop()\n",
        "              d_logits = d_logitsSig * torch.sigmoid(input)*(1-torch.sigmoid(input))\n",
        "              d_logits = d_logits/T\n",
        "            d_logits = torch.clamp(d_logits, -10, 10)\n",
        "            if i+1 < len(h):\n",
        "              grad_weight[i+1] += (d_logits.t() @ save_queues[6].pop()).t()\n",
        "              odd_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            temp = save_queues[7].pop()\n",
        "            grad_weight[i] += d_logits.t() @ temp\n",
        "            if i==0:\n",
        "              odd_v += d_logits @ weight[i]\n",
        "            else:\n",
        "              odd_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "\n",
        "        if even.sum() > 0:\n",
        "          for i in reversed(range(0, len(h), 2)):\n",
        "            if T==0:\n",
        "              d_logits =  even_h[i]\n",
        "            else:\n",
        "              d_logitsSig =  even_h[i]\n",
        "              input = save_queues[8].pop()\n",
        "              d_logits = d_logitsSig * torch.sigmoid(input)*(1-torch.sigmoid(input))\n",
        "              d_logits = d_logits/T\n",
        "            d_logits = torch.clamp(d_logits, -10, 10)\n",
        "            if i+1<len(h):\n",
        "              grad_weight[i+1] += (d_logits.t() @ save_queues[9].pop()).t()\n",
        "              even_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            grad_weight[i] += d_logits.t() @ save_queues[10].pop()\n",
        "            if i==0:\n",
        "              even_v += d_logits @ weight[i]\n",
        "            else:\n",
        "              even_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "          for i in reversed(range(1, len(h), 2)):\n",
        "            if T==0:\n",
        "              d_logits =  even_h[i]\n",
        "            else:\n",
        "              d_logitsSig = even_h[i]\n",
        "              input = save_queues[11].pop()\n",
        "              d_logits =  d_logitsSig * torch.sigmoid(input)*(1-torch.sigmoid(input))\n",
        "              d_logits = d_logits/T\n",
        "            d_logits = torch.clamp(d_logits, -10, 10)\n",
        "            if i+1<len(h):\n",
        "              grad_weight[i+1] += (d_logits.t() @ save_queues[12].pop()).t()\n",
        "              even_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            grad_weight[i] += d_logits.t() @ save_queues[13].pop()\n",
        "            even_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "          if not fix_v:\n",
        "            if T==0:\n",
        "              d_logits = even_v\n",
        "            else:\n",
        "              d_logitsSig = even_v\n",
        "              input = save_queues[14].pop()\n",
        "              d_logits = d_logitsSig * torch.sigmoid(input)*(1-torch.sigmoid(input))\n",
        "              d_logits = d_logits/T\n",
        "            d_logits = torch.clamp(d_logits, -10, 10)\n",
        "            grad_weight[0] += (d_logits.t() @ save_queues[15].pop()).t()\n",
        "            even_h[0] += d_logits @ weight[0].t()\n",
        "            grad_bias[0] += d_logits.sum(0)\n",
        "        grad_v2return[even] = even_v\n",
        "        grad_v2return[odd] = odd_v\n",
        "        grad_h2return = []\n",
        "        for ind, h7 in enumerate(h2return):\n",
        "          h7[even] = even_h[ind]\n",
        "          h7[odd] = odd_h[ind]\n",
        "          grad_h2return.append(h7)\n",
        "        grads = []\n",
        "        for tensor in grad_h2return:\n",
        "            grads.append(tensor)\n",
        "        for parameter in grad_weight:\n",
        "          grads.append(parameter)\n",
        "        for parameter in grad_bias:\n",
        "          grads.append(parameter)\n",
        "        return grad_v2return, None, None, None, None, None, None, *grads\n",
        "\n",
        "\n",
        "class DBM(nn.Module):\n",
        "    def __init__(self, nv, hidden_layers,comparator=False):\n",
        "        super().__init__()\n",
        "        self.input_layer = torch.load(\"conv_trained.pth\")\n",
        "        self.weight = nn.ParameterList([nn.Parameter(torch.Tensor(hidden_layers[0], nv))])\n",
        "        for i in range(len(hidden_layers)-1):\n",
        "          self.weight.append(nn.Parameter(torch.Tensor(hidden_layers[i+1], hidden_layers[i])))\n",
        "        self.bias = nn.ParameterList([nn.Parameter(torch.Tensor(nv))])\n",
        "        for i in range(len(hidden_layers)):\n",
        "          self.bias.append(nn.Parameter(torch.Tensor(hidden_layers[i])))\n",
        "\n",
        "        self.nv = nv\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.L = len(hidden_layers)\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.dummy = torch.tensor(0.0).requires_grad_()\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for w in self.weight:\n",
        "            nn.init.orthogonal_(w)\n",
        "\n",
        "        for b in self.bias:\n",
        "            nn.init.zeros_(b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(\"x: \",x)\n",
        "        input = self.input_layer(x)\n",
        "        N = input.size(0)\n",
        "        device = x.device\n",
        "        v = input.clone().detach()\n",
        "        assert self.L != 1\n",
        "        energy_pos_samples = self.positive_phase(N, v)\n",
        "\n",
        "        energy_neg_samples = self.negative_phase(10, N)\n",
        "        pos_energy = torch.mean(torch.stack(energy_pos_samples))\n",
        "       # print(\"energy_pos: \",pos_energy)\n",
        "        neg_energy = torch.mean(torch.stack(energy_neg_samples))\n",
        "       # print(\"energy_neg: \",neg_energy)\n",
        "        energy_loss = pos_energy - neg_energy\n",
        "        outputs = []\n",
        "        output_rands = []\n",
        "        for r in range(input.shape[2]):\n",
        "          output, output_rand = self.reconstruct(input[:,:,r])\n",
        "          outputs.append(output)\n",
        "          output_rands.append(output_rand)\n",
        "        return energy_loss, outputs, output_rands\n",
        "    def positive_phase(self, N, v):\n",
        "      energy_pos_samples = []  # Store energy samples\n",
        "      v2 = v\n",
        "      print(\"v: \",v)\n",
        "      for r in range(v2.shape[2]):\n",
        "        h = []\n",
        "        for i in range(self.L):\n",
        "          h_i = torch.full((N, self.hidden_layers[i]), 0.5, device=self.device,requires_grad=True)\n",
        "          h_i = self.bernoulli_sample(h_i)\n",
        "          h.append(h_i)\n",
        "        v, h = self.local_search(v2[:,:,r], h, True)\n",
        "        v, h = self.gibbs_step(v, h, True)\n",
        "        energy_pos = self.coupling(v, h, True)\n",
        "        energy_pos_samples.append(energy_pos)\n",
        "      return energy_pos_samples\n",
        "\n",
        "    def negative_phase(self, num_samples, N):\n",
        "      energy_neg_samples = []\n",
        "      for _ in range(num_samples):\n",
        "        v = self.bernoulli_sample(torch.full((N, self.nv), 0.5, device=self.device, requires_grad=True))\n",
        "        h = []\n",
        "        for i in range(self.L):\n",
        "            probs = torch.full((N, self.hidden_layers[i]), 0.5, device=self.device, requires_grad=True)\n",
        "            h_i = self.bernoulli_sample(probs)\n",
        "            h.append(h_i)\n",
        "        v, h = self.local_search(v, h)\n",
        "        v, h = self.gibbs_step(v, h)\n",
        "        energy_neg = self.coupling(v, h)\n",
        "        energy_neg_samples.append(energy_neg)\n",
        "      return energy_neg_samples\n",
        "\n",
        "\n",
        "    def local_search(self, v, h, fix_v=False):\n",
        "        N = v.size(0)\n",
        "        device= v.device\n",
        "        _v = v.clone()\n",
        "        _h = []\n",
        "        for r in h:\n",
        "          _h.append(r.clone())\n",
        "        rand_u = torch.rand(N, device=device)\n",
        "        v, h = self.gibbs_step(v, h, fix_v, rand_u=rand_u, T=0)\n",
        "        converged = torch.ones(N, dtype=torch.bool, device=device) if fix_v \\\n",
        "                    else torch.all(v == _v, 1)\n",
        "        for i in range(self.L):\n",
        "            converged = converged.logical_and(torch.all(h[i] == _h[i], 1))\n",
        "        count = 0\n",
        "        while not converged.all():\n",
        "            count+=1\n",
        "            not_converged = converged.logical_not()\n",
        "            _v = v[not_converged]\n",
        "            _h = [h[i][not_converged] for i in range(self.L)]\n",
        "            M = _v.size(0)\n",
        "            v_, h_ = self.gibbs_step(_v, _h, fix_v,\n",
        "                                     rand_u=rand_u[not_converged], T=0)\n",
        "            if fix_v:\n",
        "                converged_ = torch.ones(M, dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                converged_ = torch.all(v_ == _v, 1)\n",
        "                v = torch.scatter(v,0,not_converged.nonzero().repeat(1,v.shape[1]), v_)\n",
        "            for i in range(self.L):\n",
        "                converged_ = converged_.logical_and(torch.all(h_[i] == _h[i], 1))\n",
        "                h[i] = torch.scatter(h[i], 0, not_converged.nonzero().repeat(1,h_[i].shape[1]), h_[i])\n",
        "            converged[not_converged] = converged_\n",
        "        return v, h\n",
        "\n",
        "    def coupling(self, v, h, fix_v=False):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "        _v = v.clone()\n",
        "        _h = []\n",
        "        for r in h:\n",
        "          _h.append(r.clone())\n",
        "        v, h = self.mh_step(v, h, fix_v)\n",
        "        energy = self.energy(v, h)\n",
        "        if fix_v:\n",
        "          converged = torch.ones(N, dtype=torch.bool, device=device)\n",
        "        else:\n",
        "          converged = torch.all(v == _v, 1)\n",
        "        for i in range(self.L):\n",
        "            converged = converged.logical_and(torch.all(h[i] == _h[i], 1))\n",
        "        while not converged.all():\n",
        "            not_converged = converged.logical_not()\n",
        "            _v = v[not_converged]\n",
        "            _h = [h[i][not_converged] for i in range(self.L)]\n",
        "            M = _v.size(0)\n",
        "            rand_v = None if fix_v else torch.rand_like(_v)\n",
        "            rand_h = [torch.rand_like(_h[i]) for i in range(self.L)]\n",
        "            rand_u = torch.rand(M, device=device)\n",
        "            v_, h_ = self.mh_step(_v, _h, fix_v, rand_v, rand_h, rand_u)\n",
        "            aaa = self.energy(v_, h_)\n",
        "            bbb = self.energy(_v, _h)\n",
        "            energy[not_converged] = energy[not_converged] + (aaa - bbb)\n",
        "            if fix_v:\n",
        "                converged_ = torch.ones(M, dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                converged_ = torch.all(v_ == _v, 1)\n",
        "                v = torch.scatter(v,0,not_converged.nonzero().repeat(1,v.shape[1]), v_)\n",
        "            for i in range(self.L):\n",
        "                converged_ = converged_.logical_and(torch.all(h_[i] == _h[i], 1))\n",
        "                h[i] = torch.scatter(h[i], 0, not_converged.nonzero().repeat(1,h_[i].shape[1]), h_[i])\n",
        "            converged[not_converged] = converged_\n",
        "        return energy\n",
        "\n",
        "    def energy(self, v, h):\n",
        "        energy = - torch.sum(v * self.bias[0].unsqueeze(0), 1)\n",
        "        for i in range(self.L):\n",
        "            logits = F.linear(v if i==0 else h[i-1], self.weight[i], self.bias[i+1])\n",
        "\n",
        "            energy = energy - torch.sum(h[i] * logits, 1)\n",
        "        return energy\n",
        "\n",
        "    def bernoulli_sample(self,probabilities):\n",
        "      random_numbers = torch.rand(probabilities.shape,device=self.device)\n",
        "      return GreaterThanFunction.apply(probabilities, random_numbers)\n",
        "\n",
        "    def gibbs_step(self, v, h, fix_v=False, rand_v=None, rand_h=None, rand_u=None, rand_z=None, T=1):\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "            params.append(tensor)\n",
        "        for parameter in self.weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in self.bias:\n",
        "            params.append(parameter)\n",
        "        if rand_v is None:\n",
        "          rand_v = self.dummy\n",
        "        if rand_h is None:\n",
        "          rand_h = self.dummy\n",
        "        if rand_u is None:\n",
        "          rand_u = self.dummy\n",
        "        if rand_z is None:\n",
        "          rand_z = self.dummy\n",
        "        v, *h = GibbsStepFunction.apply(v, torch.tensor(float(fix_v)).requires_grad_(), rand_v,rand_h, rand_u, rand_z, torch.tensor(float(T)).requires_grad_(), *params)\n",
        "        return v,h\n",
        "\n",
        "\n",
        "    def mh_step(self, v, h, fix_v=False, rand_v=None, rand_h=None, rand_u=None):\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "            params.append(tensor)\n",
        "        for parameter in self.weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in self.bias:\n",
        "            params.append(parameter)\n",
        "        if rand_v is None:\n",
        "          rand_v = self.dummy\n",
        "        if rand_h is None:\n",
        "          rand_h = self.dummy\n",
        "        if rand_u is None:\n",
        "          rand_u = self.dummy\n",
        "        v, *h = MHStepFunction.apply( v,torch.tensor(float(fix_v)).requires_grad_(),rand_v,rand_h,rand_u,*params)\n",
        "        return v, h\n",
        "\n",
        "    def reconstruct(self, v):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "\n",
        "        h = [torch.empty(N, layer, device=device).bernoulli_() for layer in self.hidden_layers]\n",
        "\n",
        "        v, h = self.local_search(v, h, True)\n",
        "        v_mode, h_mode = self.gibbs_step(v, h, T=0)\n",
        "        v_rand, h_rand = self.gibbs_step(v, h)\n",
        "\n",
        "        return v_mode, v_rand\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import reduce\n",
        "from copy import deepcopy\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "\n",
        "def is_legal(v):\n",
        "    \"\"\"\n",
        "    Checks that tensor is not NaN or Inf.\n",
        "\n",
        "    Inputs:\n",
        "        v (tensor): tensor to be checked\n",
        "\n",
        "    \"\"\"\n",
        "    legal = not torch.isnan(v).any() and not torch.isinf(v)\n",
        "\n",
        "    return legal\n",
        "\n",
        "\n",
        "def polyinterp(points, x_min_bound=None, x_max_bound=None, plot=False):\n",
        "    \"\"\"\n",
        "    Gives the minimizer and minimum of the interpolating polynomial over given points\n",
        "    based on function and derivative information. Defaults to bisection if no critical\n",
        "    points are valid.\n",
        "\n",
        "    Based on polyinterp.m Matlab function in minFunc by Mark Schmidt with some slight\n",
        "    modifications.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 12/6/18.\n",
        "\n",
        "    Inputs:\n",
        "        points (nparray): two-dimensional array with each point of form [x f g]\n",
        "        x_min_bound (float): minimum value that brackets minimum (default: minimum of points)\n",
        "        x_max_bound (float): maximum value that brackets minimum (default: maximum of points)\n",
        "        plot (bool): plot interpolating polynomial\n",
        "\n",
        "    Outputs:\n",
        "        x_sol (float): minimizer of interpolating polynomial\n",
        "        F_min (float): minimum of interpolating polynomial\n",
        "\n",
        "    Note:\n",
        "      . Set f or g to np.nan if they are unknown\n",
        "\n",
        "    \"\"\"\n",
        "    no_points = points.shape[0]\n",
        "    order = np.sum(1 - np.isnan(points[:, 1:3]).astype('int')) - 1\n",
        "\n",
        "    x_min = np.min(points[:, 0])\n",
        "    x_max = np.max(points[:, 0])\n",
        "\n",
        "    # compute bounds of interpolation area\n",
        "    if x_min_bound is None:\n",
        "        x_min_bound = x_min\n",
        "    if x_max_bound is None:\n",
        "        x_max_bound = x_max\n",
        "\n",
        "    # explicit formula for quadratic interpolation\n",
        "    if no_points == 2 and order == 2 and plot is False:\n",
        "        # Solution to quadratic interpolation is given by:\n",
        "        # a = -(f1 - f2 - g1(x1 - x2))/(x1 - x2)^2\n",
        "        # x_min = x1 - g1/(2a)\n",
        "        # if x1 = 0, then is given by:\n",
        "        # x_min = - (g1*x2^2)/(2(f2 - f1 - g1*x2))\n",
        "\n",
        "        if points[0, 0] == 0:\n",
        "            x_sol = -points[0, 2] * points[1, 0] ** 2 / (2 * (points[1, 1] - points[0, 1] - points[0, 2] * points[1, 0]))\n",
        "        else:\n",
        "            a = -(points[0, 1] - points[1, 1] - points[0, 2] * (points[0, 0] - points[1, 0])) / (points[0, 0] - points[1, 0]) ** 2\n",
        "            x_sol = points[0, 0] - points[0, 2]/(2*a)\n",
        "\n",
        "        x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n",
        "\n",
        "    # explicit formula for cubic interpolation\n",
        "    elif no_points == 2 and order == 3 and plot is False:\n",
        "        # Solution to cubic interpolation is given by:\n",
        "        # d1 = g1 + g2 - 3((f1 - f2)/(x1 - x2))\n",
        "        # d2 = sqrt(d1^2 - g1*g2)\n",
        "        # x_min = x2 - (x2 - x1)*((g2 + d2 - d1)/(g2 - g1 + 2*d2))\n",
        "        d1 = points[0, 2] + points[1, 2] - 3 * ((points[0, 1] - points[1, 1]) / (points[0, 0] - points[1, 0]))\n",
        "        d2 = np.sqrt(d1 ** 2 - points[0, 2] * points[1, 2])\n",
        "        if np.isreal(d2):\n",
        "            x_sol = points[1, 0] - (points[1, 0] - points[0, 0]) * ((points[1, 2] + d2 - d1) / (points[1, 2] - points[0, 2] + 2 * d2))\n",
        "            x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n",
        "        else:\n",
        "            x_sol = (x_max_bound + x_min_bound)/2\n",
        "\n",
        "    # solve linear system\n",
        "    else:\n",
        "        # define linear constraints\n",
        "        A = np.zeros((0, order + 1))\n",
        "        b = np.zeros((0, 1))\n",
        "\n",
        "        # add linear constraints on function values\n",
        "        for i in range(no_points):\n",
        "            if not np.isnan(points[i, 1]):\n",
        "                constraint = np.zeros((1, order + 1))\n",
        "                for j in range(order, -1, -1):\n",
        "                    constraint[0, order - j] = points[i, 0] ** j\n",
        "                A = np.append(A, constraint, 0)\n",
        "                b = np.append(b, points[i, 1])\n",
        "\n",
        "        # add linear constraints on gradient values\n",
        "        for i in range(no_points):\n",
        "            if not np.isnan(points[i, 2]):\n",
        "                constraint = np.zeros((1, order + 1))\n",
        "                for j in range(order):\n",
        "                    constraint[0, j] = (order - j) * points[i, 0] ** (order - j - 1)\n",
        "                A = np.append(A, constraint, 0)\n",
        "                b = np.append(b, points[i, 2])\n",
        "\n",
        "        # check if system is solvable\n",
        "        if A.shape[0] != A.shape[1] or np.linalg.matrix_rank(A) != A.shape[0]:\n",
        "            x_sol = (x_min_bound + x_max_bound)/2\n",
        "            f_min = np.Inf\n",
        "        else:\n",
        "            # solve linear system for interpolating polynomial\n",
        "            coeff = np.linalg.solve(A, b)\n",
        "\n",
        "            # compute critical points\n",
        "            dcoeff = np.zeros(order)\n",
        "            for i in range(len(coeff) - 1):\n",
        "                dcoeff[i] = coeff[i] * (order - i)\n",
        "\n",
        "            crit_pts = np.array([x_min_bound, x_max_bound])\n",
        "            crit_pts = np.append(crit_pts, points[:, 0])\n",
        "\n",
        "            if not np.isinf(dcoeff).any():\n",
        "                roots = np.roots(dcoeff)\n",
        "                crit_pts = np.append(crit_pts, roots)\n",
        "\n",
        "            # test critical points\n",
        "            f_min = np.Inf\n",
        "            x_sol = (x_min_bound + x_max_bound) / 2 # defaults to bisection\n",
        "            for crit_pt in crit_pts:\n",
        "                if np.isreal(crit_pt) and crit_pt >= x_min_bound and crit_pt <= x_max_bound:\n",
        "                    F_cp = np.polyval(coeff, crit_pt)\n",
        "                    if np.isreal(F_cp) and F_cp < f_min:\n",
        "                        x_sol = np.real(crit_pt)\n",
        "                        f_min = np.real(F_cp)\n",
        "\n",
        "            if(plot):\n",
        "                plt.figure()\n",
        "                x = np.arange(x_min_bound, x_max_bound, (x_max_bound - x_min_bound)/10000)\n",
        "                f = np.polyval(coeff, x)\n",
        "                plt.plot(x, f)\n",
        "                plt.plot(x_sol, f_min, 'x')\n",
        "\n",
        "    return x_sol\n",
        "\n",
        "\n",
        "class LBFGS(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements the L-BFGS algorithm. Compatible with multi-batch and full-overlap\n",
        "    L-BFGS implementations and (stochastic) Powell damping. Partly based on the\n",
        "    original L-BFGS implementation in PyTorch, Mark Schmidt's minFunc MATLAB code,\n",
        "    and Michael Overton's weak Wolfe line search MATLAB code.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 10/20/20.\n",
        "\n",
        "    Warnings:\n",
        "      . Does not support per-parameter options and parameter groups.\n",
        "      . All parameters have to be on a single device.\n",
        "\n",
        "    Inputs:\n",
        "        lr (float): steplength or learning rate (default: 1)\n",
        "        history_size (int): update history size (default: 10)\n",
        "        line_search (str): designates line search to use (default: 'Wolfe')\n",
        "            Options:\n",
        "                'None': uses steplength designated in algorithm\n",
        "                'Armijo': uses Armijo backtracking line search\n",
        "                'Wolfe': uses Armijo-Wolfe bracketing line search\n",
        "        dtype: data type (default: torch.float)\n",
        "        debug (bool): debugging mode\n",
        "\n",
        "    References:\n",
        "    [1] Berahas, Albert S., Jorge Nocedal, and Martin Takc. \"A Multi-Batch L-BFGS\n",
        "        Method for Machine Learning.\" Advances in Neural Information Processing\n",
        "        Systems. 2016.\n",
        "    [2] Bollapragada, Raghu, et al. \"A Progressive Batching L-BFGS Method for Machine\n",
        "        Learning.\" International Conference on Machine Learning. 2018.\n",
        "    [3] Lewis, Adrian S., and Michael L. Overton. \"Nonsmooth Optimization via Quasi-Newton\n",
        "        Methods.\" Mathematical Programming 141.1-2 (2013): 135-163.\n",
        "    [4] Liu, Dong C., and Jorge Nocedal. \"On the Limited Memory BFGS Method for\n",
        "        Large Scale Optimization.\" Mathematical Programming 45.1-3 (1989): 503-528.\n",
        "    [5] Nocedal, Jorge. \"Updating Quasi-Newton Matrices With Limited Storage.\"\n",
        "        Mathematics of Computation 35.151 (1980): 773-782.\n",
        "    [6] Nocedal, Jorge, and Stephen J. Wright. \"Numerical Optimization.\" Springer New York,\n",
        "        2006.\n",
        "    [7] Schmidt, Mark. \"minFunc: Unconstrained Differentiable Multivariate Optimization\n",
        "        in Matlab.\" Software available at http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html\n",
        "        (2005).\n",
        "    [8] Schraudolph, Nicol N., Jin Yu, and Simon Gnter. \"A Stochastic Quasi-Newton\n",
        "        Method for Online Convex Optimization.\" Artificial Intelligence and Statistics.\n",
        "        2007.\n",
        "    [9] Wang, Xiao, et al. \"Stochastic Quasi-Newton Methods for Nonconvex Stochastic\n",
        "        Optimization.\" SIAM Journal on Optimization 27.2 (2017): 927-956.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1., history_size=10, line_search='Wolfe',\n",
        "                 dtype=torch.float, debug=False):\n",
        "\n",
        "        # ensure inputs are valid\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0 <= history_size:\n",
        "            raise ValueError(\"Invalid history size: {}\".format(history_size))\n",
        "        if line_search not in ['Armijo', 'Wolfe', 'None']:\n",
        "            raise ValueError(\"Invalid line search: {}\".format(line_search))\n",
        "\n",
        "        defaults = dict(lr=lr, history_size=history_size, line_search=line_search, dtype=dtype, debug=debug)\n",
        "        super(LBFGS, self).__init__(params, defaults)\n",
        "\n",
        "        if len(self.param_groups) != 1:\n",
        "            raise ValueError(\"L-BFGS doesn't support per-parameter options \"\n",
        "                             \"(parameter groups)\")\n",
        "\n",
        "        self._params = self.param_groups[0]['params']\n",
        "        self._numel_cache = None\n",
        "\n",
        "        state = self.state['global_state']\n",
        "        state.setdefault('n_iter', 0)\n",
        "        state.setdefault('curv_skips', 0)\n",
        "        state.setdefault('fail_skips', 0)\n",
        "        state.setdefault('H_diag',1)\n",
        "        state.setdefault('fail', True)\n",
        "\n",
        "        state['old_dirs'] = []\n",
        "        state['old_stps'] = []\n",
        "\n",
        "    def _numel(self):\n",
        "        if self._numel_cache is None:\n",
        "            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)\n",
        "        return self._numel_cache\n",
        "\n",
        "    def _gather_flat_grad(self):\n",
        "        views = []\n",
        "        for p in self._params:\n",
        "            if p.grad is None:\n",
        "                view = p.data.new(p.data.numel()).zero_()\n",
        "            elif p.grad.data.is_sparse:\n",
        "                view = p.grad.data.to_dense().view(-1)\n",
        "            else:\n",
        "                view = p.grad.data.view(-1)\n",
        "            views.append(view)\n",
        "        return torch.cat(views, 0)\n",
        "\n",
        "    def _add_update(self, step_size, update):\n",
        "        offset = 0\n",
        "        for p in self._params:\n",
        "            numel = p.numel()\n",
        "            # view as to avoid deprecated pointwise semantics\n",
        "            p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n",
        "            offset += numel\n",
        "        assert offset == self._numel()\n",
        "\n",
        "    def _copy_params(self):\n",
        "        current_params = []\n",
        "        for param in self._params:\n",
        "            current_params.append(deepcopy(param.data))\n",
        "        return current_params\n",
        "\n",
        "    def _load_params(self, current_params):\n",
        "        i = 0\n",
        "        for param in self._params:\n",
        "            param.data[:] = current_params[i]\n",
        "            i += 1\n",
        "\n",
        "    def line_search(self, line_search):\n",
        "        \"\"\"\n",
        "        Switches line search option.\n",
        "\n",
        "        Inputs:\n",
        "            line_search (str): designates line search to use\n",
        "                Options:\n",
        "                    'None': uses steplength designated in algorithm\n",
        "                    'Armijo': uses Armijo backtracking line search\n",
        "                    'Wolfe': uses Armijo-Wolfe bracketing line search\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        group['line_search'] = line_search\n",
        "\n",
        "        return\n",
        "\n",
        "    def two_loop_recursion(self, vec):\n",
        "        \"\"\"\n",
        "        Performs two-loop recursion on given vector to obtain Hv.\n",
        "\n",
        "        Inputs:\n",
        "            vec (tensor): 1-D tensor to apply two-loop recursion to\n",
        "\n",
        "        Output:\n",
        "            r (tensor): matrix-vector product Hv\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        history_size = group['history_size']\n",
        "\n",
        "        state = self.state['global_state']\n",
        "        old_dirs = state.get('old_dirs')    # change in gradients\n",
        "        old_stps = state.get('old_stps')    # change in iterates\n",
        "        H_diag = state.get('H_diag')\n",
        "\n",
        "        # compute the product of the inverse Hessian approximation and the gradient\n",
        "        num_old = len(old_dirs)\n",
        "\n",
        "        if 'rho' not in state:\n",
        "            state['rho'] = [None] * history_size\n",
        "            state['alpha'] = [None] * history_size\n",
        "        rho = state['rho']\n",
        "        alpha = state['alpha']\n",
        "\n",
        "        for i in range(num_old):\n",
        "            rho[i] = 1. / old_stps[i].dot(old_dirs[i])\n",
        "\n",
        "        q = vec\n",
        "        for i in range(num_old - 1, -1, -1):\n",
        "            alpha[i] = old_dirs[i].dot(q) * rho[i]\n",
        "            q.add_(-alpha[i], old_stps[i])\n",
        "\n",
        "        # multiply by initial Hessian\n",
        "        # r/d is the final direction\n",
        "        r = torch.mul(q, H_diag)\n",
        "        for i in range(num_old):\n",
        "            beta = old_stps[i].dot(r) * rho[i]\n",
        "            r.add_(alpha[i] - beta, old_dirs[i])\n",
        "\n",
        "        return r\n",
        "\n",
        "    def curvature_update(self, flat_grad, eps=1e-2, damping=False):\n",
        "        \"\"\"\n",
        "        Performs curvature update.\n",
        "\n",
        "        Inputs:\n",
        "            flat_grad (tensor): 1-D tensor of flattened gradient for computing\n",
        "                gradient difference with previously stored gradient\n",
        "            eps (float): constant for curvature pair rejection or damping (default: 1e-2)\n",
        "            damping (bool): flag for using Powell damping (default: False)\n",
        "        \"\"\"\n",
        "\n",
        "        assert len(self.param_groups) == 1\n",
        "\n",
        "        # load parameters\n",
        "        if(eps <= 0):\n",
        "            raise(ValueError('Invalid eps; must be positive.'))\n",
        "\n",
        "        group = self.param_groups[0]\n",
        "        history_size = group['history_size']\n",
        "        debug = group['debug']\n",
        "\n",
        "        # variables cached in state (for tracing)\n",
        "        state = self.state['global_state']\n",
        "        fail = state.get('fail')\n",
        "\n",
        "        # check if line search failed\n",
        "        if not fail:\n",
        "\n",
        "            d = state.get('d')\n",
        "            t = state.get('t')\n",
        "            old_dirs = state.get('old_dirs')\n",
        "            old_stps = state.get('old_stps')\n",
        "            H_diag = state.get('H_diag')\n",
        "            prev_flat_grad = state.get('prev_flat_grad')\n",
        "            Bs = state.get('Bs')\n",
        "\n",
        "            # compute y's\n",
        "            y = flat_grad.sub(prev_flat_grad)\n",
        "            s = d.mul(t)\n",
        "            sBs = s.dot(Bs)\n",
        "            ys = y.dot(s)  # y*s\n",
        "\n",
        "            # update L-BFGS matrix\n",
        "            if ys > eps * sBs or damping == True:\n",
        "\n",
        "                # perform Powell damping\n",
        "                if damping == True and ys < eps*sBs:\n",
        "                    if debug:\n",
        "                        print('Applying Powell damping...')\n",
        "                    theta = ((1 - eps) * sBs)/(sBs - ys)\n",
        "                    y = theta * y + (1 - theta) * Bs\n",
        "\n",
        "                # updating memory\n",
        "                if len(old_dirs) == history_size:\n",
        "                    # shift history by one (limited-memory)\n",
        "                    old_dirs.pop(0)\n",
        "                    old_stps.pop(0)\n",
        "\n",
        "                # store new direction/step\n",
        "                old_dirs.append(s)\n",
        "                old_stps.append(y)\n",
        "\n",
        "                # update scale of initial Hessian approximation\n",
        "                H_diag = ys / y.dot(y)  # (y*y)\n",
        "\n",
        "                state['old_dirs'] = old_dirs\n",
        "                state['old_stps'] = old_stps\n",
        "                state['H_diag'] = H_diag\n",
        "\n",
        "            else:\n",
        "                # save skip\n",
        "                state['curv_skips'] += 1\n",
        "                if debug:\n",
        "                    print('Curvature pair skipped due to failed criterion')\n",
        "\n",
        "        else:\n",
        "            # save skip\n",
        "            state['fail_skips'] += 1\n",
        "            if debug:\n",
        "                print('Line search failed; curvature pair update skipped')\n",
        "\n",
        "        return\n",
        "\n",
        "    def _step(self, p_k, g_Ok, g_Sk=None, options=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Inputs:\n",
        "            p_k (tensor): 1-D tensor specifying search direction\n",
        "            g_Ok (tensor): 1-D tensor of flattened gradient over overlap O_k used\n",
        "                            for gradient differencing in curvature pair update\n",
        "            g_Sk (tensor): 1-D tensor of flattened gradient over full sample S_k\n",
        "                            used for curvature pair damping or rejection criterion,\n",
        "                            if None, will use g_Ok (default: None)\n",
        "            options (dict): contains options for performing line search (default: None)\n",
        "\n",
        "        Options for Armijo backtracking line search:\n",
        "            'closure' (callable): reevaluates model and returns function value\n",
        "            'current_loss' (tensor): objective value at current iterate (default: F(x_k))\n",
        "            'gtd' (tensor): inner product g_Ok'd in line search (default: g_Ok'd)\n",
        "            'eta' (tensor): factor for decreasing steplength > 0 (default: 2)\n",
        "            'c1' (tensor): sufficient decrease constant in (0, 1) (default: 1e-4)\n",
        "            'max_ls' (int): maximum number of line search steps permitted (default: 10)\n",
        "            'interpolate' (bool): flag for using interpolation (default: True)\n",
        "            'inplace' (bool): flag for inplace operations (default: True)\n",
        "            'ls_debug' (bool): debugging mode for line search\n",
        "\n",
        "        Options for Wolfe line search:\n",
        "            'closure' (callable): reevaluates model and returns function value\n",
        "            'current_loss' (tensor): objective value at current iterate (default: F(x_k))\n",
        "            'gtd' (tensor): inner product g_Ok'd in line search (default: g_Ok'd)\n",
        "            'eta' (float): factor for extrapolation (default: 2)\n",
        "            'c1' (float): sufficient decrease constant in (0, 1) (default: 1e-4)\n",
        "            'c2' (float): curvature condition constant in (0, 1) (default: 0.9)\n",
        "            'max_ls' (int): maximum number of line search steps permitted (default: 10)\n",
        "            'interpolate' (bool): flag for using interpolation (default: True)\n",
        "            'inplace' (bool): flag for inplace operations (default: True)\n",
        "            'ls_debug' (bool): debugging mode for line search\n",
        "\n",
        "        Outputs (depends on line search):\n",
        "          . No line search:\n",
        "                t (float): steplength\n",
        "          . Armijo backtracking line search:\n",
        "                F_new (tensor): loss function at new iterate\n",
        "                t (tensor): final steplength\n",
        "                ls_step (int): number of backtracks\n",
        "                closure_eval (int): number of closure evaluations\n",
        "                desc_dir (bool): descent direction flag\n",
        "                    True: p_k is descent direction with respect to the line search\n",
        "                    function\n",
        "                    False: p_k is not a descent direction with respect to the line\n",
        "                    search function\n",
        "                fail (bool): failure flag\n",
        "                    True: line search reached maximum number of iterations, failed\n",
        "                    False: line search succeeded\n",
        "          . Wolfe line search:\n",
        "                F_new (tensor): loss function at new iterate\n",
        "                g_new (tensor): gradient at new iterate\n",
        "                t (float): final steplength\n",
        "                ls_step (int): number of backtracks\n",
        "                closure_eval (int): number of closure evaluations\n",
        "                grad_eval (int): number of gradient evaluations\n",
        "                desc_dir (bool): descent direction flag\n",
        "                    True: p_k is descent direction with respect to the line search\n",
        "                    function\n",
        "                    False: p_k is not a descent direction with respect to the line\n",
        "                    search function\n",
        "                fail (bool): failure flag\n",
        "                    True: line search reached maximum number of iterations, failed\n",
        "                    False: line search succeeded\n",
        "\n",
        "        Notes:\n",
        "          . If encountering line search failure in the deterministic setting, one\n",
        "            should try increasing the maximum number of line search steps max_ls.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if options is None:\n",
        "            options = {}\n",
        "        assert len(self.param_groups) == 1\n",
        "\n",
        "        # load parameter options\n",
        "        group = self.param_groups[0]\n",
        "        lr = group['lr']\n",
        "        line_search = group['line_search']\n",
        "        dtype = group['dtype']\n",
        "        debug = group['debug']\n",
        "\n",
        "        # variables cached in state (for tracing)\n",
        "        state = self.state['global_state']\n",
        "        d = state.get('d')\n",
        "        t = state.get('t')\n",
        "        prev_flat_grad = state.get('prev_flat_grad')\n",
        "        Bs = state.get('Bs')\n",
        "\n",
        "        # keep track of nb of iterations\n",
        "        state['n_iter'] += 1\n",
        "\n",
        "        # set search direction\n",
        "        d = p_k\n",
        "\n",
        "        # modify previous gradient\n",
        "        if prev_flat_grad is None:\n",
        "            prev_flat_grad = g_Ok.clone()\n",
        "        else:\n",
        "            prev_flat_grad.copy_(g_Ok)\n",
        "\n",
        "        # set initial step size\n",
        "        t = lr\n",
        "\n",
        "        # closure evaluation counter\n",
        "        closure_eval = 0\n",
        "\n",
        "        if g_Sk is None:\n",
        "            g_Sk = g_Ok.clone()\n",
        "\n",
        "        # perform Armijo backtracking line search\n",
        "        if line_search == 'Armijo':\n",
        "\n",
        "            # load options\n",
        "            if options:\n",
        "                if 'closure' not in options.keys():\n",
        "                    raise(ValueError('closure option not specified.'))\n",
        "                else:\n",
        "                    closure = options['closure']\n",
        "\n",
        "                if 'gtd' not in options.keys():\n",
        "                    gtd = g_Sk.dot(d)\n",
        "                else:\n",
        "                    gtd = options['gtd']\n",
        "\n",
        "                if 'current_loss' not in options.keys():\n",
        "                    F_k = closure()\n",
        "                    closure_eval += 1\n",
        "                else:\n",
        "                    F_k = options['current_loss']\n",
        "\n",
        "                if 'eta' not in options.keys():\n",
        "                    eta = 2\n",
        "                elif options['eta'] <= 0:\n",
        "                    raise(ValueError('Invalid eta; must be positive.'))\n",
        "                else:\n",
        "                    eta = options['eta']\n",
        "\n",
        "                if 'c1' not in options.keys():\n",
        "                    c1 = 1e-4\n",
        "                elif options['c1'] >= 1 or options['c1'] <= 0:\n",
        "                    raise(ValueError('Invalid c1; must be strictly between 0 and 1.'))\n",
        "                else:\n",
        "                    c1 = options['c1']\n",
        "\n",
        "                if 'max_ls' not in options.keys():\n",
        "                    max_ls = 10\n",
        "                elif options['max_ls'] <= 0:\n",
        "                    raise(ValueError('Invalid max_ls; must be positive.'))\n",
        "                else:\n",
        "                    max_ls = options['max_ls']\n",
        "\n",
        "                if 'interpolate' not in options.keys():\n",
        "                    interpolate = True\n",
        "                else:\n",
        "                    interpolate = options['interpolate']\n",
        "\n",
        "                if 'inplace' not in options.keys():\n",
        "                    inplace = True\n",
        "                else:\n",
        "                    inplace = options['inplace']\n",
        "\n",
        "                if 'ls_debug' not in options.keys():\n",
        "                    ls_debug = False\n",
        "                else:\n",
        "                    ls_debug = options['ls_debug']\n",
        "\n",
        "            else:\n",
        "                raise(ValueError('Options are not specified; need closure evaluating function.'))\n",
        "\n",
        "            # initialize values\n",
        "            if interpolate:\n",
        "                if torch.cuda.is_available():\n",
        "                    F_prev = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                else:\n",
        "                    F_prev = torch.tensor(np.nan, dtype=dtype)\n",
        "\n",
        "            ls_step = 0\n",
        "            t_prev = 0 # old steplength\n",
        "            fail = False # failure flag\n",
        "\n",
        "            # begin print for debug mode\n",
        "            if ls_debug:\n",
        "                print('==================================== Begin Armijo line search ===================================')\n",
        "                print('F(x): %.8e  g*d: %.8e' % (F_k, gtd))\n",
        "\n",
        "            # check if search direction is descent direction\n",
        "            if gtd >= 0:\n",
        "                desc_dir = False\n",
        "                if debug:\n",
        "                    print('Not a descent direction!')\n",
        "            else:\n",
        "                desc_dir = True\n",
        "\n",
        "            # store values if not in-place\n",
        "            if not inplace:\n",
        "                current_params = self._copy_params()\n",
        "\n",
        "            # update and evaluate at new point\n",
        "            self._add_update(t, d)\n",
        "            F_new = closure()\n",
        "            closure_eval += 1\n",
        "\n",
        "            # print info if debugging\n",
        "            if ls_debug:\n",
        "                print('LS Step: %d  t: %.8e  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
        "                      % (ls_step, t, F_new, F_k + c1 * t * gtd, F_k))\n",
        "\n",
        "            # check Armijo condition\n",
        "            while F_new > F_k + c1*t*gtd or not is_legal(F_new):\n",
        "\n",
        "                # check if maximum number of iterations reached\n",
        "                if ls_step >= max_ls:\n",
        "                    if inplace:\n",
        "                        self._add_update(-t, d)\n",
        "                    else:\n",
        "                        self._load_params(current_params)\n",
        "\n",
        "                    t = 0\n",
        "                    F_new = closure()\n",
        "                    closure_eval += 1\n",
        "                    fail = True\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    # store current steplength\n",
        "                    t_new = t\n",
        "\n",
        "                    # compute new steplength\n",
        "\n",
        "                    # if first step or not interpolating, then multiply by factor\n",
        "                    if ls_step == 0 or not interpolate or not is_legal(F_new):\n",
        "                        t = t/eta\n",
        "\n",
        "                    # if second step, use function value at new point along with\n",
        "                    # gradient and function at current iterate\n",
        "                    elif ls_step == 1 or not is_legal(F_prev):\n",
        "                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan]]))\n",
        "\n",
        "                    # otherwise, use function values at new point, previous point,\n",
        "                    # and gradient and function at current iterate\n",
        "                    else:\n",
        "                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan],\n",
        "                                                [t_prev, F_prev.item(), np.nan]]))\n",
        "\n",
        "                    # if values are too extreme, adjust t\n",
        "                    if interpolate:\n",
        "                        if t < 1e-3 * t_new:\n",
        "                            t = 1e-3 * t_new\n",
        "                        elif t > 0.6 * t_new:\n",
        "                            t = 0.6 * t_new\n",
        "\n",
        "                        # store old point\n",
        "                        F_prev = F_new\n",
        "                        t_prev = t_new\n",
        "\n",
        "                    # update iterate and reevaluate\n",
        "                    if inplace:\n",
        "                        self._add_update(t - t_new, d)\n",
        "                    else:\n",
        "                        self._load_params(current_params)\n",
        "                        self._add_update(t, d)\n",
        "\n",
        "                    F_new = closure()\n",
        "                    closure_eval += 1\n",
        "                    ls_step += 1 # iterate\n",
        "\n",
        "                    # print info if debugging\n",
        "                    if ls_debug:\n",
        "                        print('LS Step: %d  t: %.8e  F(x+td):   %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
        "                              % (ls_step, t, F_new, F_k + c1 * t * gtd, F_k))\n",
        "\n",
        "            # store Bs\n",
        "            if Bs is None:\n",
        "                Bs = (g_Sk.mul(-t)).clone()\n",
        "            else:\n",
        "                Bs.copy_(g_Sk.mul(-t))\n",
        "\n",
        "            # print final steplength\n",
        "            if ls_debug:\n",
        "                print('Final Steplength:', t)\n",
        "                print('===================================== End Armijo line search ====================================')\n",
        "\n",
        "            state['d'] = d\n",
        "            state['prev_flat_grad'] = prev_flat_grad\n",
        "            state['t'] = t\n",
        "            state['Bs'] = Bs\n",
        "            state['fail'] = fail\n",
        "\n",
        "            return F_new, t, ls_step, closure_eval, desc_dir, fail\n",
        "\n",
        "        # perform weak Wolfe line search\n",
        "        elif line_search == 'Wolfe':\n",
        "\n",
        "            # load options\n",
        "            if options:\n",
        "                if 'closure' not in options.keys():\n",
        "                    raise(ValueError('closure option not specified.'))\n",
        "                else:\n",
        "                    closure = options['closure']\n",
        "\n",
        "                if 'current_loss' not in options.keys():\n",
        "                    F_k = closure()\n",
        "                    closure_eval += 1\n",
        "                else:\n",
        "                    F_k = options['current_loss']\n",
        "\n",
        "                if 'gtd' not in options.keys():\n",
        "                    gtd = g_Sk.dot(d)\n",
        "                else:\n",
        "                    gtd = options['gtd']\n",
        "\n",
        "                if 'eta' not in options.keys():\n",
        "                    eta = 2\n",
        "                elif options['eta'] <= 1:\n",
        "                    raise(ValueError('Invalid eta; must be greater than 1.'))\n",
        "                else:\n",
        "                    eta = options['eta']\n",
        "\n",
        "                if 'c1' not in options.keys():\n",
        "                    c1 = 1e-4\n",
        "                elif options['c1'] >= 1 or options['c1'] <= 0:\n",
        "                    raise(ValueError('Invalid c1; must be strictly between 0 and 1.'))\n",
        "                else:\n",
        "                    c1 = options['c1']\n",
        "\n",
        "                if 'c2' not in options.keys():\n",
        "                    c2 = 0.9\n",
        "                elif options['c2'] >= 1 or options['c2'] <= 0:\n",
        "                    raise(ValueError('Invalid c2; must be strictly between 0 and 1.'))\n",
        "                elif options['c2'] <= c1:\n",
        "                    raise(ValueError('Invalid c2; must be strictly larger than c1.'))\n",
        "                else:\n",
        "                    c2 = options['c2']\n",
        "\n",
        "                if 'max_ls' not in options.keys():\n",
        "                    max_ls = 10\n",
        "                elif options['max_ls'] <= 0:\n",
        "                    raise(ValueError('Invalid max_ls; must be positive.'))\n",
        "                else:\n",
        "                    max_ls = options['max_ls']\n",
        "\n",
        "                if 'interpolate' not in options.keys():\n",
        "                    interpolate = True\n",
        "                else:\n",
        "                    interpolate = options['interpolate']\n",
        "\n",
        "                if 'inplace' not in options.keys():\n",
        "                    inplace = True\n",
        "                else:\n",
        "                    inplace = options['inplace']\n",
        "\n",
        "                if 'ls_debug' not in options.keys():\n",
        "                    ls_debug = False\n",
        "                else:\n",
        "                    ls_debug = options['ls_debug']\n",
        "\n",
        "            else:\n",
        "                raise(ValueError('Options are not specified; need closure evaluating function.'))\n",
        "\n",
        "            # initialize counters\n",
        "            ls_step = 0\n",
        "            grad_eval = 0 # tracks gradient evaluations\n",
        "            t_prev = 0 # old steplength\n",
        "\n",
        "            # initialize bracketing variables and flag\n",
        "            alpha = 0\n",
        "            beta = float('Inf')\n",
        "            fail = False\n",
        "\n",
        "            # initialize values for line search\n",
        "            if(interpolate):\n",
        "                F_a = F_k\n",
        "                g_a = gtd\n",
        "\n",
        "                if(torch.cuda.is_available()):\n",
        "                    F_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                    g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                else:\n",
        "                    F_b = torch.tensor(np.nan, dtype=dtype)\n",
        "                    g_b = torch.tensor(np.nan, dtype=dtype)\n",
        "\n",
        "            # begin print for debug mode\n",
        "            if ls_debug:\n",
        "                print('==================================== Begin Wolfe line search ====================================')\n",
        "                print('F(x): %.8e  g*d: %.8e' % (F_k, gtd))\n",
        "\n",
        "            # check if search direction is descent direction\n",
        "            if gtd >= 0:\n",
        "                desc_dir = False\n",
        "                if debug:\n",
        "                    print('Not a descent direction!')\n",
        "            else:\n",
        "                desc_dir = True\n",
        "\n",
        "            # store values if not in-place\n",
        "            if not inplace:\n",
        "                current_params = self._copy_params()\n",
        "\n",
        "            # update and evaluate at new point\n",
        "            self._add_update(t, d)\n",
        "            F_new = closure()\n",
        "            closure_eval += 1\n",
        "\n",
        "            # main loop\n",
        "            while True:\n",
        "\n",
        "                # check if maximum number of line search steps have been reached\n",
        "                if ls_step >= max_ls:\n",
        "                    if inplace:\n",
        "                        self._add_update(-t, d)\n",
        "                    else:\n",
        "                        self._load_params(current_params)\n",
        "\n",
        "                    t = 0\n",
        "                    F_new = closure()\n",
        "                    F_new.backward()\n",
        "                    g_new = self._gather_flat_grad()\n",
        "                    closure_eval += 1\n",
        "                    grad_eval += 1\n",
        "                    fail = True\n",
        "                    break\n",
        "\n",
        "                # print info if debugging\n",
        "                if ls_debug:\n",
        "                    print('LS Step: %d  t: %.8e  alpha: %.8e  beta: %.8e'\n",
        "                          % (ls_step, t, alpha, beta))\n",
        "                    print('Armijo:  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e'\n",
        "                          % (F_new, F_k + c1 * t * gtd, F_k))\n",
        "\n",
        "                # check Armijo condition\n",
        "                if F_new > F_k + c1 * t * gtd:\n",
        "\n",
        "                    # set upper bound\n",
        "                    beta = t\n",
        "                    t_prev = t\n",
        "\n",
        "                    # update interpolation quantities\n",
        "                    if interpolate:\n",
        "                        F_b = F_new\n",
        "                        if torch.cuda.is_available():\n",
        "                            g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n",
        "                        else:\n",
        "                            g_b = torch.tensor(np.nan, dtype=dtype)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    # compute gradient\n",
        "                    F_new.backward()\n",
        "                    g_new = self._gather_flat_grad()\n",
        "                    grad_eval += 1\n",
        "                    gtd_new = g_new.dot(d)\n",
        "\n",
        "                    # print info if debugging\n",
        "                    if ls_debug:\n",
        "                        print('Wolfe: g(x+td)*d: %.8e  c2*g*d: %.8e  gtd: %.8e'\n",
        "                              % (gtd_new, c2 * gtd, gtd))\n",
        "\n",
        "                    # check curvature condition\n",
        "                    if gtd_new < c2 * gtd:\n",
        "\n",
        "                        # set lower bound\n",
        "                        alpha = t\n",
        "                        t_prev = t\n",
        "\n",
        "                        # update interpolation quantities\n",
        "                        if interpolate:\n",
        "                            F_a = F_new\n",
        "                            g_a = gtd_new\n",
        "\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # compute new steplength\n",
        "\n",
        "                # if first step or not interpolating, then bisect or multiply by factor\n",
        "                if not interpolate or not is_legal(F_b):\n",
        "                    if beta == float('Inf'):\n",
        "                        t = eta*t\n",
        "                    else:\n",
        "                        t = (alpha + beta)/2.0\n",
        "\n",
        "                # otherwise interpolate between a and b\n",
        "                else:\n",
        "                    t = polyinterp(np.array([[alpha, F_a.item(), g_a.item()], [beta, F_b.item(), g_b.item()]]))\n",
        "\n",
        "                    # if values are too extreme, adjust t\n",
        "                    if beta == float('Inf'):\n",
        "                        if t > 2 * eta * t_prev:\n",
        "                            t = 2 * eta * t_prev\n",
        "                        elif t < eta * t_prev:\n",
        "                            t = eta * t_prev\n",
        "                    else:\n",
        "                        if t < alpha + 0.2 * (beta - alpha):\n",
        "                            t = alpha + 0.2 * (beta - alpha)\n",
        "                        elif t > (beta - alpha) / 2.0:\n",
        "                            t = (beta - alpha) / 2.0\n",
        "\n",
        "                    # if we obtain nonsensical value from interpolation\n",
        "                    if t <= 0:\n",
        "                        t = (beta - alpha) / 2.0\n",
        "\n",
        "                # update parameters\n",
        "                if inplace:\n",
        "                    self._add_update(t - t_prev, d)\n",
        "                else:\n",
        "                    self._load_params(current_params)\n",
        "                    self._add_update(t, d)\n",
        "\n",
        "                # evaluate closure\n",
        "                F_new = closure()\n",
        "                closure_eval += 1\n",
        "                ls_step += 1\n",
        "\n",
        "            # store Bs\n",
        "            if Bs is None:\n",
        "                Bs = (g_Sk.mul(-t)).clone()\n",
        "            else:\n",
        "                Bs.copy_(g_Sk.mul(-t))\n",
        "\n",
        "            # print final steplength\n",
        "            if ls_debug:\n",
        "                print('Final Steplength:', t)\n",
        "                print('===================================== End Wolfe line search =====================================')\n",
        "\n",
        "            state['d'] = d\n",
        "            state['prev_flat_grad'] = prev_flat_grad\n",
        "            state['t'] = t\n",
        "            state['Bs'] = Bs\n",
        "            state['fail'] = fail\n",
        "\n",
        "            return F_new, g_new, t, ls_step, closure_eval, grad_eval, desc_dir, fail\n",
        "\n",
        "        else:\n",
        "\n",
        "            # perform update\n",
        "            self._add_update(t, d)\n",
        "\n",
        "            # store Bs\n",
        "            if Bs is None:\n",
        "                Bs = (g_Sk.mul(-t)).clone()\n",
        "            else:\n",
        "                Bs.copy_(g_Sk.mul(-t))\n",
        "\n",
        "            state['d'] = d\n",
        "            state['prev_flat_grad'] = prev_flat_grad\n",
        "            state['t'] = t\n",
        "            state['Bs'] = Bs\n",
        "            state['fail'] = False\n",
        "\n",
        "            return t\n",
        "\n",
        "    def step(self, p_k, g_Ok, g_Sk=None, options={}):\n",
        "        return self._step(p_k, g_Ok, g_Sk, options)\n",
        "\n",
        "\n",
        "class FullBatchLBFGS(LBFGS):\n",
        "    \"\"\"\n",
        "    Implements full-batch or deterministic L-BFGS algorithm. Compatible with\n",
        "    Powell damping. Can be used when evaluating a deterministic function and\n",
        "    gradient. Wraps the LBFGS optimizer. Performs the two-loop recursion,\n",
        "    updating, and curvature updating in a single step.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 11/15/18.\n",
        "\n",
        "    Warnings:\n",
        "      . Does not support per-parameter options and parameter groups.\n",
        "      . All parameters have to be on a single device.\n",
        "\n",
        "    Inputs:\n",
        "        lr (float): steplength or learning rate (default: 1)\n",
        "        history_size (int): update history size (default: 10)\n",
        "        line_search (str): designates line search to use (default: 'Wolfe')\n",
        "            Options:\n",
        "                'None': uses steplength designated in algorithm\n",
        "                'Armijo': uses Armijo backtracking line search\n",
        "                'Wolfe': uses Armijo-Wolfe bracketing line search\n",
        "        dtype: data type (default: torch.float)\n",
        "        debug (bool): debugging mode\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1, history_size=10, line_search='Wolfe',\n",
        "                 dtype=torch.float, debug=False):\n",
        "        super(FullBatchLBFGS, self).__init__(params, lr, history_size, line_search,\n",
        "             dtype, debug)\n",
        "\n",
        "    def step(self, options=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Inputs:\n",
        "            options (dict): contains options for performing line search (default: None)\n",
        "\n",
        "        General Options:\n",
        "            'eps' (float): constant for curvature pair rejection or damping (default: 1e-2)\n",
        "            'damping' (bool): flag for using Powell damping (default: False)\n",
        "\n",
        "        Options for Armijo backtracking line search:\n",
        "            'closure' (callable): reevaluates model and returns function value\n",
        "            'current_loss' (tensor): objective value at current iterate (default: F(x_k))\n",
        "            'gtd' (tensor): inner product g_Ok'd in line search (default: g_Ok'd)\n",
        "            'eta' (tensor): factor for decreasing steplength > 0 (default: 2)\n",
        "            'c1' (tensor): sufficient decrease constant in (0, 1) (default: 1e-4)\n",
        "            'max_ls' (int): maximum number of line search steps permitted (default: 10)\n",
        "            'interpolate' (bool): flag for using interpolation (default: True)\n",
        "            'inplace' (bool): flag for inplace operations (default: True)\n",
        "            'ls_debug' (bool): debugging mode for line search\n",
        "\n",
        "        Options for Wolfe line search:\n",
        "            'closure' (callable): reevaluates model and returns function value\n",
        "            'current_loss' (tensor): objective value at current iterate (default: F(x_k))\n",
        "            'gtd' (tensor): inner product g_Ok'd in line search (default: g_Ok'd)\n",
        "            'eta' (float): factor for extrapolation (default: 2)\n",
        "            'c1' (float): sufficient decrease constant in (0, 1) (default: 1e-4)\n",
        "            'c2' (float): curvature condition constant in (0, 1) (default: 0.9)\n",
        "            'max_ls' (int): maximum number of line search steps permitted (default: 10)\n",
        "            'interpolate' (bool): flag for using interpolation (default: True)\n",
        "            'inplace' (bool): flag for inplace operations (default: True)\n",
        "            'ls_debug' (bool): debugging mode for line search\n",
        "\n",
        "        Outputs (depends on line search):\n",
        "          . No line search:\n",
        "                t (float): steplength\n",
        "          . Armijo backtracking line search:\n",
        "                F_new (tensor): loss function at new iterate\n",
        "                t (tensor): final steplength\n",
        "                ls_step (int): number of backtracks\n",
        "                closure_eval (int): number of closure evaluations\n",
        "                desc_dir (bool): descent direction flag\n",
        "                    True: p_k is descent direction with respect to the line search\n",
        "                    function\n",
        "                    False: p_k is not a descent direction with respect to the line\n",
        "                    search function\n",
        "                fail (bool): failure flag\n",
        "                    True: line search reached maximum number of iterations, failed\n",
        "                    False: line search succeeded\n",
        "          . Wolfe line search:\n",
        "                F_new (tensor): loss function at new iterate\n",
        "                g_new (tensor): gradient at new iterate\n",
        "                t (float): final steplength\n",
        "                ls_step (int): number of backtracks\n",
        "                closure_eval (int): number of closure evaluations\n",
        "                grad_eval (int): number of gradient evaluations\n",
        "                desc_dir (bool): descent direction flag\n",
        "                    True: p_k is descent direction with respect to the line search\n",
        "                    function\n",
        "                    False: p_k is not a descent direction with respect to the line\n",
        "                    search function\n",
        "                fail (bool): failure flag\n",
        "                    True: line search reached maximum number of iterations, failed\n",
        "                    False: line search succeeded\n",
        "\n",
        "        Notes:\n",
        "          . If encountering line search failure in the deterministic setting, one\n",
        "            should try increasing the maximum number of line search steps max_ls.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # load options for damping and eps\n",
        "        if 'damping' not in options.keys():\n",
        "            damping = False\n",
        "        else:\n",
        "            damping = options['damping']\n",
        "\n",
        "        if 'eps' not in options.keys():\n",
        "            eps = 1e-2\n",
        "        else:\n",
        "            eps = options['eps']\n",
        "\n",
        "        # gather gradient\n",
        "        grad = self._gather_flat_grad()\n",
        "\n",
        "        # update curvature if after 1st iteration\n",
        "        state = self.state['global_state']\n",
        "        if state['n_iter'] > 0:\n",
        "            self.curvature_update(grad, eps, damping)\n",
        "\n",
        "        # compute search direction\n",
        "        p = self.two_loop_recursion(-grad)\n",
        "\n",
        "        # take step\n",
        "        return self._step(p, grad, options=options)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "#%% Compute Objective and Gradient Helper Function\n",
        "\n",
        "def get_grad(optimizer, X_Sk, y_Sk, opfun, ghost_batch= 128):\n",
        "    \"\"\"\n",
        "    Computes objective and gradient of neural network over data sample.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 8/29/18.\n",
        "\n",
        "    Inputs:\n",
        "        optimizer (Optimizer): the PBQN optimizer\n",
        "        X_Sk (nparray): set of training examples over sample Sk\n",
        "        y_Sk (nparray): set of training labels over sample Sk\n",
        "        opfun (callable): computes forward pass over network over sample Sk\n",
        "        ghost_batch (int): maximum size of effective batch (default: 128)\n",
        "\n",
        "    Outputs:\n",
        "        grad (tensor): stochastic gradient over sample Sk\n",
        "        obj (tensor): stochastic function value over sample Sk\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if(torch.cuda.is_available()):\n",
        "        obj = torch.tensor(0, dtype=torch.float).cuda()\n",
        "    else:\n",
        "        obj = torch.tensor(0, dtype=torch.float)\n",
        "\n",
        "    Sk_size = X_Sk.shape[0]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # loop through relevant data\n",
        "    for idx in np.array_split(np.arange(Sk_size), max(int(Sk_size/ghost_batch), 1)):\n",
        "\n",
        "        # define ops\n",
        "        energy, ops, output_rand = opfun(X_Sk[idx])\n",
        "\n",
        "        # define loss and perform forward-backward pass\n",
        "        loss_fn = energy*(len(idx)/Sk_size)\n",
        "        for output in ops:\n",
        "          loss_fn += (0.1*F.cross_entropy(output, y_Sk[idx]))*(len(idx)/Sk_size)\n",
        "        loss_fn.backward()\n",
        "\n",
        "        # accumulate loss\n",
        "        obj += loss_fn\n",
        "\n",
        "    # gather flat gradient\n",
        "    grad = optimizer._gather_flat_grad()\n",
        "\n",
        "    return grad, obj\n",
        "\n",
        "#%% Adjusts Learning Rate Helper Function\n",
        "\n",
        "def adjust_learning_rate(optimizer, learning_rate):\n",
        "    \"\"\"\n",
        "    Sets the learning rate of optimizer.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 8/29/18.\n",
        "\n",
        "    Inputs:\n",
        "        optimizer (Optimizer): any optimizer\n",
        "        learning_rate (float): desired steplength\n",
        "\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = learning_rate\n",
        "\n",
        "    return\n",
        "\n",
        "#%% CUTEst PyTorch Interface\n",
        "\n",
        "class CUTEstFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Converts CUTEst problem using PyCUTEst to PyTorch function.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 9/21/18.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, problem):\n",
        "        x = input.clone().detach().numpy()\n",
        "        obj, grad = problem.obj(x, gradient=True)\n",
        "        ctx.save_for_backward(torch.tensor(grad, dtype=torch.float))\n",
        "        return torch.tensor(obj, dtype=torch.float)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        grad, = ctx.saved_tensors\n",
        "        return grad, None\n",
        "\n",
        "class CUTEstProblem(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Converts CUTEst problem to torch neural network module.\n",
        "\n",
        "    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n",
        "    Last edited 9/21/18.\n",
        "\n",
        "    Inputs:\n",
        "        problem (callable): CUTEst problem interfaced through PyCUTEst\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, problem):\n",
        "        super(CUTEstProblem, self).__init__()\n",
        "        # get initialization\n",
        "        x = torch.tensor(problem.x0, dtype=torch.float)\n",
        "        x.requires_grad_()\n",
        "\n",
        "        # store variables and problem\n",
        "        self.variables = torch.nn.Parameter(x)\n",
        "        self.problem = problem\n",
        "\n",
        "    def forward(self):\n",
        "        model = CUTEstFunction.apply\n",
        "        return model(self.variables, self.problem)\n",
        "\n",
        "    def grad(self):\n",
        "        return self.variables.grad\n",
        "\n",
        "    def x(self):\n",
        "        return self.variables"
      ],
      "metadata": {
        "id": "pWIPeO8AmfQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam, LBFGS\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import math\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "dataloader = DataLoader(dataset, batch_size=409, shuffle=True)\n",
        "\n",
        "def train_dbn(dataloader, dbn_model, num_epochs, learning_rate, device):\n",
        "    dbn_model.to(device)  # Ensure model is on the correct device\n",
        "    dbn_model.train()\n",
        "\n",
        "    optimizer = Adam(dbn_model.parameters(),lr=learning_rate)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss=0\n",
        "        total_val=0\n",
        "        i=0\n",
        "        initial_start_time = time.time()\n",
        "        for step, (x, x_e) in enumerate(dataloader):\n",
        "            print(\"x_e: \",x_e)\n",
        "            start_time = time.time()\n",
        "            i+=1\n",
        "            optimizer.zero_grad()\n",
        "            energy, ops, rand_ops = dbn_model(x.float().to(device))\n",
        "            loss=energy\n",
        "            val_loss=0\n",
        "            for r in range(len(ops)):\n",
        "                val_loss1 = 0.1*F.cross_entropy(ops[r], x_e.to(device))\n",
        "                loss+= val_loss1\n",
        "                val_loss+=val_loss1\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            total_val+=val_loss.item()\n",
        "            #print(\"data \",step, \" loss: \",loss,\" validation loss: \",val_loss, \" rand val: \",rand_val, f\" time: {elapsed_time:.4f}\")\n",
        "        elapsed_time = end_time - initial_start_time\n",
        "        print(f\"Epoch {epoch} avg loss: {total_loss/i:.4f} avg val loss: {total_val/i:.4f}  time: {elapsed_time:.4f}\")\n",
        "\n",
        "\n",
        "dbn_model = DBM(num_features, hidden_layers).to(device)\n",
        "#final_dict = torch.load(\"dbn_final_dict.pth\")\n",
        "#dbn_model.load_state_dict(final_dict)\n",
        "train_dbn(dataloader, dbn_model, 1000, 0.0001, device)\n"
      ],
      "metadata": {
        "id": "YUoVU3RsmrfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import math\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "def train_dbn(X_train, Y_train, dbn_model, num_epochs, learning_rate, device, overlap_ratio=0.25, batch_size=8192):\n",
        "    dbn_model.to(device)  # Ensure model is on the correct device\n",
        "    dbn_model.train()\n",
        "    X_train=X_train.to(device)\n",
        "    Y_train=Y_train.to(device)\n",
        "    opfun = lambda X: dbn_model.forward(X.to(device).float())\n",
        "    optimizer = LBFGS(dbn_model.parameters(),lr=learning_rate,history_size=100, line_search='Wolfe')\n",
        "\n",
        "    Ok_size = int(overlap_ratio * batch_size)\n",
        "    Nk_size = int((1 - 2 * overlap_ratio) * batch_size)\n",
        "    random_index = np.random.permutation(range(X_train.shape[0]))\n",
        "    Ok_prev = random_index[0:Ok_size]\n",
        "    g_Ok_prev, obj_Ok_prev = get_grad(optimizer,X_train[Ok_prev],Y_train[Ok_prev].to(device), opfun)\n",
        "    total_loss=0\n",
        "    total_val=0\n",
        "    total_val_rand=0\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        # sample current non-overlap and next overlap gradient\n",
        "        random_index = np.random.permutation(range(X_train.shape[0]))\n",
        "        Ok = random_index[0:Ok_size]\n",
        "        Nk = random_index[Ok_size:(Ok_size + Nk_size)]\n",
        "        # compute overlap gradient and objective\n",
        "        g_Ok, obj_Ok = get_grad(optimizer, X_train[Ok].to(device),Y_train[Ok].to(device), opfun)\n",
        "        # compute non-overlap gradient and objective\n",
        "        g_Nk, obj_Nk = get_grad(optimizer, X_train[Nk].to(device),Y_train[Nk].to(device), opfun)\n",
        "        # compute accumulated gradient over sample\n",
        "        g_Sk = overlap_ratio * (g_Ok_prev + g_Ok) + (1 - 2 * overlap_ratio) * g_Nk\n",
        "        # two-loop recursion to compute search direction\n",
        "        p = optimizer.two_loop_recursion(-g_Sk)\n",
        "        # define closure for line search\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            loss_fn = torch.tensor(0, dtype=torch.float).to(device)\n",
        "            for subsmpl in np.array_split(Ok, max(int(Ok_size / 128), 1)):\n",
        "                energy, ops, rand_ops = dbn_model(X_train[subsmpl].to(device).float())\n",
        "                loss_fn = energy * (len(subsmpl) / batch_size)\n",
        "                for output in ops:\n",
        "                   loss_fn+= 0.1*F.cross_entropy(output, Y_train[subsmpl].to(device)) * (len(subsmpl) / batch_size)\n",
        "            return loss_fn\n",
        "        # perform line search step\n",
        "        obj, grad, lr, _, _, _, _, _ = optimizer.step(p, g_Ok, g_Sk=g_Sk,options={'closure': closure, 'current_loss': obj_Ok})\n",
        "        Ok_prev = Ok\n",
        "        g_Ok_prev, obj_Ok_prev = get_grad(optimizer, X_train[Ok_prev].to(device),Y_train[Ok_prev], opfun)\n",
        "        # curvature update\n",
        "        optimizer.curvature_update(g_Ok_prev, eps=0.2, damping=True)\n",
        "        dbn_model.eval()\n",
        "        output, output_rand = dbn_model.reconstruct(Y_train.to(device).float())\n",
        "        val_loss = F.cross_entropy(output, Y_train.to(device).float())\n",
        "        val_loss_rand = F.cross_entropy(output_rand, Y_train.to(device).float())\n",
        "        total_val+=val_loss.item()\n",
        "        total_val_rand+=val_loss_rand.item()\n",
        "        dbn_model.train()\n",
        "        end_time=time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Epoch {epoch} lr: {lr} loss: {obj} val loss: {val_loss:.4f}  val_rand loss {val_loss_rand:.4f} time: {elapsed_time:.4f}\")\n",
        "\n",
        "\n",
        "dbn_model = DBM(num_features, hidden_layers).to(device)\n",
        "#final_dict = torch.load(\"dbn_final_dict2.pth\")\n",
        "#dbn_model.load_state_dict(final_dict)\n",
        "train_dbn(X_train,Y_train, dbn_model, 1000, 0.0001, device)\n"
      ],
      "metadata": {
        "id": "IkJi6VXD_TK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NN4oiJJlkiy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ajpc3ACxeuFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NgMXzajiqLRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(dbn_model,\"LBFGS_model.pth\")"
      ],
      "metadata": {
        "id": "nG_WmlfjsO63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i2fAwMfsUl-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCPz2EB80lNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HFvxdEz_urz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you have to train it in the specific order of first with a low learning rate to improve the energy, then a higher one to improve the validation loss, considering making a custom optimization algorithm for the learning rate. consider different magnitudes for each energy (i.e add a beta value)\n",
        "\n",
        "\n",
        "Removing element wise operations (perhaps with approximations) and transfering to tensor flow with TPU is it at full power."
      ],
      "metadata": {
        "id": "b47_y7bZhsTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(dbn_model,'8192_1024_256_32v2.pth')"
      ],
      "metadata": {
        "id": "zpUzwgxxr1i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_OZ5VJC9icQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def decimal_to_binary_tensor(decimal_num):\n",
        "    \"\"\"\n",
        "    Converts a decimal number to a binary representation and returns it as a PyTorch tensor.\n",
        "\n",
        "    Args:\n",
        "        decimal_num: The decimal number to convert.\n",
        "\n",
        "    Returns:\n",
        "        A PyTorch tensor representing the binary format.\n",
        "    \"\"\"\n",
        "    decimal_num = max(-100, min(decimal_num, 100))\n",
        "\n",
        "    sign_bit = 1 if decimal_num < 0 else 0\n",
        "    integer_part = abs(int(decimal_num))\n",
        "    fractional_part = abs(decimal_num) - integer_part\n",
        "\n",
        "    # Convert integer part to binary with 7 bits\n",
        "    integer_bits = bin(integer_part)[2:].zfill(7)\n",
        "\n",
        "    # Convert fractional part to binary with 10 bits\n",
        "    fractional_bits = \"\"\n",
        "    for _ in range(10):\n",
        "        fractional_part *= 2\n",
        "        fractional_bits += str(int(fractional_part))\n",
        "        fractional_part -= int(fractional_part)\n",
        "\n",
        "    # Combine the bits\n",
        "    binary_str = str(sign_bit) + integer_bits + fractional_bits\n",
        "\n",
        "    # Convert to a PyTorch tensor\n",
        "    binary_tensor = torch.tensor([int(bit) for bit in binary_str])\n",
        "    return binary_tensor\n",
        "\n",
        "def generate_data(num_samples=100,range=100,uniform=False):\n",
        "    if uniform:\n",
        "      a_values = np.random.uniform(low=0, high=1, size=num_samples)\n",
        "      b_values = np.random.uniform(low=0, high=1, size=num_samples)\n",
        "    else:\n",
        "      a_values = np.random.uniform(low=-range, high=range, size=num_samples)\n",
        "      b_values = np.random.uniform(low=-range, high=range, size=num_samples)\n",
        "    y_values = (a_values > b_values).astype(int)\n",
        "    a_tensors = torch.stack([decimal_to_binary_tensor(a) for a in a_values])\n",
        "    b_tensors = torch.stack([decimal_to_binary_tensor(b) for b in b_values])\n",
        "\n",
        "    y_tensor = torch.tensor(y_values[:, np.newaxis])  # Convert y_values to tensor\n",
        "    data = torch.cat((a_tensors, b_tensors, y_tensor), dim=1)\n",
        "    return data\n",
        "\n",
        "class ComparatorDBM(nn.Module):\n",
        "    def __init__(self, hidden_layersl):\n",
        "        super().__init__()\n",
        "        self.dbm = DBM(36, hidden_layers, True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        energy_loss, output = self.dbm(x)\n",
        "        return energy_loss, output  # We use the output from DBM\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "def train_comparator_dbm():\n",
        "    \"\"\"Trains the ComparatorDBM model on TPU.\"\"\"\n",
        "    # Generate a large dataset once\n",
        "    data_list = [\n",
        "        generate_data(num_samples=10000, range=1, uniform=True),\n",
        "        generate_data(num_samples=5000),\n",
        "        generate_data(num_samples=10000, range=0.5),\n",
        "        generate_data(num_samples=5000, range=1),\n",
        "        generate_data(num_samples=5000, range=10),\n",
        "        generate_data(num_samples=5000, range=0.1),\n",
        "    ]\n",
        "    data = np.vstack(data_list)\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    # Create a PyTorch Dataset and DataLoader\n",
        "    dataset = torch.utils.data.TensorDataset(\n",
        "        torch.tensor(data[:, :36], dtype=torch.float32).to(device),\n",
        "        torch.tensor(data[:, 36], dtype=torch.float32).to(device)\n",
        "    )\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "    # Model, optimizer, and loss function\n",
        "    model = ComparatorDBM(hidden_layers).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(2000000):\n",
        "        total_energy_loss = 0\n",
        "        total_val_loss = 0\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            optimizer.zero_grad()\n",
        "            energy_loss, output = model(inputs)\n",
        "            loss2 = mse_loss(output.squeeze(), targets)\n",
        "            loss = energy_loss + loss2\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_energy_loss += energy_loss.item()\n",
        "            total_val_loss += loss2.item()\n",
        "            print(f\"{i+1} energy_loss: {total_energy_loss/(i+1)}, val_loss: {total_val_loss/(i+1)}\")\n",
        "        print(f\"Epoch {epoch}: energy_loss: {total_energy_loss/(i+1)}, val_loss: {total_val_loss/(i+1)}\")\n",
        "\n",
        "    torch.save(model, 'greaterthanDBM.pth')\n",
        "\n",
        "\n",
        "train_comparator_dbm()"
      ],
      "metadata": {
        "id": "jACvTSSrKww4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gplearn"
      ],
      "metadata": {
        "id": "Vls4bKySYatl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import gplearn.genetic as gp\n",
        "from gplearn.functions import make_function\n",
        "import scipy.special as sp\n",
        "def generate_data(num_samples=100,range=100,uniform=False):\n",
        "    if uniform:\n",
        "      a_values = np.random.uniform(low=0, high=1, size=num_samples)\n",
        "      b_values = np.random.uniform(low=0, high=1, size=num_samples)\n",
        "    else:\n",
        "      a_values = np.random.uniform(low=-range, high=range, size=num_samples)\n",
        "      b_values = np.random.uniform(low=-range, high=range, size=num_samples)\n",
        "    y_values = (a_values > b_values).astype(int)\n",
        "    data = torch.cat((torch.tensor(a_values).unsqueeze(1), torch.tensor(b_values).unsqueeze(1), torch.tensor(y_values).unsqueeze(1)), dim=1)\n",
        "    return data\n",
        "\n",
        "data_list = [\n",
        "        generate_data(num_samples=10000, range=1, uniform=True),\n",
        "        generate_data(num_samples=5000),\n",
        "        generate_data(num_samples=10000, range=0.5),\n",
        "        generate_data(num_samples=5000, range=1),\n",
        "        generate_data(num_samples=5000, range=10),\n",
        "        generate_data(num_samples=5000, range=0.1),\n",
        "    ]\n",
        "data = np.vstack(data_list)\n",
        "np.random.shuffle(data)\n",
        "def internalSigmoid(x,y):\n",
        "  return sp.expit(x - y)\n",
        "X = torch.tensor(data[:, :2], dtype=torch.float32)\n",
        "y = torch.tensor(data[:, 2], dtype=torch.float32)\n",
        "\n",
        "def intSigmoid(x):\n",
        "  return sp.expit(x)\n",
        "\n",
        "dsig = make_function(function=internalSigmoid, name='dsig',arity=2)\n",
        "sig = make_function(function=intSigmoid, name='sig',arity=1)\n",
        "# Symbolic regression:\n",
        "function_set = ['add', 'sub', 'mul', 'div','sqrt', 'log', 'sin','cos','tan','abs', 'neg', 'inv',dsig,sig]  # Customize as needed\n",
        "est_gp = gp.SymbolicRegressor(population_size=10000,\n",
        "                             generations=200, stopping_criteria=0.000001,\n",
        "                             p_crossover=0.7, p_subtree_mutation=0.1,\n",
        "                             p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
        "                             max_samples=0.9, verbose=1,\n",
        "                             parsimony_coefficient=0.01, random_state=0,\n",
        "                             function_set=function_set)\n",
        "est_gp.fit(X, y)\n",
        "best_expr = str(est_gp._program)\n",
        "print(best_expr)"
      ],
      "metadata": {
        "id": "nifywQFZYSr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcAZxL8ObDqZ"
      },
      "outputs": [],
      "source": [
        "torch.save(dbn_model,'512l128l32.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q04zLoAybPIw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOYjoL9Q-uFt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class ComparatorNetwork(tf.keras.Model):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
        "        self.linear2 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
        "        self.linear3 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
        "        self.linear4 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x):  # TensorFlow uses 'call' instead of 'forward'\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.linear3(x)\n",
        "        x = self.linear4(x)\n",
        "        return x\n",
        "\n",
        "# Load model weights (similar to PyTorch but using TensorFlow's format)\n",
        "temp_model = tf.keras.models.load_model('greaterThanMerged.pth')\n",
        "model = ComparatorNetwork(32)\n",
        "model.set_weights(temp_model.get_weights())\n",
        "\n",
        "# merge the bernoulli approximator into a 'greater than' vs random number [0,1]\n",
        "# for bernoulli a is probability, b is random number\n",
        "# Train the model without the sigmoid layer\n",
        "def greaterThanFunction(a, b):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch([a, b])  # Watch the inputs\n",
        "\n",
        "        # Forward Pass\n",
        "        result = tf.zeros_like(a)\n",
        "        for i in tf.range(a.shape[1]):\n",
        "            input_i = tf.concat([a[:, i:i+1], b[:, i:i+1]], axis=1)\n",
        "            output = model(input_i)\n",
        "            result[:, i] = tf.squeeze(output)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace the sigmoid logits with a specialized 'sigmoid logits approximator'"
      ],
      "metadata": {
        "id": "uXycNHft9uP3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X-Vcyzh0O3c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "def generate_data(x):\n",
        "    num_configurations = 20\n",
        "\n",
        "    # Generate the continuous number sequence\n",
        "    configurations = np.arange(num_configurations)\n",
        "\n",
        "    # Threshold based on x\n",
        "    threshold = int(x * num_configurations)\n",
        "    y_values = (configurations < threshold).astype(int)\n",
        "\n",
        "    # Combine input x, configurations, and output y\n",
        "    data = np.concatenate((np.tile(x, (num_configurations, 1)),\n",
        "                           configurations[:, np.newaxis],  # Reshape for concatenation\n",
        "                           y_values[:, np.newaxis]), axis=1)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "class BernoulliApproximator(nn.Module):\n",
        "  def __init__(self, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(2, hidden_dim)\n",
        "    self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.input1 = None\n",
        "    self.input2 = None\n",
        "    self.input3 = None\n",
        "    self.input4 = None\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.input1 = self.linear1(x)\n",
        "    out = self.relu(self.input1)\n",
        "    self.input2 = self.linear2(out)\n",
        "    out = self.relu(self.input2)\n",
        "    self.input3 = self.linear3(out)\n",
        "    out = self.relu(self.input3)\n",
        "    self.input4 = self.linear4(out)\n",
        "    out = torch.sigmoid(self.input4)\n",
        "    return out\n",
        "\n",
        "probs = torch.rand(10000, 1)\n",
        "data_list = []\n",
        "for p in probs.flatten():  # Iterate over probabilities\n",
        "    data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "probs = torch.rand(5000, 1)*0.1\n",
        "for p in probs.flatten():  # Iterate over probabilities\n",
        "    data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "probs = 0.9 * torch.rand(5000, 1)*0.1\n",
        "for p in probs.flatten():  # Iterate over probabilities\n",
        "    data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "data = np.vstack(data_list)  # Combine the data from all probabilities\n",
        "np.random.shuffle(data)\n",
        "# Create X and Y\n",
        "X = torch.tensor(data[:, :2], dtype=torch.float32)  # Input (x and binary variables)\n",
        "Y = torch.tensor(data[:, 2], dtype=torch.float32)  # Output (y)\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = BernoulliApproximator(hidden_dim=32).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "for epoch in range(1000000):\n",
        "  if epoch % 10000==0:\n",
        "    probs = torch.rand(1000, 1)\n",
        "    data_list = []\n",
        "    for p in probs.flatten():  # Iterate over probabilities\n",
        "        data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "    probs = torch.rand(500, 1)*0.1\n",
        "    for p in probs.flatten():  # Iterate over probabilities\n",
        "        data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "    probs = 0.9 * torch.rand(500, 1)*0.1\n",
        "    for p in probs.flatten():  # Iterate over probabilities\n",
        "        data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "    data = np.vstack(data_list)  # Combine the data from all probabilities\n",
        "    np.random.shuffle(data)\n",
        "    # Create X and Y\n",
        "    X = torch.tensor(data[:, :2], dtype=torch.float32).to(device)  # Input (x and binary variables)\n",
        "    Y = torch.tensor(data[:, 2], dtype=torch.float32).to(device)  # Output (y)\n",
        "    total_loss = 0\n",
        "    i=0\n",
        "  i+=1\n",
        "  optimizer.zero_grad()\n",
        "  #  print(X[i])\n",
        "  result = model(X)\n",
        "   # print(result)\n",
        "   # print(Y[i])\n",
        "  loss = nn.BCELoss()(result.squeeze(),Y)\n",
        "  total_loss+=loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 1000 == 0:\n",
        "    print(epoch, ': avg_loss: ',total_loss/i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0diz5OSz-9s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGmLBtyFKG7z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Data Generation\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def generate_sigmoid_like_data(num_samples=10000, range_min=-10, range_max=10):\n",
        "    x_values = np.random.uniform(low=range_min, high=range_max, size=num_samples)\n",
        "\n",
        "    # Function 1: Approaching 0 from the left\n",
        "    def func1(x):\n",
        "        return 1 / (1 + np.exp(-x+4.59512))\n",
        "\n",
        "    # Function 2: Approaching 1 from the right\n",
        "    def func2(x):\n",
        "        return 1 / (1 + np.exp(-x-3.59512))\n",
        "\n",
        "    # Initialize y_values\n",
        "    y_values = np.zeros_like(x_values)\n",
        "\n",
        "    # Apply functions based on x value\n",
        "    mask1 = x_values < 0\n",
        "    mask2 = x_values >= 1\n",
        "    y_values[mask1] = func1(x_values[mask1])\n",
        "    y_values[mask2] = func2(x_values[mask2])\n",
        "\n",
        "    # For 0 <= x < 1, y = x\n",
        "    mask3 = (x_values >= 0) & (x_values < 1)\n",
        "    y_values[mask3] = x_values[mask3]\n",
        "\n",
        "    data = np.concatenate((x_values[:, np.newaxis], y_values[:, np.newaxis]), axis=1)\n",
        "    return data\n",
        "\n",
        "def generate_sigmoid_data(num_samples=10000, range_min=-10, range_max=10):\n",
        "    x_values = np.random.uniform(low=range_min, high=range_max, size=num_samples)\n",
        "\n",
        "    # Initialize y_values\n",
        "    y_values = 1 / (1 + np.exp(-x_values))\n",
        "\n",
        "    data = np.concatenate((x_values[:, np.newaxis], y_values[:, np.newaxis]), axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "# Network Architecture\n",
        "class SigmoidNetwork(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(1, hidden_dim)  # Input is a single x value\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.linear1(x))\n",
        "        out = self.relu(self.linear2(out))\n",
        "        out = self.relu(self.linear3(out))\n",
        "        out = self.linear4(out)\n",
        "        return out\n",
        "\n",
        "# Training Setup\n",
        "model = SigmoidNetwork(hidden_dim=16).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss is common for function approximation\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(2000000):\n",
        "    if epoch==0 or (epoch-1) % 1000 == 0:\n",
        "      data_list = []\n",
        "      data_list.append(generate_sigmoid_data())\n",
        "      data_list.append(generate_sigmoid_data(range_min=-100, range_max=100))\n",
        "      data_list.append(generate_sigmoid_data(range_min=-1, range_max=1))\n",
        "      data_list.append(generate_sigmoid_data(range_min=0, range_max=1))\n",
        "      data = np.vstack(data_list)\n",
        "      np.random.shuffle(data)\n",
        "      X = torch.tensor(data[:, 0], dtype=torch.float32).to(device)\n",
        "      Y = torch.tensor(data[:, 1], dtype=torch.float32).to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    result = model(X.unsqueeze(1))  # Add a dimension for batch processing\n",
        "    loss = criterion(result, Y.unsqueeze(1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 5000 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss {loss.item()}\")\n",
        "\n",
        "torch.save(model.state_dict(), 'true_sigmoid_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1Iy81cCC16R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "nv=5\n",
        "hidden_layers = [10,10]\n",
        "L = len(hidden_layers)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def energy(v, h,weight,bias):\n",
        "  energy = - torch.sum(v * bias[0].unsqueeze(0), 1)\n",
        "\n",
        "  for i in range(L):\n",
        "      logits = F.linear(v.double() if i==0 else h[i-1].double(),\n",
        "                        weight[i].double(), bias[i+1].double())\n",
        "\n",
        "      energy -= torch.sum(h[i] * logits, 1)\n",
        "\n",
        "  return energy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "total_nodes = nv+sum(hidden_layers)\n",
        "\n",
        "\n",
        "class InverseEnergyApproximator(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(1, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear4 = nn.Linear(hidden_dim, total_nodes)\n",
        "        self.relu = nn.GELU()\n",
        "        self.out1 = None\n",
        "        self.weight = nn.ParameterList([nn.Parameter(torch.Tensor(hidden_layers[0], nv)).to(device)])\n",
        "        for i in range(len(hidden_layers)-1):\n",
        "          self.weight.append(nn.Parameter(torch.Tensor(hidden_layers[i+1], hidden_layers[i]).to(device)))\n",
        "        self.bias = nn.ParameterList([nn.Parameter(torch.Tensor(nv)).to(device)])\n",
        "        for i in range(len(hidden_layers)):\n",
        "          self.bias.append(nn.Parameter(torch.Tensor(hidden_layers[i])).to(device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.linear1(x))\n",
        "        out = self.relu(self.linear2(out))\n",
        "        out = self.relu(self.linear3(out))\n",
        "        out = torch.sigmoid(self.linear4(out))\n",
        "        self.out1 = out\n",
        "        v = out[:,:nv]\n",
        "        out = out[:,nv:]\n",
        "        h = []\n",
        "        for l in hidden_layers:\n",
        "          h.append(out[:,:l])\n",
        "          out=out[:,l:]\n",
        "        energy1 = energy(v,h,self.weight,self.bias)\n",
        "        return energy1\n",
        "\n",
        "\n",
        "def generate_data(weight, bias, batch_size = 40000):\n",
        "    states = torch.randint(0,2,(batch_size,nv+sum(hidden_layers))).float().requires_grad_().to(device)\n",
        "    states2 = states.clone()\n",
        "    v = states[:,:nv]\n",
        "    states = states[:,nv:]\n",
        "    h = []\n",
        "    for l in hidden_layers:\n",
        "      h.append(states[:,:l])\n",
        "      states=states[:,l:]\n",
        "    output = energy(v,h,weight,bias).unsqueeze(1)\n",
        "    X = output\n",
        "    return X\n",
        "\n",
        "\n",
        "invEnergyModel = InverseEnergyApproximator(hidden_dim=512).to(device)\n",
        "optimizer = torch.optim.Adam(invEnergyModel.parameters(), lr=0.0001)\n",
        "X=torch.tensor(1.0)\n",
        "total_loss = 0\n",
        "i=0\n",
        "for epoch in range(2000000):\n",
        "  X = generate_data(invEnergyModel.weight,invEnergyModel.bias)\n",
        "  X = X.clone().detach().requires_grad_().to(device)\n",
        "  i+=1\n",
        "  optimizer.zero_grad()\n",
        "  result = invEnergyModel(X)\n",
        "  max1 = max(result.squeeze())/1000\n",
        "  result = result/max1\n",
        "  X = X/max1\n",
        "#  print(\"result: \",result)\n",
        " # print(\"X: \",X)\n",
        "  loss = nn.MSELoss()(result,X)\n",
        "  total_loss+=loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  del X\n",
        "  if epoch % 100 == 0:\n",
        "    print(epoch, ': avg_loss: ',total_loss/i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hibq6T5tI_b"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow.keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozCFj-rWs8wU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()  # Default distribution strategy\n",
        "\n",
        "# --- Constants & Hyperparameters ---\n",
        "nv = 5\n",
        "hidden_layers = [10, 10]\n",
        "L = len(hidden_layers)\n",
        "batch_size = 8192\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def energy(v, h, weight, bias):\n",
        "    energy = -tf.reduce_sum(v * bias[0][tf.newaxis, :], axis=1)\n",
        "\n",
        "    for i in range(L):\n",
        "        logits = tf.matmul(\n",
        "            tf.cast(v if i == 0 else h[i - 1], tf.float32),\n",
        "            tf.cast(weight[i], tf.float32)\n",
        "        ) + tf.cast(bias[i + 1], tf.float32)\n",
        "\n",
        "        energy -= tf.reduce_sum(h[i] * logits, axis=1)\n",
        "\n",
        "    return energy\n",
        "\n",
        "\n",
        "total_nodes = nv + sum(hidden_layers)\n",
        "\n",
        "\n",
        "# --- Model ---\n",
        "def create_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(1,),dtype=tf.float32),\n",
        "        layers.Dense(1024, activation='gelu',dtype=tf.float32),\n",
        "        layers.Dense(1024, activation='gelu',dtype=tf.float32),\n",
        "        layers.Dense(1024, activation='gelu',dtype=tf.float32),\n",
        "        layers.Dense(total_nodes, activation='sigmoid',dtype=tf.float32)\n",
        "    ])\n",
        "    # Create Parameter Variables\n",
        "    weights1 = [tf.Variable(tf.random.normal([nv, hidden_layers[0]]), trainable=True)]\n",
        "    for i in range(len(hidden_layers)-1):\n",
        "          weights1.append(tf.Variable(tf.random.normal([hidden_layers[i], hidden_layers[i+1]]), trainable=True))\n",
        "    biases = [tf.Variable(tf.random.normal([nv]), trainable=True)]\n",
        "    for i in range(len(hidden_layers)):\n",
        "          biases.append(tf.Variable(tf.random.normal([hidden_layers[i]]), trainable=True))\n",
        "\n",
        "    def custom_forward(inputs, training=None):\n",
        "        out = model(x)  # Pass input through the Keras model (up to sigmoid)\n",
        "\n",
        "        # Extract v and h (similar to PyTorch logic)\n",
        "        v = out[:, :nv]\n",
        "        h = tf.split(out[:, nv:], hidden_layers, axis=1)\n",
        "\n",
        "        # Call energy function\n",
        "        energy1 = model.energy(v, h, model.weights1, model.biases)\n",
        "\n",
        "        return out, energy1  # Return both the output and energy\n",
        "\n",
        "    model.call = custom_forward  # Replace model's call method\n",
        "    model.energy = energy       # Assign energy function\n",
        "    model.weights1 = weights1  # Assign weights\n",
        "    model.biases = biases\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- Dataset Generation on TPU ---\n",
        "def generate_data(model, batch_size=8192):\n",
        "    # TPU-compatible data generation using tf.random.uniform\n",
        "    states = tf.random.uniform((batch_size, total_nodes), 0, 2, dtype=tf.float32)\n",
        "\n",
        "    v = states[:, :nv]\n",
        "    h = tf.split(states[:, nv:], hidden_layers, axis=1)\n",
        "\n",
        "    output = model.energy(v, h, model.weights1, model.biases)[:, tf.newaxis]\n",
        "    return output\n",
        "\n",
        "# --- Training (Within TPU Strategy Scope) ---\n",
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "    optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "\n",
        "    for epoch in range(2000000):\n",
        "        X = generate_data(model)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            result = model.call(X)\n",
        "            max1 = tf.reduce_max(result) / 1000.0  # For scaling\n",
        "            result = result / max1\n",
        "            X = X / max1\n",
        "            loss = tf.keras.losses.MSE(result, X)\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables + model.weights + model.biases)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables + model.weights + model.biases))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss {loss.numpy()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqCWDRYqZYBg"
      },
      "outputs": [],
      "source": [
        "torch.save(model2,'greater_than4.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auWTnGV3C7QZ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1000000):\n",
        "  total_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  #  print(X[i])\n",
        "  result = model2(X)\n",
        "   # print(result)\n",
        "   # print(Y[i])\n",
        "  loss = nn.BCELoss()(result.squeeze(),Y)\n",
        "  total_loss+=loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 10000==0:\n",
        "    print(epoch, ': avg_loss: ',total_loss/len(X))\n",
        "\n",
        "\n",
        "torch.save(model2,'greater_than2.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLUjfruse-yP"
      },
      "outputs": [],
      "source": [
        "torch.save(model2,'greater_than2.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc-cLCK9_Bk_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!cp 'dbn_modelv2_deepConv.pt' /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve3egHwfzPUA"
      },
      "source": [
        "Combined VAE representation - VAE encoder for continous variables, other type of encoder for categorical variables, combined into decoder.\n",
        "\n",
        "Working with this data I see why tabular data is much harder. Training a VAE on this is much more difficult than other times I've done it. Right now I've split the data into 2, categorical and continous but I'm going to have to split them into groupings of dependencies between variables.\n",
        "\n",
        "OPTICS is clustering between datapoints but we want clustering of dependencies between variables. Even then there will be large groups of disparate variables.\n",
        "\n",
        "\n",
        "CLUSTER ACCORDING TO COMBINED ENCODED LATENT SPACE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9BxN5PUWoSW"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # Provides additional layers and functions\n",
        "\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "\n",
        "\n",
        "def train_vae(num_vae, dataloader, num_epochs, optimizer_num, device):\n",
        "    num_vae.train()\n",
        "    # Train Numerical VAE\n",
        "    for epoch in range(num_epochs):\n",
        "      num_vae.train()\n",
        "      total_loss_num = 0\n",
        "      for batch_num,_ in dataloader:  # Only iterate over numerical data\n",
        "        batch_num = batch_num.to(device)\n",
        "        optimizer_num.zero_grad()\n",
        "        recon_num, mu_num, logvar_num = num_vae(batch_num)\n",
        "        loss_num = F.mse_loss(recon_num, batch_num) + \\\n",
        "                       -0.5 * torch.sum(1 + logvar_num - mu_num.pow(2) - logvar_num.exp())\n",
        "        loss_num.backward()\n",
        "        optimizer_num.step()\n",
        "        total_loss_num += loss_num.item()\n",
        "      avg_loss_num = total_loss_num / len(dataloader)\n",
        "      print(f'Epoch {epoch + 1}: Num. VAE Loss - {avg_loss_num:.4f}')\n",
        "\n",
        "\n",
        "#--- Using the loop ---\n",
        "\n",
        "# Instantiate your VAE model\n",
        "num_vae = VAE(input_size=X_num.shape[1], hidden_size=512, latent_size=16).to(device)\n",
        "\n",
        "# Optimizers for each VAE\n",
        "optimizer_num = optim.Adam(num_vae.parameters(), lr=1e-3)\n",
        "\n",
        "# Train!\n",
        "train_vae(num_vae, dataloader, num_epochs=200, optimizer_num=optimizer_num, optimizer_cat=optimizer_cat, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xVV73zMfglg"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def run_optics_and_visualize(data):\n",
        "    # Perform OPTICS clustering using the provided metric\n",
        "    optics = OPTICS(min_samples=20)\n",
        "    optics.fit(data)\n",
        "    # Get cluster labels\n",
        "    cluster_labels = optics.labels_\n",
        "\n",
        "    # Get reachability distances (useful for understanding cluster structure)\n",
        "    reachability_distances = optics.reachability_\n",
        "    # Create a dictionary for mapping cluster labels to colors\n",
        "    color_map = plt.cm.get_cmap('tab10', max(cluster_labels) + 1)  # Adjust colormap as needed\n",
        "    colors = [color_map(label) for label in cluster_labels]\n",
        "\n",
        "    # Reduce dimensionality to 2D using PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    data_reduced = pca.fit_transform(data)\n",
        "\n",
        "\n",
        "    # Plot the reduced data with cluster labels as colors\n",
        "    plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=cluster_labels)\n",
        "    plt.title('OPTICS Clusters (PCA Visualization)')\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.show()\n",
        "\n",
        "    tsne = TSNE(n_components=2)\n",
        "    data_reduced = tsne.fit_transform(data)\n",
        "\n",
        "    # Plot the reduced data with cluster labels as colors\n",
        "    plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=cluster_labels)\n",
        "    plt.title('OPTICS Clusters (t-SNE Visualization)')\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhsENycGJX9x"
      },
      "outputs": [],
      "source": [
        "run_optics_and_visualize(np.concatenate((X_num, X_cat), axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqc99903JgRJ"
      },
      "outputs": [],
      "source": [
        "run_optics_and_visualize(X_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6SX9WqCJg_q"
      },
      "outputs": [],
      "source": [
        "run_optics_and_visualize(X_cat)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}