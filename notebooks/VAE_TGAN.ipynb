{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa4nnkdUKwWj",
        "outputId": "f5a5f46d-69ef-446c-e9ed-461a9a217aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.6)\n"
          ]
        }
      ],
      "source": [
        "pip install ucimlrepo\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "X = pd.read_csv(\"censusData.csv\")"
      ],
      "metadata": {
        "id": "_1FGcBJtfwDs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1DhCnFMeKw_4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr\n",
        "import pyarrow as pa\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, attention_hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(attention_hidden_size, attention_hidden_size).to(device)\n",
        "\n",
        "    def forward(self, encoder_outputs):\n",
        "      # Transform x using a linear layer; output shape will be (sq, b, hidden_size)\n",
        "      x_transformed = self.linear(encoder_outputs)\n",
        "      # Step 2: Compute attention scores using softmax across the sequence dimension (sq)\n",
        "      # Attention scores shape: (sq, b, hidden_size) -> (b, sq, hidden_size) for softmax\n",
        "      x_transposed = x_transformed.transpose(0, 1)  # Transposing for softmax operation\n",
        "      attention_scores = F.softmax(x_transposed, dim=1)  # Applying softmax; shape remains (b, sq, hidden_size)\n",
        "      # Step 3: Apply attention scores to the original input tensor\n",
        "      # For weighted sum, first transpose x back: (sq, b, hidden_size) -> (b, sq, hidden_size)\n",
        "      x = encoder_outputs.transpose(0, 1)  # Transposing x to match attention_scores shape\n",
        "      # Compute the context vector as the weighted sum of the input vectors\n",
        "      # (b, sq, hidden_size) * (b, sq, hidden_size) -> (b, hidden_size) after summing over sq dimension\n",
        "      context_vector = torch.sum(attention_scores * x, dim=1)\n",
        "      return context_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size,only_z=False):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear4 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear5 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear6 = nn.Linear(hidden_size, latent_size)\n",
        "        self.linear_mu = nn.Linear(latent_size, latent_size)\n",
        "        self.linear_logvar = nn.Linear(latent_size, latent_size)\n",
        "        self.only_z = only_z\n",
        "        self.relu1 = nn.GELU()\n",
        "        self.relu2 = nn.GELU()\n",
        "        self.relu3 = nn.GELU()\n",
        "        self.relu4 = nn.GELU()\n",
        "        self.relu5 = nn.GELU()\n",
        "        self.relu6 = nn.GELU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu1(self.linear1(x))\n",
        "        out = self.relu2(self.linear2(out))\n",
        "        out = self.relu3(self.linear3(out))\n",
        "        out = self.relu4(self.linear4(out))\n",
        "        out = self.relu5(self.linear5(out))\n",
        "        out = self.relu6(self.linear6(out))\n",
        "        mu = self.linear_mu(out)\n",
        "        logvar = self.linear_logvar(out)\n",
        "\n",
        "        # Reparameterization trick (as before)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = eps.mul(std).add_(mu)\n",
        "        if self.only_z:\n",
        "          return z\n",
        "        return z, mu, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(latent_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear4 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "        self.relu1 = nn.GELU()\n",
        "        self.relu2 = nn.GELU()\n",
        "        self.relu3 = nn.GELU()\n",
        "        self.relu4 = nn.GELU()\n",
        "        self.output=None\n",
        "\n",
        "    def forward(self, z, sig=False):\n",
        "        #print(\"decoder 1: \",z.shape)\n",
        "        #print(\"decoder 1: \",z)\n",
        "        out = self.relu1(self.linear1(z))\n",
        "        out = self.relu2(self.linear2(out))\n",
        "        out = self.relu3(self.linear3(out))\n",
        "        out = self.relu4(self.linear4(out))\n",
        "        out = self.output_layer(out)\n",
        "        self.output=out\n",
        "        return out\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, latent_size, cat=False):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(input_size, hidden_size, latent_size).to(device)\n",
        "        self.decoder = Decoder(latent_size, hidden_size, input_size).to(device)\n",
        "        self.cat=cat\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.cat:\n",
        "          z, logits = self.encoder(x,True)\n",
        "          recon = self.decoder(z,True)\n",
        "          return recon, logits\n",
        "        else:\n",
        "          z, mu, logvar = self.encoder(x)\n",
        "          recon = self.decoder(z)\n",
        "          return recon, mu, logvar\n",
        "\n",
        "class ConditionalBatchNorm1d(nn.Module):\n",
        "    def __init__(self, num_features, num_conditions):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "\n",
        "        self.gamma_layer = nn.Linear(num_conditions, num_features)\n",
        "        self.beta_layer = nn.Linear(num_conditions, num_features)\n",
        "\n",
        "    def forward(self, input, condition):\n",
        "\n",
        "        out = F.batch_norm(input, None, None, training=True).to(device)  # Standard batch normalization\n",
        "        gamma = self.gamma_layer(condition).to(device)\n",
        "        beta = self.beta_layer(condition).to(device)\n",
        "\n",
        "        out = gamma * out + beta\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PVNvFy0Opu_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72d6cc61-e6fc-4ebe-d886-e8b219ff24ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48842, 107)\n",
            "torch.Size([2, 5671])\n",
            "torch.Size([5671, 131])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "rbf_hsic_matrix = torch.load('rbf_hsic_matrix_updated.pt')\n",
        "linear_hsic_matrix = torch.load('linear_hsic_matrix_updated.pt')\n",
        "mutual_information_matrix = torch.load('mutual_information_matrix.pt')\n",
        "distance_correlation_matrix = torch.load('distance_correlation_matrix.pt')\n",
        "chi2_matrix = torch.load('chi2_matrix.pt')\n",
        "theils_u_matrix = torch.load('theils_u_matrix.pt')\n",
        "cramers_v_matrix = torch.load('cramers_v_matrix.pt')\n",
        "\n",
        "def load_measure_matrix(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data['matrix'], data['feature_names']\n",
        "\n",
        "agreement_matrix, agreement_feature_names = load_measure_matrix('agreement_matrix.pkl')\n",
        "binary_matrix, binary_feature_names = load_measure_matrix('binary_matrix.pkl')\n",
        "categorical_matrix, categorical_feature_names = load_measure_matrix('categorical_matrix.pkl')\n",
        "confusion_matrix, confusion_feature_names = load_measure_matrix('confusion_matrix.pkl')\n",
        "\n",
        "num_features = rbf_hsic_matrix.shape[0]\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categorical_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country','income']\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "X_encoded = encoder.fit_transform(X[categorical_columns])\n",
        "feature_names = encoder.get_feature_names_out(categorical_columns)\n",
        "\n",
        "column_mapping = {}\n",
        "start_index = 0\n",
        "for col in categorical_columns:\n",
        "    column_mapping[col] = start_index\n",
        "    start_index += len(encoder.categories_[categorical_columns.index(col)])\n",
        "\n",
        "print(X_encoded.shape)\n",
        "\n",
        "index = []\n",
        "attr = []\n",
        "\n",
        "for i in range(num_features):\n",
        "    for j in range(i + 1, num_features):\n",
        "        index.append([i, j])\n",
        "\n",
        "        # Find the categorical columns associated with features i and j\n",
        "        col_i = next(col for col, start_idx in column_mapping.items() if start_idx <= i < start_idx + len(encoder.categories_[categorical_columns.index(col)]))\n",
        "        col_j = next(col for col, start_idx in column_mapping.items() if start_idx <= j < start_idx + len(encoder.categories_[categorical_columns.index(col)]))\n",
        "\n",
        "        # Create the categorical column vector (1 for the corresponding column, 0 otherwise)\n",
        "        categorical_col_vec = np.zeros(len(categorical_columns))\n",
        "        categorical_col_vec[categorical_columns.index(col_i)] = 1\n",
        "        categorical_col_vec[categorical_columns.index(col_j)] = 1\n",
        "\n",
        "        list1 = [linear_hsic_matrix[i, j],\n",
        "            rbf_hsic_matrix[i, j],\n",
        "            mutual_information_matrix[i, j],\n",
        "            distance_correlation_matrix[i, j],\n",
        "            chi2_matrix[i, j],\n",
        "            theils_u_matrix[i, j],\n",
        "            cramers_v_matrix[i, j]]\n",
        "       # print(agreement_matrix[i][j])\n",
        "        for measure in agreement_matrix[i][j].keys():\n",
        "          list1.append(agreement_matrix[i][j][measure])\n",
        "      #  print(binary_matrix[i][j])\n",
        "        for measure in binary_matrix[i][j].keys():\n",
        "          if measure == 'mcnemar_test':\n",
        "            list1.append(binary_matrix[i][j][measure][0])\n",
        "          else:\n",
        "            list1.append(binary_matrix[i][j][measure])\n",
        "       # print(categorical_matrix[i][j])\n",
        "        for measure in categorical_matrix[i][j].keys():\n",
        "          list1.append(categorical_matrix[i][j][measure])\n",
        "       # print(confusion_matrix[i][j])\n",
        "        for measure in confusion_matrix[i][j].keys():\n",
        "          list1.append(confusion_matrix[i][j][measure])\n",
        "        list1.extend(categorical_col_vec)\n",
        "        #for ele in list1:\n",
        "          #  print(ele, type(ele))\n",
        "      #  print(list1)\n",
        "        attr.append(list1)\n",
        "\n",
        "\n",
        "index = torch.tensor(index, dtype=torch.long).t().contiguous()\n",
        "attr = torch.tensor(attr, dtype=torch.float).to(device)\n",
        "print(index.shape)\n",
        "print(attr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YV8ZZHK56qhs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea9bb27-e20c-4e48-ac5f-2cbd67a8fc7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([48842, 5671])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch\n",
        "index1 = index[0]\n",
        "index2 = index[1]\n",
        "\n",
        "X_encoded = torch.tensor(X_encoded)\n",
        "# Optimization using torch.expand and torch.gather\n",
        "index1_expanded = index1.expand(X_encoded.shape[0], -1)\n",
        "index2_expanded = index2.expand(X_encoded.shape[0], -1)\n",
        "print(index1_expanded.shape)\n",
        "# Efficiently gather feature pairs using indexing\n",
        "features1 = torch.gather(X_encoded, dim=1, index=index1_expanded)\n",
        "features2 = torch.gather(X_encoded, dim=1, index=index2_expanded)\n",
        "\n",
        "dataset = list(zip(torch.stack([features1, features2], dim=2),X_encoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BLjGwJ9jYgU5"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "neighborhoods = defaultdict(list)\n",
        "for i in range(index.shape[1]):\n",
        "    n1_index = index1[i].item()\n",
        "    n2_index = index2[i].item()\n",
        "    neighborhoods[n1_index].append(i)\n",
        "    neighborhoods[n2_index].append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aJWvNSnBlXWp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Bernoulli\n",
        "\n",
        "\n",
        "class DeepLinear(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None):\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DeepConv(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_output_shape, neighbourhoods):\n",
        "        super().__init__()\n",
        "        self.num_variables = 107\n",
        "        self.e_features = attr.shape[1]\n",
        "        self.e_scoring_network = DeepLinear(self.num_variables+self.e_features, hidden_dim, output_dim=1)\n",
        "        self.neighborhood_agg_network = DeepLinear(len(neighbourhoods[0]),hidden_dim, n_output_shape)\n",
        "        self.output_dim = n_output_shape * len(neighbourhoods)\n",
        "        max_neighborhood_size = max(len(v) for v in neighborhoods.values())\n",
        "        neighborhood_edge_indices = torch.zeros((len(neighborhoods), max_neighborhood_size), dtype=torch.long)\n",
        "        for node_index, edge_list in neighborhoods.items():\n",
        "          neighborhood_edge_indices[node_index] = torch.tensor(edge_list)\n",
        "        self.neighbourhoods = neighborhood_edge_indices.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x.requires_grad=True\n",
        "        batch_size = x.shape[0]\n",
        "        # Save original values\n",
        "        original_x0 = x[:, :, 0] # .clone() necessary here\n",
        "        original_x1 = x[:, :, 1]\n",
        "        x_mod = torch.zeros(batch_size, x.shape[1], self.num_variables,requires_grad=True).to(device)\n",
        "        index1_expanded = index1.unsqueeze(0).expand(x_mod.shape[0], -1).to(device)\n",
        "        index2_expanded = index2.unsqueeze(0).expand(x_mod.shape[0], -1).to(device)\n",
        "\n",
        "        x_mod = torch.scatter(x_mod, 2, index1_expanded.unsqueeze(2), original_x0.unsqueeze(2))\n",
        "        x_mod = torch.scatter(x_mod, 2, index2_expanded.unsqueeze(2), original_x1.unsqueeze(2))\n",
        "        broadcasted_attr = attr.unsqueeze(0).expand(x_mod.shape[0], -1, -1)\n",
        "        final_tensor = torch.cat([x_mod, broadcasted_attr], dim=2)\n",
        "        all_edge_scores = self.e_scoring_network(final_tensor).squeeze()\n",
        "        neighborhood_edge_indices = self.neighbourhoods[None, :, :]\n",
        "        neighborhood_edge_indices=neighborhood_edge_indices.expand(all_edge_scores.shape[0],-1,-1)\n",
        "        batch_size, v, r = neighborhood_edge_indices.shape\n",
        "        e = all_edge_scores.shape[1]\n",
        "        vector1_expanded = all_edge_scores.unsqueeze(1).expand(-1, v, -1)\n",
        "        neighborhood_scores = torch.gather(vector1_expanded, 2, neighborhood_edge_indices)\n",
        "        neighborhood_outputs = self.neighborhood_agg_network(neighborhood_scores)\n",
        "        output = neighborhood_outputs.flatten(start_dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "investigate whether the 0.4 equality range makes sense, i.e if v and h enters in as between 0 and 1 (like closer to 0.5), does that carry over past gibbs sampling or mh sampling, can rows get stuck apart.\n",
        "\n"
      ],
      "metadata": {
        "id": "hWDbbEHTh-Gb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pGIG2eEiRQPJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random\n",
        "hidden_layers = [512,128,32]\n",
        "L = len(hidden_layers)\n",
        "class ComparatorNetwork(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(2, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.x=None\n",
        "        self.out1 = None\n",
        "        self.out2 = None\n",
        "        self.out3 = None\n",
        "        self.out4 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x=x\n",
        "        out = self.relu(self.linear1(x))\n",
        "        self.out1 = out\n",
        "        out = self.relu(self.linear2(out))\n",
        "        self.out2 = out\n",
        "        out = self.relu(self.linear3(out))\n",
        "        self.out3 = out\n",
        "        out = torch.sigmoid(self.linear4(out))\n",
        "        self.out4 = out\n",
        "        return out\n",
        "\n",
        "class BernoulliApproximator(nn.Module):\n",
        "  def __init__(self, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(2, hidden_dim)\n",
        "    self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.relu(self.linear1(x))\n",
        "    out = self.relu(self.linear2(out))\n",
        "    out = self.relu(self.linear3(out))\n",
        "    out = torch.sigmoid(self.linear4(out))\n",
        "    return out\n",
        "\n",
        "model = torch.load('bernoullimodel9.pth',map_location=device)\n",
        "\n",
        "\n",
        "model2 = torch.load('greater_than4.pth',map_location=device)\n",
        "\n",
        "import torch.nn.utils as nn_utils\n",
        "max_norm = 100\n",
        "import time\n",
        "class BernoulliSampleFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, probabilities, random_numbers):\n",
        "        result = torch.zeros_like(probabilities)\n",
        "        inputs = []\n",
        "        for i in range(probabilities.shape[1]):\n",
        "          with torch.enable_grad():\n",
        "            input = torch.cat((probabilities[:, i].unsqueeze(1), random_numbers[:, i].unsqueeze(1)), dim=1).clone().requires_grad_(True)\n",
        "            inputs.append(input)\n",
        "            result[:, i] = model(input).squeeze().detach()\n",
        "        ctx._dict = model.state_dict()\n",
        "        ctx.save_for_backward(*inputs)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "      grad_output = F.normalize(grad_output, p=2.0, dim=1)\n",
        "      inputs = ctx.saved_tensors\n",
        "      toReturn = torch.zeros_like(grad_output)\n",
        "      toReturn2 = torch.zeros_like(grad_output)\n",
        "      for i in range(toReturn.shape[1]):\n",
        "        input = inputs[i]\n",
        "        delta = grad_output[:,i].unsqueeze(1)\n",
        "        for y in reversed(range(1,5)):\n",
        "          strin = \"linear\"+str(y)+'.weight'\n",
        "          weights = ctx._dict[strin]\n",
        "          delta = delta @ weights.clone()\n",
        "        toReturn[:,i] = delta[:,0]\n",
        "        toReturn2[:,i] = delta[:,1]\n",
        "      return toReturn, toReturn2\n",
        "\n",
        "\n",
        "\n",
        "class GreaterThanFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, a, b):\n",
        "        result = torch.zeros_like(a)\n",
        "        inputs = []\n",
        "        for i in range(a.shape[1]):\n",
        "          input = torch.concat((a[:,i].unsqueeze(1),b[:,i].unsqueeze(1)),dim=1)\n",
        "          inputs.append(input)\n",
        "          result[:,i] = model2(input).squeeze()\n",
        "        ctx._dict = model.state_dict()\n",
        "        ctx.save_for_backward(*inputs)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "      grad_output = F.normalize(grad_output, p=2.0, dim=1)\n",
        "      inputs = ctx.saved_tensors\n",
        "      toReturn1 = torch.zeros_like(grad_output)\n",
        "      toReturn2 = torch.zeros_like(grad_output)\n",
        "      for i in range(toReturn1.shape[1]):\n",
        "        input = inputs[i]\n",
        "        delta = grad_output[:,i].unsqueeze(1)\n",
        "        for y in reversed(range(1,5)):\n",
        "          strin = \"linear\"+str(y)+'.weight'\n",
        "          weights = ctx._dict[strin]\n",
        "          delta = delta @ weights.clone()\n",
        "        toReturn1[:,i] = delta[:,0]\n",
        "        toReturn2[:,i] = delta[:,1]\n",
        "      return toReturn1, toReturn2\n",
        "\n",
        "\n",
        "from torch import autograd, nn\n",
        "\n",
        "def energy(v, *params):\n",
        "  h = params[:L]\n",
        "  weight = params[L:L*2]\n",
        "  bias = params[L*2:]\n",
        "  energy = - torch.sum(v * bias[0].unsqueeze(0), 1)\n",
        "  for i in range(L):\n",
        "      logits = F.linear(v if i==0 else h[i-1], weight[i], bias[i+1])\n",
        "      energy -= torch.sum(h[i] * logits, 1)\n",
        "  return energy\n",
        "\n",
        "\n",
        "class MHStepFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, v, fix_v, rand_v, rand_h, rand_u, *params):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "       # print(\"params2: \",params)\n",
        "        h = params[:L]\n",
        "        weight = params[L:L*2]\n",
        "        bias = params[L*2:]\n",
        "        fix_v = fix_v==1.0\n",
        "        temp = []\n",
        "        for x in [rand_v,rand_h,rand_u]:\n",
        "          if isinstance(x, torch.Tensor) and x.numel() == 1 and x.item() == 0.0:\n",
        "            temp.append(None)\n",
        "          else:\n",
        "            temp.append(x)\n",
        "        rand_v = temp[0]\n",
        "        rand_h = temp[1]\n",
        "        rand_u = temp[2]\n",
        "       # print(\"h: \",h)\n",
        "        ctxs = []\n",
        "        if fix_v:\n",
        "            v_ = v\n",
        "        else:\n",
        "            if rand_v is None:\n",
        "                v_ = torch.empty_like(v).bernoulli_()\n",
        "            else:\n",
        "                v_ = (rand_v < 0.5).float()\n",
        "\n",
        "        if rand_h is None:\n",
        "            h_ = [torch.empty_like(h[i]).bernoulli_() for i in range(L)]\n",
        "        else:\n",
        "            h_ = [(rand_h[i] < 0.5).float() for i in range(L)]\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "            params.append(tensor)\n",
        "        for parameter in weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in bias:\n",
        "            params.append(parameter)\n",
        "        energy1 = energy(v,*params)\n",
        "        energy1CTX = tuple((v, *params))\n",
        "        ctxs.append(energy1CTX)\n",
        "        params = []\n",
        "        for tensor in h_:\n",
        "            params.append(tensor)\n",
        "        for parameter in weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in bias:\n",
        "            params.append(parameter)\n",
        "        energy2 = energy(v_,*params)\n",
        "        energy2CTX = tuple((v_, *params))\n",
        "        ctxs.append(energy2CTX)\n",
        "        log_ratio = energy1 - energy2\n",
        "        toSave = []\n",
        "        toSave.append(log_ratio)\n",
        "        if rand_u is None:\n",
        "            input1 = log_ratio.exp().clamp(0,1).unsqueeze(1)\n",
        "            random_numbers = torch.randint(0, 20, input1.shape).float().to(device)\n",
        "            accepted = BernoulliSampleFunction.apply(input1,random_numbers)\n",
        "            ctxs.append([input1,random_numbers])\n",
        "        else:\n",
        "            accepted = GreaterThanFunction.apply(log_ratio.exp().unsqueeze(1),rand_u.unsqueeze(1))\n",
        "            ctxs.append([log_ratio.exp().unsqueeze(1), rand_u.unsqueeze(1)])\n",
        "        if not fix_v:\n",
        "            toSave.append(v_)\n",
        "            toSave.append(v)\n",
        "        for i in range(L):\n",
        "          toSave.append(h_[i])\n",
        "          toSave.append(h[i])\n",
        "        accepted = torch.round(accepted,decimals=0).bool()\n",
        "\n",
        "        params = []\n",
        "        for parameter in weight:\n",
        "          params.append(parameter)\n",
        "        for parameter in bias:\n",
        "          params.append(parameter)\n",
        "        for sav in toSave:\n",
        "          params.append(sav)\n",
        "        for lis in ctxs:\n",
        "          params.append(torch.tensor(len(lis)))\n",
        "          params.extend(lis)\n",
        "        savLength = torch.tensor(len(toSave))\n",
        "        if not fix_v:\n",
        "            v = torch.where(accepted, v_, v)\n",
        "        h = [torch.where(accepted, h_[i], h[i]) for i in range(L)]\n",
        "        if rand_u is None:\n",
        "          rand_u = torch.tensor(0.0)\n",
        "        fix_v = torch.tensor(float(fix_v))\n",
        "        accepted = accepted.float()\n",
        "        ctx.save_for_backward(accepted, fix_v, rand_u,savLength, *params)\n",
        "        return v, *h\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_v, *grad_h):\n",
        "        accepted, fix_v, rand_u,savLength, *params = ctx.saved_tensors\n",
        "        L=3\n",
        "        if len(rand_u.shape)==0 :\n",
        "          rand_u = None\n",
        "        fix_v = fix_v==1.0\n",
        "        accepted = accepted.bool()\n",
        "        weight = params[:L]\n",
        "        bias = params[L:L*2+1]\n",
        "        toSave = params[L*2+1:L*2+1+savLength.item()]\n",
        "        ctxTensors = params[L*2+1+savLength.item():]\n",
        "        ctxTuples = []\n",
        "        i = 0\n",
        "        while i < len(ctxTensors):\n",
        "            tuple_length = ctxTensors[i].item()\n",
        "            start = i + 1  # Start index of tuple elements\n",
        "            end = start + tuple_length\n",
        "            ctxTuples.append(tuple(ctxTensors[start:end]))\n",
        "            i = end\n",
        "        ctx1 = ctxTuples[0]\n",
        "        ctx2 = ctxTuples[1]\n",
        "        ctx3 = ctxTuples[2]\n",
        "        grad_h = list(grad_h)\n",
        "        grad_weight = [torch.zeros_like(w) for w in weight]\n",
        "        grad_bias = [torch.zeros_like(b) for b in bias]\n",
        "        toSave = list(toSave)\n",
        "        if not fix_v:\n",
        "          d_accepted = torch.sum((toSave[1]-toSave[2]) * grad_v, dim=1, keepdim=True)\n",
        "          for i in range(len(grad_h)):\n",
        "            d_accepted = d_accepted +torch.sum((toSave[3+i*2]-toSave[3+(i*2)+1]) * grad_h[i], dim=1, keepdim=True)\n",
        "        else:\n",
        "          d_accepted = torch.sum((toSave[1]-toSave[2]) * grad_h[0], dim=1, keepdim=True)\n",
        "          for i in range(1,len(grad_h)):\n",
        "            d_accepted = d_accepted +torch.sum((toSave[1+i*2]-toSave[1+(i*2)+1]) * grad_h[i], dim=1, keepdim=True)\n",
        "        if rand_u is None:\n",
        "          with torch.enable_grad():\n",
        "            input = (ctx3[0].detach().requires_grad_(), ctx3[1].detach().requires_grad_())\n",
        "            accepted1 = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "            d_log_ratio_exp, _ = autograd.grad(accepted1, input, d_accepted)\n",
        "        else:\n",
        "          with torch.enable_grad():\n",
        "            input = (ctx3[0].detach().requires_grad_(), ctx3[1].detach().requires_grad_())\n",
        "            accepted1 = GreaterThanFunction.apply(input[0],input[1])\n",
        "            d_log_ratio_exp, _ = autograd.grad(accepted1, input, d_accepted)\n",
        "        d_log_ratio = d_log_ratio_exp * toSave[0].exp().unsqueeze(1)\n",
        "        with torch.enable_grad():\n",
        "            v1 = ctx1[0].detach().requires_grad_()\n",
        "            params1 = ctx1[1:]\n",
        "            params1 = [item.detach().requires_grad_() for item in params1]\n",
        "            input = (v1, *params1)\n",
        "            energy8 = energy(*input)\n",
        "            if d_log_ratio.shape[0]==1:\n",
        "              d_log_ratio= d_log_ratio.squeeze(1)\n",
        "            else:\n",
        "              d_log_ratio=d_log_ratio.squeeze()\n",
        "            v1, *params1 = autograd.grad(energy8, input, d_log_ratio)\n",
        "        params1 = list(params1)\n",
        "        h1 = params1[:L]\n",
        "        weight1 = params1[L:L*2]\n",
        "        bias1 = params1[L*2:]\n",
        "        with torch.enable_grad():\n",
        "            v2 = ctx2[0].detach().requires_grad_()\n",
        "            params2 = ctx2[1:]\n",
        "            params2 = [item.detach().requires_grad_() for item in params2]\n",
        "            input = (v2, *params2)\n",
        "            energy8 = energy(*input)\n",
        "            v2, *params2 = autograd.grad(energy8, input, -1*d_log_ratio)\n",
        "        params2 = list(params2)\n",
        "        weight2 = params2[L:L*2]\n",
        "        bias2 = params2[L*2:]\n",
        "        if not fix_v:\n",
        "            grad_v = torch.where(accepted,0,grad_v)\n",
        "        for i in range(len(grad_h)):\n",
        "            grad_h[i] = torch.where(accepted,0,grad_h[i])\n",
        "        grad_v += v1\n",
        "        for i in range(len(grad_h)):\n",
        "          grad_h[i]+=h1[i]\n",
        "        for i in range(len(grad_weight)):\n",
        "          grad_weight[i] += (weight1[i]-weight2[i])\n",
        "        for i in range(len(grad_bias)):\n",
        "          grad_bias[i] += (bias1[i]-bias2[i])\n",
        "        grads = []\n",
        "        for tensor in grad_h:\n",
        "            grads.append(tensor)\n",
        "        for parameter in grad_weight:\n",
        "          grads.append(parameter)\n",
        "        for parameter in grad_bias:\n",
        "          grads.append(parameter)\n",
        "        return grad_v, None, None, None, None, *grads\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "from torch.autograd import Function\n",
        "\n",
        "class GibbsStepFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, v,fix_v, rand_v, rand_h, rand_u, rand_z, T, *params):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "        fix_v= fix_v==1.0\n",
        "        params = list(params)\n",
        "        h = params[:L]\n",
        "        weight = params[L:L*2]\n",
        "        bias = params[L*2:]\n",
        "\n",
        "        temp = []\n",
        "        for x in [rand_v,rand_h,rand_u,rand_z]:\n",
        "          if isinstance(x, torch.Tensor) and x.numel() == 1 and x.item() == 0.0:\n",
        "            temp.append(None)\n",
        "          else:\n",
        "            temp.append(x)\n",
        "        rand_v = temp[0]\n",
        "        rand_h = temp[1]\n",
        "        rand_u = temp[2]\n",
        "        rand_z = temp[3]\n",
        "        if rand_u is None:\n",
        "            rand_u = torch.rand(N, device=device)\n",
        "        even = rand_u < 0.5\n",
        "        odd = even.logical_not()\n",
        "        ctx_l = []\n",
        "        ctxID = []\n",
        "        toSave = []\n",
        "        toSaveID = []\n",
        "        if even.sum() > 0:\n",
        "          #  print(\"TEST\",)\n",
        "            if not fix_v:\n",
        "                logits = F.linear(h[0][even],\n",
        "                                  weight[0].t(), bias[0])\n",
        "                toSaveID.append(15)\n",
        "                toSave.append(h[0][even])\n",
        "\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(18)\n",
        "                    v = torch.scatter(v, 0, even.nonzero().repeat(1,v.shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(14)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_v is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).float().to(device)\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(6)\n",
        "                        v =  torch.scatter(v,0,even.nonzero().repeat(1,v.shape[1]),sample)\n",
        "                      #  print(\"v_2: \",v.grad)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_v[even])\n",
        "                        ctx_l.append([sigLogits,rand_v[even]])\n",
        "                        ctxID.append(17)\n",
        "                        v = torch.scatter(v,0,even.nonzero().repeat(1,v.shape[1]),sample)\n",
        "\n",
        "            for i in range(1, len(h), 2):\n",
        "              #  print(\"TEST2\")\n",
        "                logits = F.linear(h[i-1][even], weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][even], weight[i+1].t(), None)\n",
        "                    toSaveID.append(12)\n",
        "                    toSave.append(h[i+1][even])\n",
        "\n",
        "                toSaveID.append(13)\n",
        "                toSave.append(h[i-1][even])\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(16)\n",
        "                    h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(11)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).float().to(device)\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(5)\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_h[i][even])\n",
        "                        ctx_l.append([sigLogits,rand_h[i][even]])\n",
        "                        ctxID.append(15)\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "\n",
        "            for i in range(0, len(h), 2):\n",
        "               # print(\"TEST3\")\n",
        "                logits = F.linear(v[even] if i==0 else h[i-1][even],\n",
        "                                  weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][even], weight[i+1].t(), None)\n",
        "                    toSaveID.append(9)\n",
        "                    toSave.append(h[i+1][even])\n",
        "\n",
        "                toSaveID.append(10)\n",
        "                toSave.append(v[even] if i==0 else h[i-1][even])\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(14)\n",
        "                    h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSaveID.append(8)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).float().to(device)\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(4)\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_h[i][even])\n",
        "                        ctx_l.append([sigLogits,rand_h[i][even]])\n",
        "                        ctxID.append(13)\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "        if odd.sum() > 0:\n",
        "            for i in range(0, len(h), 2):\n",
        "                logits = F.linear(v[odd] if i==0 else h[i-1][odd], weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][odd], weight[i+1].t(), None)\n",
        "                    toSaveID.append(6)\n",
        "                    toSave.append(h[i+1][odd])\n",
        "\n",
        "                toSaveID.append(7)\n",
        "                toSave.append(v[odd] if i==0 else h[i-1][odd])\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(12)\n",
        "                    h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(5)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).float().to(device)\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(3)\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_h[i][odd])\n",
        "                        ctx_l.append([sigLogits,rand_h[i][odd]])\n",
        "                        ctxID.append(11)\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "\n",
        "            if not fix_v:\n",
        "                logits = F.linear(h[0][odd], weight[0].t(), bias[0])\n",
        "                toSaveID.append(4)\n",
        "                toSave.append(h[0][odd])\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(10)\n",
        "                    v = torch.scatter(v, 0, odd.nonzero().repeat(1,v.shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(3)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_v is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).float().to(device)\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(2)\n",
        "                        v = torch.scatter(v,0,odd.nonzero().repeat(1,v.shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_v[odd])\n",
        "                        ctx_l.append([sigLogits,rand_v[odd]])\n",
        "                        ctxID.append(9)\n",
        "                        v = torch.scatter(v,0,odd.nonzero().repeat(1,v.shape[1]),sample)\n",
        "\n",
        "            for i in range(1, len(h), 2):\n",
        "                logits = F.linear(h[i-1][odd], weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][odd], weight[i+1].t(), None)\n",
        "                    toSaveID.append(1)\n",
        "                    toSave.append(h[i+1][odd])\n",
        "                toSaveID.append(2)\n",
        "                toSave.append(h[i-1][odd])\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(8)\n",
        "                    h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSaveID.append(0)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).float().to(device)\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(1)\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_h[i][odd])\n",
        "                        ctx_l.append([sigLogits,rand_h[i][odd]])\n",
        "                        ctxID.append(7)\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "          params.append(tensor)\n",
        "        for parameter in weight:\n",
        "          params.append(parameter)\n",
        "        for parameter in bias:\n",
        "          params.append(parameter)\n",
        "        saveLen = torch.tensor(len(toSave))\n",
        "        for sav in toSave:\n",
        "          params.append(sav)\n",
        "        for tup in ctx_l:\n",
        "            params.append(torch.tensor(len(tup)))\n",
        "            params.extend(tup)\n",
        "\n",
        "        ctxIDs = torch.tensor(ctxID)\n",
        "        toSaveID = torch.tensor(toSaveID)\n",
        "        ctx.save_for_backward(v,even, odd,torch.tensor(fix_v), rand_v, rand_h, rand_u, rand_z, torch.tensor(T), saveLen, ctxIDs,toSaveID, *params)\n",
        "        return v,  *h\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_v, *grad_h):\n",
        "        v,even, odd,fix_v, rand_v, rand_h, rand_u, rand_z, T, saveLen, ctxIDs, toSaveID, *params = ctx.saved_tensors\n",
        "        params = list(params)\n",
        "        h = params[:L]\n",
        "        weight = params[L:L*2]\n",
        "        bias = params[L*2:L*3+1]\n",
        "        toSave = params[L*3+1:L*3+1+saveLen]\n",
        "        ctxTensors = params[L*3+1+saveLen:]\n",
        "        ctxTuples = []\n",
        "        i = 0\n",
        "        while i < len(ctxTensors):\n",
        "            tuple_length = ctxTensors[i]\n",
        "            start = i + 1  # Start index of tuple elements\n",
        "            end = start + tuple_length\n",
        "            ctxTuples.append(list(ctxTensors[start:end]))\n",
        "            i = end\n",
        "        grad_v2return = torch.zeros_like(grad_v)\n",
        "        h2return = []\n",
        "        for h5 in grad_h:\n",
        "          h2return.append(torch.zeros_like(h5))\n",
        "        grad_weight = []\n",
        "        grad_bias = []\n",
        "        for i in range(L):\n",
        "            grad_weight.append(torch.zeros_like(weight[i]))\n",
        "        for i in range(L+1):\n",
        "            grad_bias.append(torch.zeros_like(bias[i]))\n",
        "        even_v = v[even]\n",
        "        odd_v = v[odd]\n",
        "        even_h = []\n",
        "        odd_h = []\n",
        "        grad_h = list(grad_h)\n",
        "        for gh in grad_h:\n",
        "          even_h.append(gh[even])\n",
        "        for gh in grad_h:\n",
        "          odd_h.append(gh[odd])\n",
        "        h = list(h)\n",
        "        ctxIDs2 = []\n",
        "        for ctID in list(ctxIDs):\n",
        "          ctxIDs2.append(int(ctID))\n",
        "        toSaveID2 = []\n",
        "        for tsID in list(toSaveID):\n",
        "          toSaveID2.append(int(tsID))\n",
        "        ctxIDs = ctxIDs2\n",
        "        toSaveID = toSaveID2\n",
        "      #  print(\"to save: \",toSave)\n",
        "       # print(\"to save ID: \",toSaveID)\n",
        "\n",
        "        ctxTensors = reversed(ctxTuples)\n",
        "        ctxIDs = reversed(ctxIDs)\n",
        "        ctx_queues = defaultdict(deque)\n",
        "        for obj, category_id in zip(ctxTensors, ctxIDs):\n",
        "                ctx_queues[category_id].appendleft(obj)\n",
        "\n",
        "        toSave = reversed(toSave)\n",
        "        toSaveID = reversed(toSaveID)\n",
        "        save_queues = defaultdict(deque)\n",
        "        for obj, category_id in zip(toSave, toSaveID):\n",
        "                save_queues[category_id].appendleft(obj)\n",
        "\n",
        "        if odd.sum() > 0:\n",
        "          for i in reversed(range(1, len(h), 2)):\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                input = ctx_queues[8].pop()\n",
        "                input[0] = input[0].detach().requires_grad_()\n",
        "                input[1] = input[1].detach().requires_grad_()\n",
        "                sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                d_logits, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "            else:\n",
        "              if rand_h is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[1].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[7].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "              temp = save_queues[0].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "\n",
        "            if i+1<len(h):\n",
        "              input1 = save_queues[1].pop()\n",
        "              grad_weight[i+1] += (d_logits.t() @ input1).t()\n",
        "              odd_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            grad_weight[i] += d_logits.t() @ save_queues[2].pop()\n",
        "            odd_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "          if not fix_v:\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                  input = ctx_queues[10].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logits, _ = autograd.grad(sample, (input[0],input[1]), odd_v)\n",
        "            else:\n",
        "              if rand_v is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[2].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_v)\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[9].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_v)\n",
        "              temp = save_queues[3].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "            grad_weight[0] += (d_logits.t() @ save_queues[4].pop()).t()\n",
        "            odd_h[0] += d_logits @ weight[0].t()\n",
        "            grad_bias[0] += d_logits.sum(0)\n",
        "          for i in reversed(range(0,len(h),2)):\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                  input = ctx_queues[12].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logits, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "            else:\n",
        "              if rand_h is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[3].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[11].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "              temp = save_queues[5].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "            if i+1 < len(h):\n",
        "              grad_weight[i+1] += (d_logits.t() @ save_queues[6].pop()).t()\n",
        "              odd_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            temp = save_queues[7].pop()\n",
        "            grad_weight[i] += d_logits.t() @ temp\n",
        "            if i==0:\n",
        "              odd_v += d_logits @ weight[i]\n",
        "            else:\n",
        "              odd_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "        if even.sum() > 0:\n",
        "          for i in reversed(range(0, len(h), 2)):\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                  input = ctx_queues[14].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logits, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "            else:\n",
        "              if rand_h is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[4].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[13].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "              temp = save_queues[8].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "\n",
        "            if i+1<len(h):\n",
        "              grad_weight[i+1] += (d_logits.t() @ save_queues[9].pop()).t()\n",
        "              even_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            grad_weight[i] += d_logits.t() @ save_queues[10].pop()\n",
        "            if i==0:\n",
        "              even_v += d_logits @ weight[i]\n",
        "            else:\n",
        "              even_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "          for i in reversed(range(1, len(h), 2)):\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                  input = ctx_queues[16].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logits, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "            else:\n",
        "              if rand_h is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[5].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[15].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "              temp = save_queues[11].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "            if i+1<len(h):\n",
        "              grad_weight[i+1] += (d_logits.t() @ save_queues[12].pop()).t()\n",
        "              even_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            grad_weight[i] += d_logits.t() @ save_queues[13].pop()\n",
        "            even_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "          if not fix_v:\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                  input = ctx_queues[18].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logits, _ = autograd.grad(sample, (input[0],input[1]), even_v)\n",
        "            else:\n",
        "              if rand_v is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[6].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_v)\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[17].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_v)\n",
        "              temp = save_queues[14].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "            grad_weight[0] += (d_logits.t() @ save_queues[15].pop()).t()\n",
        "            even_h[0] += d_logits @ weight[0].t()\n",
        "            grad_bias[0] += d_logits.sum(0)\n",
        "        grad_v2return[even] = even_v\n",
        "        grad_v2return[odd] = odd_v\n",
        "        grad_h2return = []\n",
        "        for ind, h7 in enumerate(h2return):\n",
        "          h7[even] = even_h[ind]\n",
        "          h7[odd] = odd_h[ind]\n",
        "          grad_h2return.append(h7)\n",
        "        grads = []\n",
        "        for tensor in grad_h2return:\n",
        "            grads.append(tensor)\n",
        "        for parameter in grad_weight:\n",
        "          grads.append(parameter)\n",
        "        for parameter in grad_bias:\n",
        "          grads.append(parameter)\n",
        "       # print(\"gradv2: \",grad_v2return)\n",
        "       # print([item for item in grads])\n",
        "        return grad_v2return, None, None, None, None, None, None, *grads\n",
        "\n",
        "\n",
        "class DBM(nn.Module):\n",
        "    def __init__(self, nv, hidden_layers, ComparatorNetwork):\n",
        "        super().__init__()\n",
        "        self.input_layer = DeepConv(128,1,neighborhoods)\n",
        "        self.weight = nn.ParameterList([nn.Parameter(torch.Tensor(hidden_layers[0], nv))])\n",
        "        for i in range(len(hidden_layers)-1):\n",
        "          self.weight.append(nn.Parameter(torch.Tensor(hidden_layers[i+1], hidden_layers[i])))\n",
        "        self.bias = nn.ParameterList([nn.Parameter(torch.Tensor(nv))])\n",
        "        for i in range(len(hidden_layers)):\n",
        "          self.bias.append(nn.Parameter(torch.Tensor(hidden_layers[i])))\n",
        "\n",
        "        self.nv = nv\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.L = len(hidden_layers)\n",
        "\n",
        "        self.output_layer = Decoder(hidden_layers[-1],128,nv)\n",
        "        self.dummy = torch.tensor(0.0).requires_grad_()\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for w in self.weight:\n",
        "            nn.init.orthogonal_(w)\n",
        "\n",
        "        for b in self.bias:\n",
        "            nn.init.zeros_(b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        v_prob = torch.sigmoid(self.input_layer(x).squeeze())\n",
        "        N = v_prob.size(0)\n",
        "        device = x.device\n",
        "        input = v_prob.clone()\n",
        "        assert self.L != 1\n",
        "        energy_pos_samples = self.positive_phase(3, N, v_prob)\n",
        "        print(\"energy_pos: \",torch.mean(torch.stack(energy_pos_samples)))\n",
        "        energy_neg_samples = self.negative_phase(5, N)\n",
        "        print(\"energy_neg: \",torch.mean(torch.stack(energy_neg_samples)))\n",
        "        energy_loss = torch.mean(torch.stack(energy_pos_samples)) - torch.mean(torch.stack(energy_neg_samples))\n",
        "        for i in range(self.L):\n",
        "          if i==0:\n",
        "            input = F.linear(input+self.bias[0], self.weight[i], self.bias[i+1])\n",
        "          else:\n",
        "            input = F.linear(input, self.weight[i], self.bias[i+1])\n",
        "        return energy_loss, self.output_layer(input,True)\n",
        "    def positive_phase(self, num_samples, N, v_prob):\n",
        "      energy_pos_samples = []  # Store energy samples\n",
        "      for _ in range(num_samples):\n",
        "        v = self.bernoulli_sample(v_prob)\n",
        "        h = []\n",
        "        for i in range(self.L):\n",
        "          h_i = torch.full((N, self.hidden_layers[i]), 0.5, device=device,requires_grad=True)\n",
        "          h_i = self.bernoulli_sample(h_i)\n",
        "          h.append(h_i)\n",
        "        v, h = self.local_search(v, h, True)\n",
        "        v, h = self.gibbs_step(v, h, True)\n",
        "        energy_pos = self.coupling(v, h, True)\n",
        "        energy_pos_samples.append(energy_pos)\n",
        "      return energy_pos_samples\n",
        "\n",
        "    def negative_phase(self, num_samples, N):\n",
        "      energy_neg_samples = []  # Store energy samples\n",
        "\n",
        "      for _ in range(num_samples):\n",
        "        v = self.bernoulli_sample(torch.full((N, self.nv), 0.5, device=device, requires_grad=True))\n",
        "        h = []\n",
        "        for i in range(self.L):\n",
        "            probs = torch.full((N, self.hidden_layers[i]), 0.5, device=device, requires_grad=True)\n",
        "            h_i = self.bernoulli_sample(probs)\n",
        "            h.append(h_i)\n",
        "        v, h = self.local_search(v, h)\n",
        "        v, h = self.gibbs_step(v, h)\n",
        "        energy_neg = self.coupling(v, h)\n",
        "        energy_neg_samples.append(energy_neg)\n",
        "      return energy_neg_samples\n",
        "\n",
        "\n",
        "    def local_search(self, v, h, fix_v=False):\n",
        "        N = v.size(0)\n",
        "        device= v.device\n",
        "        _v = v.clone()\n",
        "        _h = []\n",
        "        for r in h:\n",
        "          _h.append(r.clone())\n",
        "        rand_u = torch.rand(N, device=device)\n",
        "        v, h = self.gibbs_step(v, h, fix_v, rand_u=rand_u, T=0)\n",
        "        converged = torch.ones(N, dtype=torch.bool, device=device) if fix_v \\\n",
        "                    else self.equals(v, _v)\n",
        "        for i in range(self.L):\n",
        "            converged = converged.logical_and(self.equals(h[i], _h[i]))\n",
        "        while not converged.all():\n",
        "            not_converged = converged.logical_not()\n",
        "            _v = v[not_converged]\n",
        "            _h = [h[i][not_converged] for i in range(self.L)]\n",
        "            M = _v.size(0)\n",
        "            v_, h_ = self.gibbs_step(_v, _h, fix_v,\n",
        "                                     rand_u=rand_u[not_converged], T=0)\n",
        "            if fix_v:\n",
        "                converged_ = torch.ones(M, dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                converged_ = self.equals(v_, _v)\n",
        "                v = torch.scatter(v,0,not_converged.nonzero().repeat(1,v.shape[1]), v_)\n",
        "            for i in range(self.L):\n",
        "                converged_ = converged_.logical_and(self.equals(h_[i], _h[i]))\n",
        "                h[i] = torch.scatter(h[i], 0, not_converged.nonzero().repeat(1,h_[i].shape[1]), h_[i])\n",
        "            converged[not_converged] = converged_\n",
        "\n",
        "        return v, h\n",
        "\n",
        "    def equals(self, a, b):\n",
        "      similarity_scores = abs(a-b)\n",
        "      return torch.all(similarity_scores < 0.4, dim=1)\n",
        "\n",
        "    def coupling(self, v, h, fix_v=False):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "        _v = v.clone()\n",
        "        _h = []\n",
        "        for r in h:\n",
        "          _h.append(r.clone())\n",
        "        v, h = self.mh_step(v, h, fix_v)\n",
        "        energy = self.energy(v, h)\n",
        "        if fix_v:\n",
        "          converged = torch.ones(N, dtype=torch.bool, device=device)\n",
        "        else:\n",
        "          converged = self.equals(v, _v)\n",
        "        for i in range(self.L):\n",
        "            converged = converged.logical_and(self.equals(h[i], _h[i]))\n",
        "        while not converged.all():\n",
        "            not_converged = converged.logical_not()\n",
        "            _v = v[not_converged]\n",
        "            _h = [h[i][not_converged] for i in range(self.L)]\n",
        "            M = _v.size(0)\n",
        "            rand_v = None if fix_v else torch.rand_like(_v)\n",
        "            rand_h = [torch.rand_like(_h[i]) for i in range(self.L)]\n",
        "            rand_u = torch.rand(M, device=device)\n",
        "            v_, h_ = self.mh_step(_v, _h, fix_v, rand_v, rand_h, rand_u)\n",
        "            aaa = self.energy(v_, h_)\n",
        "            bbb = self.energy(_v, _h)\n",
        "            energy[not_converged] = energy[not_converged] + (aaa - bbb)\n",
        "            if fix_v:\n",
        "                converged_ = torch.ones(M, dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                converged_ = self.equals(v_, _v)\n",
        "                v = torch.scatter(v,0,not_converged.nonzero().repeat(1,v.shape[1]), v_)\n",
        "            for i in range(self.L):\n",
        "                converged_ = converged_.logical_and(self.equals(h_[i], _h[i]))\n",
        "                h[i] = torch.scatter(h[i], 0, not_converged.nonzero().repeat(1,h_[i].shape[1]), h_[i])\n",
        "            converged[not_converged] = converged_\n",
        "        return energy\n",
        "\n",
        "    def energy(self, v, h):\n",
        "        energy = - torch.sum(v * self.bias[0].unsqueeze(0), 1)\n",
        "        for i in range(self.L):\n",
        "            logits = F.linear(v if i==0 else h[i-1], self.weight[i], self.bias[i+1])\n",
        "\n",
        "            energy = energy - torch.sum(h[i] * logits, 1)\n",
        "        return energy\n",
        "\n",
        "    def greaterThan(self,a,b):\n",
        "      return GreaterThanFunction.apply(a,b)\n",
        "    def bernoulli_sample(self,probabilities):\n",
        "      random_numbers = torch.randint(0, 20, probabilities.shape).to(device)\n",
        "      return BernoulliSampleFunction.apply(probabilities, random_numbers)\n",
        "\n",
        "    def gibbs_step(self, v, h, fix_v=False, rand_v=None, rand_h=None, rand_u=None, rand_z=None, T=1):\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "            params.append(tensor)\n",
        "        for parameter in self.weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in self.bias:\n",
        "            params.append(parameter)\n",
        "        if rand_v is None:\n",
        "          rand_v = self.dummy\n",
        "        if rand_h is None:\n",
        "          rand_h = self.dummy\n",
        "        if rand_u is None:\n",
        "          rand_u = self.dummy\n",
        "        if rand_z is None:\n",
        "          rand_z = self.dummy\n",
        "        v, *h = GibbsStepFunction.apply(v, torch.tensor(float(fix_v)).requires_grad_(), rand_v,rand_h, rand_u, rand_z, torch.tensor(float(T)).requires_grad_(), *params)\n",
        "        return v,h\n",
        "\n",
        "\n",
        "    def mh_step(self, v, h, fix_v=False, rand_v=None, rand_h=None, rand_u=None):\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "            params.append(tensor)\n",
        "        for parameter in self.weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in self.bias:\n",
        "            params.append(parameter)\n",
        "        if rand_v is None:\n",
        "          rand_v = self.dummy\n",
        "        if rand_h is None:\n",
        "          rand_h = self.dummy\n",
        "        if rand_u is None:\n",
        "          rand_u = self.dummy\n",
        "        v, *h = MHStepFunction.apply( v,torch.tensor(float(fix_v)).requires_grad_(),rand_v,rand_h,rand_u,*params)\n",
        "        return v, h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HFvxdEz_urz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be7be1e-a10c-4ce9-a7c2-8a9eeb5ae92f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-009d65204741>:553: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  ctx.save_for_backward(v,even, odd,torch.tensor(fix_v), rand_v, rand_h, rand_u, rand_z, torch.tensor(T), saveLen, ctxIDs,toSaveID, *params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "energy_pos:  tensor(-52.2032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "energy_neg:  tensor(-62.8212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "data  0  loss:  tensor(52.7231, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)  validation loss:  tensor(42.1052, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward1>)  time: 420.7416\n",
            "energy_pos:  tensor(-42.0474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "energy_neg:  tensor(-43.4376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "data  1  loss:  tensor(43.4082, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)  validation loss:  tensor(42.0180, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward1>)  time: 447.7463\n",
            "energy_pos:  tensor(-20.5282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "energy_neg:  tensor(-29.3590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "data  2  loss:  tensor(50.7753, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)  validation loss:  tensor(41.9445, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward1>)  time: 466.8255\n",
            "energy_pos:  tensor(-2.2232, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import math\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "def train_dbn(dataloader, dbn_model, num_epochs, learning_rate, device):\n",
        "    dbn_model.to(device)  # Ensure model is on the correct device\n",
        "    dbn_model.train()\n",
        "    optimizer = Adam(dbn_model.parameters(), lr=learning_rate)\n",
        "    scheduler = LambdaLR(optimizer, lr_lambda=lambda t: 1 / math.sqrt(1 + 0.001))\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss=0\n",
        "        total_val=0\n",
        "        i=0\n",
        "        for step, (x, x_e) in enumerate(dataloader):\n",
        "            start_time = time.time()\n",
        "            i+=1\n",
        "            optimizer.zero_grad()\n",
        "            loss, output = dbn_model(x.to(device).float())\n",
        "            total_loss+=loss.mean().item()\n",
        "            loss2=F.cross_entropy(output, x_e.to(device))\n",
        "            loss = loss.mean() + loss2\n",
        "            total_val+=loss2.item()\n",
        "            end_time = time.time()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            print(\"data \",step, \" loss: \",loss,\" validation loss: \",loss2, f\" time: {elapsed_time:.4f}\")\n",
        "\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Epoch {epoch} avg loss: {total_loss/i:.4f} avg val loss: {total_val/i:.4f} time: {elapsed_time:.4f}\")\n",
        "\n",
        "dbn_model = DBM(num_features, hidden_layers,model2).to(device)\n",
        "#state_dict = torch.load(\"dbn_modelv2_deepConv.pt\")\n",
        "#dbn_model.load_state_dict(state_dict)\n",
        "train_dbn(dataloader, dbn_model, num_epochs=1000, learning_rate=0.001, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea: localize the weights for each method. That functions like an attention mechanism (same way the convolutional layer functions like a convolutional layer). So increase the amount of weights."
      ],
      "metadata": {
        "id": "4whH-Jpc1LMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import OpenerDirector\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Bernoulli, Independent\n",
        "from torch.optim import Adam\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "L = 3\n",
        "nv = 107\n",
        "hidden_layers = [9,5,3]\n",
        "v = torch.rand(batch_size, nv, requires_grad=True, dtype=torch.float64)\n",
        "h = [torch.randn(batch_size, nh, requires_grad=True, dtype=torch.float64) for nh in hidden_layers]\n",
        "weight = nn.ParameterList([nn.Parameter(torch.randn(hidden_layers[0], nv, requires_grad=True, dtype=torch.float64))])\n",
        "for i in range(len(hidden_layers)-1):\n",
        "  weight.append(nn.Parameter(torch.randn(hidden_layers[i+1], hidden_layers[i], requires_grad=True, dtype=torch.float64)))\n",
        "bias = nn.ParameterList([nn.Parameter(torch.randn(nv, requires_grad=True, dtype=torch.float64))])\n",
        "for i in range(len(hidden_layers)):\n",
        "  bias.append(nn.Parameter(torch.randn(hidden_layers[i], requires_grad=True, dtype=torch.float64)))\n",
        "\n",
        "print(\"bias 0: \",bias[0].shape)\n",
        "def energy_old(v, h):\n",
        "        energy = - torch.sum(v * bias[0].unsqueeze(0), 1)\n",
        "\n",
        "        for i in range(L):\n",
        "            logits = F.linear(v.double() if i==0 else h[i-1].double(),\n",
        "                              weight[i].double(), bias[i+1].double())\n",
        "\n",
        "            energy -= torch.sum(h[i] * logits, 1)\n",
        "\n",
        "        return energy\n",
        "\n",
        "\n",
        "def mh_step1(v, h, fix_v=False,\n",
        "                rand_v=None, rand_h=None, rand_u=None):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "\n",
        "        if fix_v:\n",
        "            v_ = v\n",
        "        else:\n",
        "            if rand_v is None:\n",
        "                v_ = torch.empty_like(v).bernoulli_()\n",
        "            else:\n",
        "                v_ = (rand_v < 0.5).float()\n",
        "\n",
        "        if rand_h is None:\n",
        "            h_ = [torch.empty_like(h[i]).bernoulli_() for i in range(L)]\n",
        "        else:\n",
        "            h_ = [(rand_h[i] < 0.5).float() for i in range(L)]\n",
        "\n",
        "        log_ratio = energy_old(v, h) - energy_old(v_, h_)\n",
        "\n",
        "        if rand_u is None:\n",
        "            accepted = log_ratio.exp().clamp(0, 1).bernoulli().bool()\n",
        "        else:\n",
        "            accepted = rand_u < log_ratio.exp()\n",
        "\n",
        "        if not fix_v:\n",
        "            v = torch.where(accepted.unsqueeze(1), v_, v)\n",
        "        h = [torch.where(accepted.unsqueeze(1), h_[i], h[i]) for i in range(L)]\n",
        "\n",
        "        return v, h\n",
        "\n",
        "\n",
        "result1 = mh_step1(v,h)\n",
        "\n",
        "\n",
        "class ComparatorNetwork(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(2, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.x=None\n",
        "        self.out1 = None\n",
        "        self.out2 = None\n",
        "        self.out3 = None\n",
        "        self.out4 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x=x\n",
        "        out = self.relu(self.linear1(x))\n",
        "        self.out1 = out\n",
        "        out = self.relu(self.linear2(out))\n",
        "        self.out2 = out\n",
        "        out = self.relu(self.linear3(out))\n",
        "        self.out3 = out\n",
        "        out = torch.sigmoid(self.linear4(out))\n",
        "        self.out4 = out\n",
        "        return out\n",
        "\n",
        "class BernoulliApproximator(nn.Module):\n",
        "  def __init__(self, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(2, hidden_dim)\n",
        "    self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.relu(self.linear1(x))\n",
        "    out = self.relu(self.linear2(out))\n",
        "    out = self.relu(self.linear3(out))\n",
        "    out = torch.sigmoid(self.linear4(out))\n",
        "    return out\n",
        "\n",
        "model = torch.load('bernoullimodel9.pth',map_location=device)\n",
        "\n",
        "\n",
        "model2 = torch.load('greater_than4.pth',map_location=device)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.data = param.data.double()  # Convert to float64\n",
        "\n",
        "for param in model2.parameters():\n",
        "    param.data = param.data.double()\n",
        "\n",
        "\n",
        "print(\"model test: \",model(torch.tensor([0.3,19]).double()))\n",
        "print(\"model test: \",model(torch.tensor([0.3,4]).double()))\n",
        "print(\"model test: \",model(torch.tensor([0.9,18]).double()))\n",
        "print(\"model test: \",model(torch.tensor([0.9,10]).double()))\n",
        "import copy\n",
        "\n",
        "import time\n",
        "class BernoulliSampleFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, probabilities, random_numbers):\n",
        "        result = torch.zeros_like(probabilities)\n",
        "        inputs = []\n",
        "        for i in range(probabilities.shape[1]):\n",
        "          with torch.enable_grad():\n",
        "            input = torch.cat((probabilities[:, i].unsqueeze(1), random_numbers[:, i].unsqueeze(1)), dim=1).clone().requires_grad_(True)\n",
        "            inputs.append(input)\n",
        "            result[:, i] = model(input).squeeze().detach()\n",
        "        ctx._dict = model.state_dict()\n",
        "        ctx.save_for_backward(*inputs)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "      inputs = ctx.saved_tensors\n",
        "      toReturn = torch.zeros_like(grad_output)\n",
        "      toReturn2 = torch.zeros_like(grad_output)\n",
        "      for i in range(toReturn.shape[1]):\n",
        "        input = inputs[i]\n",
        "        delta = grad_output[:,i].unsqueeze(1)\n",
        "        for y in reversed(range(1,5)):\n",
        "          strin = \"linear\"+str(y)+'.weight'\n",
        "          weights = ctx._dict[strin]\n",
        "          delta = delta @ weights.clone()\n",
        "        toReturn[:,i] = delta[:,0]\n",
        "        toReturn2[:,i] = delta[:,1]\n",
        "      return toReturn, toReturn2\n",
        "\n",
        "\n",
        "\n",
        "class GreaterThanFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, a, b):\n",
        "        result = torch.zeros_like(a)\n",
        "        inputs = []\n",
        "        for i in range(a.shape[1]):\n",
        "          input = torch.concat((a[:,i].unsqueeze(1),b[:,i].unsqueeze(1)),dim=1)\n",
        "          inputs.append(input)\n",
        "          result[:,i] = model2(inputs).squeeze()\n",
        "        ctx._dict = model.state_dict()\n",
        "        ctx.save_for_backward(*inputs)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "      inputs= ctx.saved_tensors\n",
        "      toReturn1 = torch.zeros_like(grad_output)\n",
        "      toReturn2 = torch.zeros_like(grad_output)\n",
        "      for i in range(toReturn1.shape[1]):\n",
        "        input = inputs[i].detach().requires_grad_()\n",
        "        delta = grad_output[:,i].unsqueeze(1)\n",
        "        for y in reversed(range(1,5)):\n",
        "          strin = \"linear\"+str(y)+'.weight'\n",
        "          weights = ctx._dict[strin]\n",
        "          delta = delta @ weights.clone()\n",
        "        toReturn1[:,i] = delta[:,0]\n",
        "        toReturn2[:,i] = delta[:,1]\n",
        "      return toReturn1, toReturn2\n",
        "\n",
        "\n",
        "from torch import autograd, nn\n",
        "\n",
        "def energy(v, *params):\n",
        "  h = params[:L]\n",
        "  weight = params[L:L*2]\n",
        "  bias = params[L*2:]\n",
        "  energy = - torch.sum(v * bias[0].unsqueeze(0), 1)\n",
        "  for i in range(L):\n",
        "      logits = F.linear(v.double() if i==0 else h[i-1].double(), weight[i].double(), bias[i+1].double())\n",
        "      energy -= torch.sum(h[i] * logits, 1)\n",
        "  return energy\n",
        "\n",
        "\n",
        "\n",
        "params = []\n",
        "for tensor in h:\n",
        "  params.append(tensor)\n",
        "for parameter in weight:\n",
        "  params.append(parameter)\n",
        "for parameter in bias:\n",
        "  params.append(parameter)\n",
        "energy1 = energy(v, *params)\n",
        "print(\"energy: \",energy1)\n",
        "loss = torch.ones_like(energy1)*100\n",
        "print(\"loss: \",loss)\n",
        "\n",
        "grad_v, *grads = autograd.grad(energy1, (v,*params), loss)\n",
        "grads = list(grads)\n",
        "h_g = grads[:L]\n",
        "weight_g = grads[L:L*2]\n",
        "bias_g = grads[L*2:]\n",
        "print(\"h_g: \",h_g)\n",
        "print(\"weight_g: \",weight_g)\n",
        "print(\"bias_g: \",bias_g)\n",
        "\n",
        "for i in range(len(h_g)):\n",
        "  h[i].grad = h_g[i]\n",
        "for i in range(len(weight_g)):\n",
        "  weight[i].grad = weight_g[i]\n",
        "for i in range(len(bias_g)):\n",
        "  bias[i].grad = bias_g[i]\n",
        "\n",
        "print(\"weight:\", [w.grad for w in weight])\n",
        "print(\"bias: \",[b.grad for b in bias])\n",
        "\n",
        "class MHStepFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, v, fix_v, rand_v, rand_h, rand_u, *params):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "        L=3\n",
        "       # print(\"params2: \",params)\n",
        "        h = params[:L]\n",
        "        weight = params[L:L*2]\n",
        "        bias = params[L*2:]\n",
        "        fix_v = fix_v==1.0\n",
        "        temp = []\n",
        "        for x in [rand_v,rand_h,rand_u]:\n",
        "          if isinstance(x, torch.Tensor) and x.numel() == 1 and x.item() == 0.0:\n",
        "            temp.append(None)\n",
        "          else:\n",
        "            temp.append(x)\n",
        "        rand_v = temp[0]\n",
        "        rand_h = temp[1]\n",
        "        rand_u = temp[2]\n",
        "       # print(\"h: \",h)\n",
        "        ctxs = []\n",
        "        if fix_v:\n",
        "            v_ = v\n",
        "        else:\n",
        "            if rand_v is None:\n",
        "                v_ = torch.empty_like(v).bernoulli_()\n",
        "            else:\n",
        "                v_ = (rand_v < 0.5).float()\n",
        "\n",
        "        if rand_h is None:\n",
        "            h_ = [torch.empty_like(h[i]).bernoulli_() for i in range(L)]\n",
        "        else:\n",
        "            h_ = [(rand_h[i] < 0.5).float() for i in range(L)]\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "            print(\"h: \",tensor.shape)\n",
        "            params.append(tensor)\n",
        "        for parameter in weight:\n",
        "            print(\"weight: \",parameter.shape)\n",
        "            params.append(parameter)\n",
        "        for parameter in bias:\n",
        "            print(\"bias: \",parameter.shape)\n",
        "            params.append(parameter)\n",
        "        energy1 = energy(v,*params)\n",
        "        energy1CTX = tuple((v, *params))\n",
        "        ctxs.append(energy1CTX)\n",
        "        params = []\n",
        "        for tensor in h_:\n",
        "            params.append(tensor)\n",
        "        for parameter in weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in bias:\n",
        "            params.append(parameter)\n",
        "        energy2 = energy(v_,*params)\n",
        "        energy2CTX = tuple((v_, *params))\n",
        "        ctxs.append(energy2CTX)\n",
        "        log_ratio = energy1 - energy2\n",
        "        toSave = []\n",
        "        toSave.append(log_ratio)\n",
        "        if rand_u is None:\n",
        "            input1 = log_ratio.exp().clamp(0,1).unsqueeze(1)\n",
        "            random_numbers = torch.randint(0, 20, input1.shape).float()\n",
        "            accepted = BernoulliSampleFunction.apply(input1,random_numbers)\n",
        "            ctxs.append([input1,random_numbers])\n",
        "        else:\n",
        "            accepted = GreaterThanFunction.apply(log_ratio.exp().unsqueeze(1),rand_u.unsqueeze(1))\n",
        "            ctxs.append([log_ratio.exp().unsqueeze(1), rand_u.unsqueeze(1)])\n",
        "        if not fix_v:\n",
        "            toSave.append(v_)\n",
        "            toSave.append(v)\n",
        "        for i in range(L):\n",
        "          toSave.append(h_[i])\n",
        "          toSave.append(h[i])\n",
        "        accepted = torch.round(accepted,decimals=0).bool()\n",
        "\n",
        "        params = []\n",
        "        for parameter in weight:\n",
        "          params.append(parameter)\n",
        "        for parameter in bias:\n",
        "          params.append(parameter)\n",
        "        for sav in toSave:\n",
        "          params.append(sav)\n",
        "        for lis in ctxs:\n",
        "          params.append(torch.tensor(len(lis)))\n",
        "          params.extend(lis)\n",
        "        savLength = torch.tensor(len(toSave))\n",
        "        if not fix_v:\n",
        "            v = torch.where(accepted, v_, v)\n",
        "        h = [torch.where(accepted, h_[i], h[i]) for i in range(L)]\n",
        "        if rand_u is None:\n",
        "          rand_u = torch.tensor(0.0)\n",
        "        fix_v = torch.tensor(float(fix_v))\n",
        "        accepted = accepted.float()\n",
        "        ctx.save_for_backward(accepted, fix_v, rand_u,savLength, *params)\n",
        "        return v, *h\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_v, *grad_h):\n",
        "        accepted, fix_v, rand_u,savLength, *params = ctx.saved_tensors\n",
        "        L=3\n",
        "        if len(rand_u.shape)==0 :\n",
        "          rand_u = None\n",
        "        fix_v = fix_v==1.0\n",
        "        accepted = accepted.bool()\n",
        "        weight = params[:L]\n",
        "        bias = params[L:L*2+1]\n",
        "        toSave = params[L*2+1:L*2+1+savLength.item()]\n",
        "        ctxTensors = params[L*2+1+savLength.item():]\n",
        "        ctxTuples = []\n",
        "        i = 0\n",
        "        while i < len(ctxTensors):\n",
        "            tuple_length = ctxTensors[i].item()\n",
        "            start = i + 1  # Start index of tuple elements\n",
        "            end = start + tuple_length\n",
        "            ctxTuples.append(tuple(ctxTensors[start:end]))\n",
        "            i = end\n",
        "        ctx1 = ctxTuples[0]\n",
        "        ctx2 = ctxTuples[1]\n",
        "        ctx3 = ctxTuples[2]\n",
        "        grad_h = list(grad_h)\n",
        "        grad_weight = [torch.zeros_like(w) for w in weight]\n",
        "        grad_bias = [torch.zeros_like(b) for b in bias]\n",
        "        toSave = list(toSave)\n",
        "        if not fix_v:\n",
        "          d_accepted = torch.sum((toSave[1]-toSave[2]) * grad_v, dim=1, keepdim=True)\n",
        "          for i in range(len(grad_h)):\n",
        "            d_accepted = d_accepted +torch.sum((toSave[3+i*2]-toSave[3+(i*2)+1]) * grad_h[i], dim=1, keepdim=True)\n",
        "        else:\n",
        "          for i in range(len(grad_h)):\n",
        "            d_accepted = d_accepted +torch.sum((toSave[1+i*2]-toSave[1+(i*2)+1]) * grad_h[i], dim=1, keepdim=True)\n",
        "        if rand_u is None:\n",
        "          with torch.enable_grad():\n",
        "            input = (ctx3[0].detach().requires_grad_(), ctx3[1].detach().requires_grad_())\n",
        "            accepted1 = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "            d_log_ratio_exp, _ = autograd.grad(accepted1, input, d_accepted)\n",
        "        else:\n",
        "          with torch.enable_grad():\n",
        "            input = (ctx3[0].detach().requires_grad_(), ctx3[1].detach().requires_grad_())\n",
        "            accepted1 = GreaterThanFunction.apply(input[0],input[1])\n",
        "            d_log_ratio_exp, _ = autograd.grad(accepted1, input, d_accepted)\n",
        "        d_log_ratio = d_log_ratio_exp * toSave[0].exp().unsqueeze(1)\n",
        "        with torch.enable_grad():\n",
        "            v1 = ctx1[0].detach().requires_grad_()\n",
        "            params1 = ctx1[1:]\n",
        "\n",
        "            params1 = [item.detach().requires_grad_() for item in params1]\n",
        "            input = (v1, *params1)\n",
        "            energy8 = energy(*input)\n",
        "            if d_log_ratio.shape[0]==1:\n",
        "              d_log_ratio= d_log_ratio.squeeze(1)\n",
        "            else:\n",
        "              d_log_ratio=d_log_ratio.squeeze()\n",
        "            v1, *params1 = autograd.grad(energy8, input, d_log_ratio)\n",
        "        params1 = list(params1)\n",
        "        h1 = params1[:L]\n",
        "        weight1 = params1[L:L*2]\n",
        "        bias1 = params1[L*2:]\n",
        "        with torch.enable_grad():\n",
        "            v2 = ctx2[0].detach().requires_grad_()\n",
        "            params2 = ctx2[1:]\n",
        "            params2 = [item.detach().requires_grad_() for item in params2]\n",
        "            input = (v2, *params2)\n",
        "            energy8 = energy(*input)\n",
        "            v2, *params2 = autograd.grad(energy8, input, -1*d_log_ratio)\n",
        "        params2 = list(params2)\n",
        "        weight2 = params2[L:L*2]\n",
        "        bias2 = params2[L*2:]\n",
        "        if not fix_v:\n",
        "            grad_v = torch.where(accepted,0,grad_v)\n",
        "        for i in range(len(grad_h)):\n",
        "            grad_h[i] = torch.where(accepted,0,grad_h[i])\n",
        "        grad_v += v1\n",
        "        for i in range(len(grad_h)):\n",
        "          grad_h[i]+=h1[i]\n",
        "        for i in range(len(grad_weight)):\n",
        "          grad_weight[i] += (weight1[i]-weight2[i])\n",
        "        for i in range(len(grad_bias)):\n",
        "          grad_bias[i] += (bias1[i]-bias2[i])\n",
        "        grads = []\n",
        "        for tensor in grad_h:\n",
        "            grads.append(tensor)\n",
        "        for parameter in grad_weight:\n",
        "          grads.append(parameter)\n",
        "        for parameter in grad_bias:\n",
        "          grads.append(parameter)\n",
        "        return grad_v, None, None, None, None, *grads\n",
        "\n",
        "\n",
        "\n",
        "params = []\n",
        "for tensor in h:\n",
        "    params.append(tensor)\n",
        "for parameter in weight:\n",
        "    params.append(parameter)\n",
        "for parameter in bias:\n",
        "    params.append(parameter)\n",
        "#test = torch.autograd.gradcheck(MHStepFunction.apply, ( v,False,None,None,None,*params))\n",
        "#print(\"AUTOGRAD1: \",test)\n",
        "\n",
        "#test = torch.autograd.gradcheck(mh_step2, (v,h[0],h[1],h[2]))\n",
        "dummy = torch.tensor(0.0).requires_grad_()\n",
        "result2 = MHStepFunction.apply( v,dummy,dummy,dummy,dummy,*params)\n",
        "loss1 = torch.ones_like(v)*100\n",
        "loss2 = [torch.ones_like(item)*100 for item in h]\n",
        "test = autograd.grad(result2, ( v,dummy,dummy,dummy,dummy,*params), (loss1, *loss2),allow_unused=True)\n",
        "print(\"TTTTTTTTTT AUTOGRAD2: \",test)\n",
        "#test = torch.autograd.gradcheck(mh_step2, (v,h[0],h[1],h[2]))\n",
        "#print(\"AUTOGRAD1: \",test)\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def plot_histograms_and_stats(results, title):\n",
        "    if len(results)==3:\n",
        "      _,v,h = results\n",
        "    else:\n",
        "      v, *h = results\n",
        "\n",
        "    # Flatten all the tensors to get a single tensor for easier handling\n",
        "    all_tensors = v\n",
        "    for h1 in h:\n",
        "      for h2 in h1:\n",
        "        all_tensors = torch.cat((all_tensors.flatten(),h2.flatten()),dim=0)\n",
        "    all_tensors = all_tensors.flatten().detach()\n",
        "\n",
        "    # Calculate mean and standard deviation\n",
        "    mean = torch.mean(all_tensors).item()\n",
        "    std = torch.std(all_tensors).item()\n",
        "\n",
        "    # Creating histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(all_tensors.numpy(), bins=50, alpha=0.75, color='blue')\n",
        "    plt.title(f'Histogram of all values in {title}\\nMean: {mean:.4f}, Std: {std:.4f}')\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return mean, std\n",
        "# Assuming result1 and result2 have been generated correctly by the provided code snippet\n",
        "mean1, std1 = plot_histograms_and_stats(result1, 'result1')\n",
        "mean2, std2 = plot_histograms_and_stats(result2, 'result2')\n",
        "\n",
        "print(f\"Result1 - Mean: {mean1:.4f}, Std: {std1:.4f}\")\n",
        "print(f\"Result2 - Mean: {mean2:.4f}, Std: {std2:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gibbs_step1(v, h, fix_v=False,\n",
        "                   rand_v=None, rand_h=None, rand_u=None, rand_z=None, T=1):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "\n",
        "        v_, h_ = (v, h)\n",
        "\n",
        "        if rand_u is None:\n",
        "            rand_u = torch.rand(N, device=device)\n",
        "\n",
        "        even = rand_u < 0.5\n",
        "        odd = even.logical_not()\n",
        "\n",
        "        if even.sum() > 0:\n",
        "            if not fix_v:\n",
        "                logits = F.linear(h_[0][even],\n",
        "                                  weight[0].t(), bias[0])\n",
        "\n",
        "                if T == 0:\n",
        "                    v_[even] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits = logits/ T\n",
        "\n",
        "                    if rand_v is None:\n",
        "                        v_[even] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        v_[even] = (rand_v[even] < logits.sigmoid()).float()\n",
        "\n",
        "            for i in range(1, len(h), 2):\n",
        "                logits = F.linear(h_[i-1][even],\n",
        "                                  weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h_[i+1][even],\n",
        "                                       weight[i+1].t(), None)\n",
        "\n",
        "                if T == 0:\n",
        "                    h_[i][even] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits = logits/T\n",
        "\n",
        "                    if rand_h is None:\n",
        "                        h_[i][even] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        h_[i][even] = (rand_h[i][even] < logits.sigmoid()).float()\n",
        "\n",
        "            for i in range(0, len(h), 2):\n",
        "                logits = F.linear(v_[even] if i==0 else h_[i-1][even],\n",
        "                                  weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits += F.linear(h_[i+1][even],\n",
        "                                       weight[i+1].t(), None)\n",
        "\n",
        "                if T == 0:\n",
        "                    h_[i][even] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits /= T\n",
        "\n",
        "                    if rand_h is None:\n",
        "                        h_[i][even] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        h_[i][even] = (rand_h[i][even] < logits.sigmoid()).float()\n",
        "\n",
        "        if odd.sum() > 0:\n",
        "            for i in range(0, len(h), 2):\n",
        "                logits = F.linear(v_[odd] if i==0 else h_[i-1][odd],\n",
        "                                  weight[i],bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits += F.linear(h_[i+1][odd],\n",
        "                                       weight[i+1].t(), None)\n",
        "\n",
        "                if T == 0:\n",
        "                    h_[i][odd] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "\n",
        "                    if rand_h is None:\n",
        "                        h_[i][odd] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        h_[i][odd] = (rand_h[i][odd] < logits.sigmoid()).float()\n",
        "\n",
        "            if not fix_v:\n",
        "                logits = F.linear(h_[0][odd],\n",
        "                                  weight[0].t(), bias[0])\n",
        "\n",
        "                if T == 0:\n",
        "                    v_[odd] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "\n",
        "                    if rand_v is None:\n",
        "                        v_[odd] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        v_[odd] = (rand_v[odd] < logits.sigmoid()).float()\n",
        "\n",
        "            for i in range(1, len(h), 2):\n",
        "                logits = F.linear(h_[i-1][odd],\n",
        "                                  weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits += F.linear(h_[i+1][odd],\n",
        "                                       weight[i+1].t(), None)\n",
        "\n",
        "                if T == 0:\n",
        "                    h_[i][odd] = (logits >= 0).float()\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "\n",
        "                    if rand_h is None:\n",
        "                        h_[i][odd] = Independent(Bernoulli(logits=logits), 1).sample()\n",
        "                    else:\n",
        "                        h_[i][odd] = (rand_h[i][odd] < logits.sigmoid()).float()\n",
        "\n",
        "        return v_, h_\n",
        "h2 = []\n",
        "for h6 in h:\n",
        "  h2.append(h6.clone().detach())\n",
        "result3  = gibbs_step1(v.detach(),h2)\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "from torch.autograd import Function\n",
        "\n",
        "class GibbsStepFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, v,fix_v, rand_v, rand_h, rand_u, rand_z, T, *params):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "        fix_v= fix_v==1.0\n",
        "        params = list(params)\n",
        "        L=3\n",
        "        h = params[:L]\n",
        "        weight = params[L:L*2]\n",
        "        bias = params[L*2:]\n",
        "\n",
        "        temp = []\n",
        "        for x in [rand_v,rand_h,rand_u,rand_z]:\n",
        "          if isinstance(x, torch.Tensor) and x.numel() == 1 and x.item() == 0.0:\n",
        "            temp.append(None)\n",
        "          else:\n",
        "            temp.append(x)\n",
        "        rand_v = temp[0]\n",
        "        rand_h = temp[1]\n",
        "        rand_u = temp[2]\n",
        "        rand_z = temp[3]\n",
        "        if rand_u is None:\n",
        "            rand_u = torch.rand(N, device=device)\n",
        "        even = rand_u < 0.5\n",
        "        odd = even.logical_not()\n",
        "        ctx_l = []\n",
        "        ctxID = []\n",
        "        toSave = []\n",
        "        toSaveID = []\n",
        "        if even.sum() > 0:\n",
        "          #  print(\"TEST\",)\n",
        "            if not fix_v:\n",
        "                logits = F.linear(h[0][even],\n",
        "                                  weight[0].t(), bias[0])\n",
        "                toSaveID.append(15)\n",
        "                toSave.append(h[0][even])\n",
        "\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(18)\n",
        "                    v = torch.scatter(v, 0, even.nonzero().repeat(1,v.shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(14)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_v is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).double()\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(6)\n",
        "                        v =  torch.scatter(v,0,even.nonzero().repeat(1,v.shape[1]),sample)\n",
        "                      #  print(\"v_2: \",v.grad)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_v[even])\n",
        "                        ctx_l.append([sigLogits,rand_v[even]])\n",
        "                        ctxID.append(17)\n",
        "                        v = torch.scatter(v,0,even.nonzero().repeat(1,v.shape[1]),sample)\n",
        "\n",
        "            for i in range(1, len(h), 2):\n",
        "              #  print(\"TEST2\")\n",
        "                logits = F.linear(h[i-1][even], weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][even], weight[i+1].t(), None)\n",
        "                    toSaveID.append(12)\n",
        "                    toSave.append(h[i+1][even])\n",
        "\n",
        "                toSaveID.append(13)\n",
        "                toSave.append(h[i-1][even])\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(16)\n",
        "                    h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(11)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).double()\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(5)\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_h[i][even])\n",
        "                        ctx_l.append([sigLogits,rand_h[i][even]])\n",
        "                        ctxID.append(15)\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "\n",
        "            for i in range(0, len(h), 2):\n",
        "               # print(\"TEST3\")\n",
        "                logits = F.linear(v[even] if i==0 else h[i-1][even],\n",
        "                                  weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][even], weight[i+1].t(), None)\n",
        "                    toSaveID.append(9)\n",
        "                    toSave.append(h[i+1][even])\n",
        "\n",
        "                toSaveID.append(10)\n",
        "                toSave.append(v[even] if i==0 else h[i-1][even])\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(14)\n",
        "                    h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSaveID.append(8)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).double()\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(4)\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_h[i][even])\n",
        "                        ctx_l.append([sigLogits,rand_h[i][even]])\n",
        "                        ctxID.append(13)\n",
        "                        h[i] = torch.scatter(h[i], 0, even.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "        if odd.sum() > 0:\n",
        "            for i in range(0, len(h), 2):\n",
        "                logits = F.linear(v[odd] if i==0 else h[i-1][odd], weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][odd], weight[i+1].t(), None)\n",
        "                    toSaveID.append(6)\n",
        "                    toSave.append(h[i+1][odd])\n",
        "\n",
        "                toSaveID.append(7)\n",
        "                toSave.append(v[odd] if i==0 else h[i-1][odd])\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(12)\n",
        "                    h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(5)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).double()\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(3)\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_h[i][odd])\n",
        "                        ctx_l.append([sigLogits,rand_h[i][odd]])\n",
        "                        ctxID.append(11)\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "\n",
        "            if not fix_v:\n",
        "                logits = F.linear(h[0][odd], weight[0].t(), bias[0])\n",
        "                toSaveID.append(4)\n",
        "                toSave.append(h[0][odd])\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(10)\n",
        "                    v = torch.scatter(v, 0, odd.nonzero().repeat(1,v.shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    toSaveID.append(3)\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_v is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).double()\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(2)\n",
        "                        v = torch.scatter(v,0,odd.nonzero().repeat(1,v.shape[1]),sample)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_v[odd])\n",
        "                        ctx_l.append([sigLogits,rand_v[odd]])\n",
        "                        ctxID.append(9)\n",
        "                        v = torch.scatter(v,0,odd.nonzero().repeat(1,v.shape[1]),sample)\n",
        "\n",
        "            for i in range(1, len(h), 2):\n",
        "                logits = F.linear(h[i-1][odd], weight[i], bias[i+1])\n",
        "                if i+1 < len(h):\n",
        "                    logits = logits + F.linear(h[i+1][odd], weight[i+1].t(), None)\n",
        "                    toSaveID.append(1)\n",
        "                    toSave.append(h[i+1][odd])\n",
        "                toSaveID.append(2)\n",
        "                toSave.append(h[i-1][odd])\n",
        "                if T == 0:\n",
        "                    sample = GreaterThanFunction.apply(logits,torch.full_like(logits,0.00))\n",
        "                    ctx_l.append([logits,torch.full_like(logits,0.00)])\n",
        "                    ctxID.append(8)\n",
        "                    h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                else:\n",
        "                    logits = logits / T\n",
        "                    sigLogits = torch.sigmoid(logits)\n",
        "                    toSaveID.append(0)\n",
        "                    toSave.append(sigLogits)\n",
        "                    if rand_h is None:\n",
        "                        random_numbers = torch.randint(0, 20, sigLogits.shape).double()\n",
        "                        sample = BernoulliSampleFunction.apply(sigLogits,random_numbers)\n",
        "                        print(\"SIGLOGITS: \",sigLogits.shape)\n",
        "                        ctx_l.append([sigLogits,random_numbers])\n",
        "                        ctxID.append(1)\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "                    else:\n",
        "                        sample = GreaterThanFunction.apply(sigLogits,rand_h[i][odd])\n",
        "                        ctx_l.append([sigLogits,rand_h[i][odd]])\n",
        "                        ctxID.append(7)\n",
        "                        h[i] = torch.scatter(h[i], 0, odd.nonzero().repeat(1,h[i].shape[1]), sample)\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "          params.append(tensor)\n",
        "        for parameter in weight:\n",
        "          params.append(parameter)\n",
        "        for parameter in bias:\n",
        "          params.append(parameter)\n",
        "        saveLen = torch.tensor(len(toSave))\n",
        "        for sav in toSave:\n",
        "          params.append(sav)\n",
        "        for tup in ctx_l:\n",
        "            params.append(torch.tensor(len(tup)))\n",
        "            params.extend(tup)\n",
        "\n",
        "        ctxIDs = torch.tensor(ctxID)\n",
        "        toSaveID = torch.tensor(toSaveID)\n",
        "        ctx.save_for_backward(v,even, odd,torch.tensor(fix_v), rand_v, rand_h, rand_u, rand_z, torch.tensor(T), saveLen, ctxIDs,toSaveID, *params)\n",
        "        return v,  *h\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_v, *grad_h):\n",
        "        v,even, odd,fix_v, rand_v, rand_h, rand_u, rand_z, T, saveLen, ctxIDs, toSaveID, *params = ctx.saved_tensors\n",
        "        params = list(params)\n",
        "        h = params[:L]\n",
        "        weight = params[L:L*2]\n",
        "        bias = params[L*2:L*3+1]\n",
        "        toSave = params[L*3+1:L*3+1+saveLen]\n",
        "        ctxTensors = params[L*3+1+saveLen:]\n",
        "        ctxTuples = []\n",
        "        i = 0\n",
        "        while i < len(ctxTensors):\n",
        "            tuple_length = ctxTensors[i]\n",
        "            start = i + 1  # Start index of tuple elements\n",
        "            end = start + tuple_length\n",
        "            ctxTuples.append(list(ctxTensors[start:end]))\n",
        "            i = end\n",
        "        grad_v2return = torch.zeros_like(grad_v)\n",
        "        h2return = []\n",
        "        for h5 in grad_h:\n",
        "          h2return.append(torch.zeros_like(h5))\n",
        "        grad_weight = []\n",
        "        grad_bias = []\n",
        "        for i in range(L):\n",
        "            grad_weight.append(torch.zeros_like(weight[i]))\n",
        "        for i in range(L+1):\n",
        "            grad_bias.append(torch.zeros_like(bias[i]))\n",
        "        even_v = v[even]\n",
        "        odd_v = v[odd]\n",
        "        even_h = []\n",
        "        odd_h = []\n",
        "        grad_h = list(grad_h)\n",
        "        for gh in grad_h:\n",
        "          even_h.append(gh[even])\n",
        "        for gh in grad_h:\n",
        "          odd_h.append(gh[odd])\n",
        "        h = list(h)\n",
        "        ctxIDs2 = []\n",
        "        for ctID in list(ctxIDs):\n",
        "          ctxIDs2.append(int(ctID))\n",
        "        toSaveID2 = []\n",
        "        for tsID in list(toSaveID):\n",
        "          toSaveID2.append(int(tsID))\n",
        "        ctxIDs = ctxIDs2\n",
        "        toSaveID = toSaveID2\n",
        "      #  print(\"to save: \",toSave)\n",
        "       # print(\"to save ID: \",toSaveID)\n",
        "\n",
        "        ctxTensors = reversed(ctxTuples)\n",
        "        ctxIDs = reversed(ctxIDs)\n",
        "        ctx_queues = defaultdict(deque)\n",
        "        for obj, category_id in zip(ctxTensors, ctxIDs):\n",
        "                ctx_queues[category_id].appendleft(obj)\n",
        "\n",
        "        toSave = reversed(toSave)\n",
        "        toSaveID = reversed(toSaveID)\n",
        "        save_queues = defaultdict(deque)\n",
        "        for obj, category_id in zip(toSave, toSaveID):\n",
        "                save_queues[category_id].appendleft(obj)\n",
        "\n",
        "        if odd.sum() > 0:\n",
        "          for i in reversed(range(1, len(h), 2)):\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                input = ctx_queues[8].pop()\n",
        "                input[0] = input[0].detach().requires_grad_()\n",
        "                input[1] = input[1].detach().requires_grad_()\n",
        "                sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                d_logits, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "            else:\n",
        "              if rand_h is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[1].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  print(\"INPUT 0: \",input[0].shape)\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "                  print(\"d_Logits Sig: \",d_logitsSig.shape)\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[7].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "              temp = save_queues[0].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "\n",
        "            if i+1<len(h):\n",
        "              print(\"grad weight i+1: \",grad_weight[i+1].shape)\n",
        "              print(\"d_logits.t(): \",d_logits.t().shape)\n",
        "              input1 = save_queues[1].pop()\n",
        "              print(\"save q 1: \",input1.shape)\n",
        "              grad_weight[i+1] += (d_logits.t() @ input1).t()\n",
        "              print(\"odd h i+1: \",odd_h[i+1].shape)\n",
        "              print(\"d_logits: \",d_logits.shape)\n",
        "              print(\"weight[i] t: \",weight[i].t().shape)\n",
        "\n",
        "              odd_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            grad_weight[i] += d_logits.t() @ save_queues[2].pop()\n",
        "            odd_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "          if not fix_v:\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                  input = ctx_queues[10].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logits, _ = autograd.grad(sample, (input[0],input[1]), odd_v)\n",
        "            else:\n",
        "              if rand_v is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[2].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_v)\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[9].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_v)\n",
        "              temp = save_queues[3].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "            grad_weight[0] += (d_logits.t() @ save_queues[4].pop()).t()\n",
        "            odd_h[0] += d_logits @ weight[0].t()\n",
        "            grad_bias[0] += d_logits.sum(0)\n",
        "          for i in reversed(range(0,len(h),2)):\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                  input = ctx_queues[12].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logits, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "            else:\n",
        "              if rand_h is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[3].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[11].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), odd_h[i])\n",
        "              temp = save_queues[5].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "            if i+1 < len(h):\n",
        "              grad_weight[i+1] += (d_logits.t() @ save_queues[6].pop()).t()\n",
        "              odd_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            temp = save_queues[7].pop()\n",
        "            grad_weight[i] += d_logits.t() @ temp\n",
        "            if i==0:\n",
        "              odd_v += d_logits @ weight[i]\n",
        "            else:\n",
        "              odd_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "        if even.sum() > 0:\n",
        "          for i in reversed(range(0, len(h), 2)):\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                  input = ctx_queues[14].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logits, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "            else:\n",
        "              if rand_h is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[4].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[13].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "              temp = save_queues[8].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "\n",
        "            if i+1<len(h):\n",
        "              grad_weight[i+1] += (d_logits.t() @ save_queues[9].pop()).t()\n",
        "              even_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            grad_weight[i] += d_logits.t() @ save_queues[10].pop()\n",
        "            if i==0:\n",
        "              even_v += d_logits @ weight[i]\n",
        "            else:\n",
        "              even_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "          for i in reversed(range(1, len(h), 2)):\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                  input = ctx_queues[16].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logits, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "            else:\n",
        "              if rand_h is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[5].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[15].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_h[i])\n",
        "              temp = save_queues[11].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "            if i+1<len(h):\n",
        "              grad_weight[i+1] += (d_logits.t() @ save_queues[12].pop()).t()\n",
        "              even_h[i+1] += d_logits @ weight[i+1].t()\n",
        "            grad_weight[i] += d_logits.t() @ save_queues[13].pop()\n",
        "            even_h[i-1] += d_logits @ weight[i]\n",
        "            grad_bias[i+1] += d_logits.sum(0)\n",
        "          if not fix_v:\n",
        "            if T==0:\n",
        "              with torch.enable_grad():\n",
        "                  input = ctx_queues[18].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logits, _ = autograd.grad(sample, (input[0],input[1]), even_v)\n",
        "            else:\n",
        "              if rand_v is None:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[6].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = BernoulliSampleFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_v)\n",
        "              else:\n",
        "                with torch.enable_grad():\n",
        "                  input = ctx_queues[17].pop()\n",
        "                  input[0] = input[0].detach().requires_grad_()\n",
        "                  input[1] = input[1].detach().requires_grad_()\n",
        "                  sample = GreaterThanFunction.apply(input[0],input[1])\n",
        "                  d_logitsSig, _ = autograd.grad(sample, (input[0],input[1]), even_v)\n",
        "              temp = save_queues[14].pop()\n",
        "              d_logits = d_logitsSig * ((1-temp)*temp)\n",
        "              d_logits = d_logits*T\n",
        "            grad_weight[0] += (d_logits.t() @ save_queues[15].pop()).t()\n",
        "            even_h[0] += d_logits @ weight[0].t()\n",
        "            grad_bias[0] += d_logits.sum(0)\n",
        "        grad_v2return[even] = even_v\n",
        "        grad_v2return[odd] = odd_v\n",
        "        grad_h2return = []\n",
        "        for ind, h7 in enumerate(h2return):\n",
        "          h7[even] = even_h[ind]\n",
        "          h7[odd] = odd_h[ind]\n",
        "          grad_h2return.append(h7)\n",
        "        grads = []\n",
        "        for tensor in grad_h2return:\n",
        "            grads.append(tensor)\n",
        "        for parameter in grad_weight:\n",
        "          grads.append(parameter)\n",
        "        for parameter in grad_bias:\n",
        "          grads.append(parameter)\n",
        "       # print(\"gradv2: \",grad_v2return)\n",
        "       # print([item for item in grads])\n",
        "        return grad_v2return, None, None, None, None, None, None, *grads\n",
        "\n",
        "\n",
        "\n",
        "params = []\n",
        "for tensor in h:\n",
        "    params.append(tensor)\n",
        "for parameter in weight:\n",
        "    params.append(parameter)\n",
        "for parameter in bias:\n",
        "    params.append(parameter)\n",
        "\n",
        "#test2 = autograd.gradcheck(GibbsStepFunction.apply, (v,False, None, None, None, None, 1, *params))\n",
        "#print(\"autograd3: \",test2)\n",
        "\n",
        "result4 = GibbsStepFunction.apply(v, dummy, dummy,dummy, dummy, dummy, torch.tensor(1.0).requires_grad_(), *params)\n",
        "loss1 = torch.ones_like(v)*100\n",
        "loss2 = [torch.ones_like(item)*100 for item in h]\n",
        "test = autograd.grad(result4, (v,  dummy, dummy,dummy, dummy, dummy, torch.tensor(1.0).requires_grad_(), *params), (loss1, *loss2),allow_unused=True)\n",
        "print(\"AUTOGRAD2: \",test)\n",
        "\n",
        "\n",
        "\n",
        "mean3, std3 = plot_histograms_and_stats(result3, 'result3')\n",
        "mean4, std4 = plot_histograms_and_stats(result4, 'result4')\n",
        "\n",
        "print(f\"Result3 - Mean: {mean3:.4f}, Std: {std3:.4f}\")\n",
        "print(f\"Result4 - Mean: {mean4:.4f}, Std: {std4:.4f}\")\n",
        "\n",
        "def coupling1(v, h, fix_v=False):\n",
        "        N = v.size(0)\n",
        "        device = v.device\n",
        "        _v = v.clone()\n",
        "        _h = []\n",
        "        for r in h:\n",
        "          _h.append(r.clone())\n",
        "\n",
        "        v, h = mh_step1(v, h, fix_v)\n",
        "        energy = energy_old(v, h)\n",
        "\n",
        "        converged = torch.ones(N, dtype=torch.bool, device=device) if fix_v \\\n",
        "                    else torch.all(v == _v, 1)\n",
        "        for i in range(L):\n",
        "            converged = converged.logical_and(torch.all(h[i] == _h[i], 1))\n",
        "\n",
        "        while not converged.all():\n",
        "            not_converged = converged.logical_not()\n",
        "            _v = v[not_converged]\n",
        "            _h = [h[i][not_converged] for i in range(L)]\n",
        "            M = _v.size(0)\n",
        "\n",
        "            rand_v = None if fix_v else torch.rand_like(_v)\n",
        "            rand_h = [torch.rand_like(_h[i]) for i in range(L)]\n",
        "            rand_u = torch.rand(M, device=device)\n",
        "\n",
        "            v_, h_ = mh_step1(_v, _h, fix_v, rand_v, rand_h, rand_u)\n",
        "            energy[not_converged] += energy_old(v_, h_) - energy_old(_v, _h)\n",
        "\n",
        "            if fix_v:\n",
        "                converged_ = torch.ones(M, dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                converged_ = torch.all(v_ == _v, 1)\n",
        "                v[not_converged] = v_\n",
        "\n",
        "            for i in range(L):\n",
        "                converged_ = converged_.logical_and(torch.all(h_[i] == _h[i], 1))\n",
        "                h[i][not_converged] = h_[i]\n",
        "\n",
        "            converged[not_converged] = converged_\n",
        "\n",
        "        return energy, v, h\n",
        "L = 3\n",
        "nh = 10\n",
        "nv = 5\n",
        "\n",
        "v = torch.rand(batch_size, nv, requires_grad=True, dtype=torch.float64)\n",
        "h = [torch.randn(batch_size, nh, requires_grad=True, dtype=torch.float64) for _ in range(L)]\n",
        "\n",
        "\n",
        "weight = nn.ParameterList([nn.Parameter(torch.randn(nh, nv, requires_grad=True, dtype=torch.float64))])\n",
        "weight.extend([nn.Parameter(torch.randn(nh, nh, requires_grad=True, dtype=torch.float64)) for _ in range(L-1)])\n",
        "bias = nn.ParameterList([nn.Parameter(torch.randn(nv, requires_grad=True, dtype=torch.float64))])\n",
        "bias.extend([nn.Parameter(torch.randn(nh, requires_grad=True, dtype=torch.float64)) for _ in range(L)])\n",
        "result5 = coupling1(v,h)\n",
        "\n",
        "\n",
        "def coupling2(v, fix_v, *params):\n",
        "    N = v.size(0)\n",
        "    device = v.device\n",
        "    fix_v = fix_v==1\n",
        "    h = params[:L]\n",
        "    weight = params[L:L*2]\n",
        "    bias = params[L*2:]\n",
        "    _v = v.clone()\n",
        "    _h = []\n",
        "    for r in h:\n",
        "      _h.append(r.clone())\n",
        "    params = []\n",
        "    for tensor in h:\n",
        "        params.append(tensor)\n",
        "    for parameter in weight:\n",
        "        params.append(parameter)\n",
        "    for parameter in bias:\n",
        "        params.append(parameter)\n",
        "    energy1 = energy(v, *params)\n",
        "    if fix_v:\n",
        "      converged = torch.ones(N, dtype=torch.bool, device=device)\n",
        "    else:\n",
        "      converged =  torch.all(abs(v-_v)<0.4, 1)\n",
        "    for i in range(L):\n",
        "        converged = converged.logical_and( torch.all(abs(h[i]-_h[i])<0.4, 1))\n",
        "    while not converged.all():\n",
        "        not_converged = converged.logical_not()\n",
        "        _v = v[not_converged]\n",
        "        _h = [h[i][not_converged] for i in range(L)]\n",
        "        M = _v.size(0)\n",
        "        rand_v = None if fix_v else torch.rand_like(_v)\n",
        "        rand_h = [torch.rand_like(_h[i]) for i in range(L)]\n",
        "        rand_u = torch.rand(M, device=device)\n",
        "        params = []\n",
        "        for tensor in _h:\n",
        "            params.append(tensor)\n",
        "        for parameter in weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in bias:\n",
        "            params.append(parameter)\n",
        "        v_, *h_ =  MHStepFunction.apply(_v,fix_v,rand_v,rand_h,rand_u,*params)\n",
        "        for tensor in h_:\n",
        "            params.append(tensor)\n",
        "        for parameter in weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in bias:\n",
        "            params.append(parameter)\n",
        "        aaa = energy(v_, h_)\n",
        "        for tensor in _h:\n",
        "            params.append(tensor)\n",
        "        for parameter in weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in bias:\n",
        "            params.append(parameter)\n",
        "        bbb = energy(_v, _h)\n",
        "        energy1[not_converged] = energy1[not_converged] + (aaa - bbb)\n",
        "        if fix_v:\n",
        "            converged_ = torch.ones(M, dtype=torch.bool, device=device)\n",
        "        else:\n",
        "            converged_ =  torch.all(abs(v_-_v)<0.4, 1)\n",
        "            v = torch.scatter(v,0,not_converged.nonzero().repeat(1,v.shape[1]), v_)\n",
        "        for i in range(L):\n",
        "            converged_ = converged_.logical_and(torch.all(abs(h_[i]-_h[i])<0.4, 1))\n",
        "            h[i] = torch.scatter(h[i], 0, not_converged.nonzero().repeat(1,h_[i].shape[1]), h_[i])\n",
        "        converged[not_converged] = converged_\n",
        "    return energy1\n",
        "\n",
        "params = []\n",
        "for tensor in h:\n",
        "    params.append(tensor)\n",
        "for parameter in weight:\n",
        "    params.append(parameter)\n",
        "for parameter in bias:\n",
        "    params.append(parameter)\n",
        "result6 = coupling2(v,torch.tensor(0.0),*params)\n",
        "\n",
        "test = torch.autograd.gradcheck(coupling2, (v,torch.tensor(0.0),*params))\n",
        "print(\"AUTOGRAD3: \",test)\n",
        "\n",
        "loss6 = torch.ones_like(result6)*100\n",
        "test = autograd.grad(result6, (v,torch.tensor(0.0).requires_grad_(),*params), (loss6), allow_unused=True)\n",
        "print(\"AUTOGRAD4: \",test)\n",
        "\n",
        "\n",
        "\n",
        "mean5, std5 = plot_histograms_and_stats(result5, 'result5')\n",
        "mean6, std6 = plot_histograms_and_stats(result6, 'result6')\n",
        "\n",
        "print(f\"Result5 - Mean: {mean5:.4f}, Std: {std5:.4f}\")\n",
        "print(f\"Result6 - Mean: {mean6:.4f}, Std: {std6:.4f}\")\n",
        "\n",
        "def local_search1(v, h, fix_v=False):\n",
        "        N = v.size(0)\n",
        "        device= v.device\n",
        "\n",
        "        rand_u = torch.rand(N, device=device)\n",
        "        _v, _h = (v, h)\n",
        "\n",
        "        v, h = gibbs_step1(v, h, fix_v, rand_u=rand_u, T=0)\n",
        "\n",
        "        converged = torch.ones(N, dtype=torch.bool, device=device) if fix_v \\\n",
        "                    else torch.all(v == _v, 1)\n",
        "        for i in range(L):\n",
        "            converged = converged.logical_and(torch.all(h[i] == _h[i], 1))\n",
        "\n",
        "        while not converged.all():\n",
        "            not_converged = converged.logical_not()\n",
        "            _v = v[not_converged]\n",
        "            _h = [h[i][not_converged] for i in range(L)]\n",
        "            M = _v.size(0)\n",
        "\n",
        "            v_, h_ = gibbs_step1(_v, _h, fix_v,\n",
        "                                     rand_u=rand_u[not_converged], T=0)\n",
        "\n",
        "            if fix_v:\n",
        "                converged_ = torch.ones(M, dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                converged_ = torch.all(v_ == _v, 1)\n",
        "                v[not_converged] = v_\n",
        "\n",
        "            for i in range(L):\n",
        "                converged_ = converged_.logical_and(torch.all(h_[i] == _h[i], 1))\n",
        "                h[i][not_converged] = h_[i]\n",
        "\n",
        "            converged[not_converged] = converged_\n",
        "\n",
        "        return v, h\n",
        "\n",
        "result7 = local_search1(v,h)\n",
        "\n",
        "def local_search2(v, fix_v, *params):\n",
        "        N = v.size(0)\n",
        "        device= v.device\n",
        "        h = params[:L]\n",
        "        weight = params[L:L*2]\n",
        "        bias = params[L*2:]\n",
        "        fix_v = fix_v==1\n",
        "        _v = v.clone()\n",
        "        _h = []\n",
        "        for r in h:\n",
        "          _h.append(r.clone())\n",
        "        rand_u = torch.rand(N, device=device)\n",
        "        params = []\n",
        "        for tensor in h:\n",
        "            params.append(tensor)\n",
        "        for parameter in weight:\n",
        "            params.append(parameter)\n",
        "        for parameter in bias:\n",
        "            params.append(parameter)\n",
        "\n",
        "        v, h = GibbsStepFunction.apply(v, float(fix_v), dummy,dummy, rand_u, dummy, torch.tensor(0.0), *params)\n",
        "        converged = torch.ones(N, dtype=torch.bool, device=device) if fix_v \\\n",
        "                    else  torch.all(abs(v-_v)<0.4, 1)\n",
        "        for i in range(L):\n",
        "            converged = converged.logical_and( torch.all(abs(h[i]-_h[i])<0.4, 1))\n",
        "        while not converged.all():\n",
        "            not_converged = converged.logical_not()\n",
        "            _v = v[not_converged]\n",
        "            _h = [h[i][not_converged] for i in range(L)]\n",
        "            M = _v.size(0)\n",
        "            params = []\n",
        "            for tensor in _h:\n",
        "                params.append(tensor)\n",
        "            for parameter in weight:\n",
        "                params.append(parameter)\n",
        "            for parameter in bias:\n",
        "                params.append(parameter)\n",
        "\n",
        "            v_, h_ = GibbsStepFunction.apply(_v, float(fix_v), dummy,dummy, rand_u[not_converged], dummy, torch.tensor(0.0), *params)\n",
        "\n",
        "            if fix_v:\n",
        "                converged_ = torch.ones(M, dtype=torch.bool, device=device)\n",
        "            else:\n",
        "                converged_ =  torch.all(abs(v_-_v)<0.4, 1)\n",
        "                v = torch.scatter(v,0,not_converged.nonzero().repeat(1,v.shape[1]), v_)\n",
        "            for i in range(L):\n",
        "                converged_ = converged_.logical_and(torch.all(abs(h_[i]-_h[i])<0.4, 1))\n",
        "                h[i] = torch.scatter(h[i], 0, not_converged.nonzero().repeat(1,h_[i].shape[1]), h_[i])\n",
        "            converged[not_converged] = converged_\n",
        "\n",
        "        return v, *h\n",
        "params = []\n",
        "for tensor in h:\n",
        "    params.append(tensor)\n",
        "for parameter in weight:\n",
        "    params.append(parameter)\n",
        "for parameter in bias:\n",
        "    params.append(parameter)\n",
        "result8 = local_search2(v,torch.tensor(0.0),*params)\n",
        "loss1 = torch.ones_like(v)*100\n",
        "loss2 = [torch.ones_like(item)*100 for item in h]\n",
        "test = autograd.grad(result8, (v,torch.tensor(0.0),*params), (loss1,*loss2), allow_unused=True)\n",
        "print(\"AUTOGRAD5: \",test)\n",
        "test = torch.autograd.gradcheck(local_search2, (v,torch.tensor(0.0),*params))\n",
        "print(\"AUTOGRAD6: \",test)\n",
        "mean7, std7 = plot_histograms_and_stats(result7, 'result7')\n",
        "mean8, std8 = plot_histograms_and_stats(result8, 'result8')\n",
        "\n",
        "print(f\"Result7 - Mean: {mean7:.4f}, Std: {std7:.4f}\")\n",
        "print(f\"Result8 - Mean: {mean8:.4f}, Std: {std8:.4f}\")"
      ],
      "metadata": {
        "id": "eSg8Anh8U0xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BernoulliApproximator(nn.Module):\n",
        "  def __init__(self, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(2, hidden_dim)\n",
        "    self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.relu(self.linear1(x))\n",
        "    out = self.relu(self.linear2(out))\n",
        "    out = self.relu(self.linear3(out))\n",
        "    out = torch.sigmoid(self.linear4(out))\n",
        "    return out\n",
        "\n",
        "model = torch.load('bernoullimodel9.pth',map_location=device)\n",
        "parameters_saved = []\n",
        "for param in model.parameters():\n",
        "   parameters_saved.append(param.data)\n",
        "\n",
        "\n",
        "approximator = BernoulliApproximator(hidden_dim=32)\n",
        "optimizer = torch.optim.Adam(approximator.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()  # or another suitable loss\n",
        "targets = []\n",
        "for saved_param in parameters_saved:\n",
        "        target_tensor = saved_param.clone().detach()\n",
        "        targets.append(target_tensor)\n",
        "import time\n",
        "start_time = time.time()\n",
        "count=0\n",
        "# Training loop\n",
        "for epoch in range(10000):  # Adjust num_epochs\n",
        "    optimizer.zero_grad()\n",
        "    dummy_input = torch.rand(1, 2)  # Adjust batch size if needed\n",
        "    output = approximator(dummy_input)\n",
        "\n",
        "    # Calculate loss based on parameter differences\n",
        "    total_loss = 0\n",
        "    for i, (approx_param, target_param) in enumerate(zip(approximator.parameters(), targets)):\n",
        "        loss = loss_fn(approx_param, target_param)\n",
        "        total_loss += loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:  # Logging for monitoring\n",
        "        val_input = torch.rand(10000, 2)\n",
        "        with torch.no_grad():\n",
        "            val_target = model(val_input)\n",
        "            val_output = approximator(val_input)\n",
        "            val_loss = loss_fn(val_output, val_target)\n",
        "        end_time = time.time()\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss.item()}, Validation Loss: {val_loss.item()}, time {end_time-start_time}\")\n",
        "\n",
        "        if val_loss.item()<0.00001:\n",
        "          count+=1\n",
        "        if count>=3:\n",
        "          print(\"TIME: \",end_time-start_time)\n",
        "          break\n",
        "\n",
        "print(\"TRAINED\")\n"
      ],
      "metadata": {
        "id": "xc694JFxNgN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"v = torch.rand(batch_size, nv, requires_grad=True, dtype=torch.float64)\n",
        "\n",
        "h = [torch.randn(batch_size, nh, requires_grad=True, dtype=torch.float64) for _ in range(L)]\n",
        "\n",
        "\n",
        "def energy_old(v, h):\n",
        "\n",
        "        energy = - torch.sum(v * bias[0].unsqueeze(0), 1)\n",
        "\n",
        "\n",
        "        for i in range(L):\n",
        "\n",
        "            logits = F.linear(v.double() if i==0 else h[i-1].double(),\n",
        "\n",
        "                              weight[i].double(), bias[i+1].double())\n",
        "\n",
        "\n",
        "            energy -= torch.sum(h[i] * logits, 1)\n",
        "\n",
        "\n",
        "        return energy\"\n",
        "\n",
        "\n",
        "We want to make a inverse energy approximator network that given an input energy value, provides an approximation of the values for the v tensor, and each of the h tensors.\n",
        "\n",
        "\n",
        "\n",
        "for the inverse energy function, the approximator network will take in 1 input node, and will ultimately output (num total nodes across all layers (visible and hidden) + all weight values + all bias values), with each output node corresponding to a position in one of the layers. It will be a standard feedforward network like the above approximator.\n",
        "\n",
        "THE CRUCIAL PART: TO CREATE THE TRAINING DATA NEED TO RECORD HISTORIES OF WEIGHT/BIAS /hidden+visible node values."
      ],
      "metadata": {
        "id": "ewLkHUoT89Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "def generate_data(x):\n",
        "    num_configurations = 20\n",
        "\n",
        "    # Generate the continuous number sequence\n",
        "    configurations = np.arange(num_configurations)\n",
        "\n",
        "    # Threshold based on x\n",
        "    threshold = int(x * num_configurations)\n",
        "    y_values = (configurations < threshold).astype(int)\n",
        "\n",
        "    # Combine input x, configurations, and output y\n",
        "    data = np.concatenate((np.tile(x, (num_configurations, 1)),\n",
        "                           configurations[:, np.newaxis],  # Reshape for concatenation\n",
        "                           y_values[:, np.newaxis]), axis=1)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "x = 0.08\n",
        "data = generate_data(x)\n",
        "print(data)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "class BernoulliApproximator(nn.Module):\n",
        "  def __init__(self, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(2, hidden_dim)\n",
        "    self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.x=None\n",
        "    self.out1 = None\n",
        "    self.out2 = None\n",
        "    self.out3 = None\n",
        "    self.out4 = None\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.x=x\n",
        "    out = self.relu(self.linear1(x))\n",
        "    self.out1 = out\n",
        "    out = self.relu(self.linear2(out))\n",
        "    self.out2 = out\n",
        "    out = self.relu(self.linear3(out))\n",
        "    self.out3 = out\n",
        "    out = torch.sigmoid(self.linear4(out))\n",
        "    self.out4 = out\n",
        "    return out\n",
        "\n",
        "probs = torch.rand(10000, 1)\n",
        "data_list = []\n",
        "for p in probs.flatten():  # Iterate over probabilities\n",
        "    data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "probs = torch.rand(5000, 1)*0.1\n",
        "for p in probs.flatten():  # Iterate over probabilities\n",
        "    data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "probs = 0.9 * torch.rand(5000, 1)*0.1\n",
        "for p in probs.flatten():  # Iterate over probabilities\n",
        "    data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "data = np.vstack(data_list)  # Combine the data from all probabilities\n",
        "np.random.shuffle(data)\n",
        "# Create X and Y\n",
        "X = torch.tensor(data[:, :2], dtype=torch.float32)  # Input (x and binary variables)\n",
        "Y = torch.tensor(data[:, 2], dtype=torch.float32)  # Output (y)\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = BernoulliApproximator(hidden_dim=32).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "id": "-X-Vcyzh0O3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000000):\n",
        "  if epoch % 10000==0:\n",
        "    probs = torch.rand(1000, 1)\n",
        "    data_list = []\n",
        "    for p in probs.flatten():  # Iterate over probabilities\n",
        "        data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "    probs = torch.rand(500, 1)*0.1\n",
        "    for p in probs.flatten():  # Iterate over probabilities\n",
        "        data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "    probs = 0.9 * torch.rand(500, 1)*0.1\n",
        "    for p in probs.flatten():  # Iterate over probabilities\n",
        "        data_list.append(generate_data(p.item()))  # Convert to float for compatibility\n",
        "\n",
        "    data = np.vstack(data_list)  # Combine the data from all probabilities\n",
        "    np.random.shuffle(data)\n",
        "    # Create X and Y\n",
        "    X = torch.tensor(data[:, :2], dtype=torch.float32).to(device)  # Input (x and binary variables)\n",
        "    Y = torch.tensor(data[:, 2], dtype=torch.float32).to(device)  # Output (y)\n",
        "    total_loss = 0\n",
        "    i=0\n",
        "  i+=1\n",
        "  optimizer.zero_grad()\n",
        "  #  print(X[i])\n",
        "  result = model(X)\n",
        "   # print(result)\n",
        "   # print(Y[i])\n",
        "  loss = nn.BCELoss()(result.squeeze(),Y)\n",
        "  total_loss+=loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 1000 == 0:\n",
        "    print(epoch, ': avg_loss: ',total_loss/i)\n"
      ],
      "metadata": {
        "id": "ymjvVdGjx8Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model(torch.tensor([0.3,])))"
      ],
      "metadata": {
        "id": "G0EmvJhua3K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'bernoullimodel9.pth')"
      ],
      "metadata": {
        "id": "7MBV46DK6irg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0diz5OSz-9s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "def generate_data(num_samples=100,range=100):\n",
        "    a_values = np.random.uniform(low=-range, high=range, size=num_samples)\n",
        "    b_values = np.random.uniform(low=-range, high=range, size=num_samples)\n",
        "    y_values = (a_values > b_values).astype(int)\n",
        "    data = np.concatenate((a_values[:, np.newaxis], b_values[:, np.newaxis], y_values[:, np.newaxis]), axis=1)\n",
        "    return data\n",
        "\n",
        "class ComparatorNetwork(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(2, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.x=None\n",
        "        self.out1 = None\n",
        "        self.out2 = None\n",
        "        self.out3 = None\n",
        "        self.out4 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x=x\n",
        "        out = self.relu(self.linear1(x))\n",
        "        self.out1 = out\n",
        "        out = self.relu(self.linear2(out))\n",
        "        self.out2 = out\n",
        "        out = self.relu(self.linear3(out))\n",
        "        self.out3 = out\n",
        "        out = torch.sigmoid(self.linear4(out))\n",
        "        self.out4 = out\n",
        "        return out\n",
        "\n",
        "\n",
        "# Generate training data\n",
        "data = generate_data()\n",
        "\n",
        "data_list = []\n",
        "data_list.append(generate_data(num_samples=5000))  # Convert to float for compatibility\n",
        "\n",
        "data_list.append(generate_data(num_samples=10000,range=0.5))  # Convert to float for compatibility\n",
        "\n",
        "data_list.append(generate_data(num_samples=5000, range=1))  # Convert to float for compatibility\n",
        "\n",
        "data_list.append(generate_data(num_samples=5000, range=10))\n",
        "data = np.vstack(data_list)\n",
        "np.random.shuffle(data)\n",
        "\n",
        "X = torch.tensor(data[:, :2], dtype=torch.float32)  # Input (a and b)\n",
        "Y = torch.tensor(data[:, 2], dtype=torch.float32)  # Output\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "model2 = ComparatorNetwork(hidden_dim=2).to(device)\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(2000000):\n",
        "  if epoch % 100000 == 0:\n",
        "    total_loss = 0\n",
        "    i=0\n",
        "    data_list = []\n",
        "    data_list.append(generate_data(num_samples=50000))  # Convert to float for compatibility\n",
        "\n",
        "    data_list.append(generate_data(num_samples=100000,range=0.5))  # Convert to float for compatibility\n",
        "\n",
        "    data_list.append(generate_data(num_samples=50000, range=1))  # Convert to float for compatibility\n",
        "\n",
        "    data_list.append(generate_data(num_samples=50000, range=10))\n",
        "\n",
        "    data_list.append(generate_data(num_samples=50000, range=0.1))\n",
        "\n",
        "    data = np.vstack(data_list)\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    X = torch.tensor(data[:, :2], dtype=torch.float32).to(device)  # Input (a and b)\n",
        "    Y = torch.tensor(data[:, 2], dtype=torch.float32).to(device)  # Output\n",
        "  i+=1\n",
        "  optimizer.zero_grad()\n",
        "  #  print(X[i])\n",
        "  result = model2(X)\n",
        "   # print(result)\n",
        "   # print(Y[i])\n",
        "  loss = nn.BCELoss()(result.squeeze(),Y)\n",
        "  total_loss+=loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 10000 == 0:\n",
        "    print(epoch, ': avg_loss: ',total_loss/i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "nv=5\n",
        "hidden_layers = [10,10]\n",
        "L = len(hidden_layers)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def energy(v, h,weight,bias):\n",
        "  energy = - torch.sum(v * bias[0].unsqueeze(0), 1)\n",
        "\n",
        "  for i in range(L):\n",
        "      logits = F.linear(v.double() if i==0 else h[i-1].double(),\n",
        "                        weight[i].double(), bias[i+1].double())\n",
        "\n",
        "      energy -= torch.sum(h[i] * logits, 1)\n",
        "\n",
        "  return energy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "total_nodes = nv+sum(hidden_layers)\n",
        "\n",
        "\n",
        "class InverseEnergyApproximator(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(1, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear4 = nn.Linear(hidden_dim, total_nodes)\n",
        "        self.relu = nn.GELU()\n",
        "        self.out1 = None\n",
        "        self.weight = nn.ParameterList([nn.Parameter(torch.Tensor(hidden_layers[0], nv)).to(device)])\n",
        "        for i in range(len(hidden_layers)-1):\n",
        "          self.weight.append(nn.Parameter(torch.Tensor(hidden_layers[i+1], hidden_layers[i]).to(device)))\n",
        "        self.bias = nn.ParameterList([nn.Parameter(torch.Tensor(nv)).to(device)])\n",
        "        for i in range(len(hidden_layers)):\n",
        "          self.bias.append(nn.Parameter(torch.Tensor(hidden_layers[i])).to(device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.linear1(x))\n",
        "        out = self.relu(self.linear2(out))\n",
        "        out = self.relu(self.linear3(out))\n",
        "        out = torch.sigmoid(self.linear4(out))\n",
        "        self.out1 = out\n",
        "        v = out[:,:nv]\n",
        "        out = out[:,nv:]\n",
        "        h = []\n",
        "        for l in hidden_layers:\n",
        "          h.append(out[:,:l])\n",
        "          out=out[:,l:]\n",
        "        energy1 = energy(v,h,self.weight,self.bias)\n",
        "        return energy1\n",
        "\n",
        "\n",
        "def generate_data(weight, bias, batch_size = 40000):\n",
        "    states = torch.randint(0,2,(batch_size,nv+sum(hidden_layers))).float().requires_grad_().to(device)\n",
        "    states2 = states.clone()\n",
        "    v = states[:,:nv]\n",
        "    states = states[:,nv:]\n",
        "    h = []\n",
        "    for l in hidden_layers:\n",
        "      h.append(states[:,:l])\n",
        "      states=states[:,l:]\n",
        "    output = energy(v,h,weight,bias).unsqueeze(1)\n",
        "    X = output\n",
        "    return X\n",
        "\n",
        "\n",
        "invEnergyModel = InverseEnergyApproximator(hidden_dim=512).to(device)\n",
        "optimizer = torch.optim.Adam(invEnergyModel.parameters(), lr=0.0001)\n",
        "X=torch.tensor(1.0)\n",
        "total_loss = 0\n",
        "i=0\n",
        "for epoch in range(2000000):\n",
        "  X = generate_data(invEnergyModel.weight,invEnergyModel.bias)\n",
        "  X = X.clone().detach().requires_grad_().to(device)\n",
        "  i+=1\n",
        "  optimizer.zero_grad()\n",
        "  result = invEnergyModel(X)\n",
        "  max1 = max(result.squeeze())/1000\n",
        "  result = result/max1\n",
        "  X = X/max1\n",
        "#  print(\"result: \",result)\n",
        " # print(\"X: \",X)\n",
        "  loss = nn.MSELoss()(result,X)\n",
        "  total_loss+=loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  del X\n",
        "  if epoch % 100 == 0:\n",
        "    print(epoch, ': avg_loss: ',total_loss/i)"
      ],
      "metadata": {
        "id": "g1Iy81cCC16R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow.keras"
      ],
      "metadata": {
        "id": "7Hibq6T5tI_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()  # Default distribution strategy\n",
        "\n",
        "# --- Constants & Hyperparameters ---\n",
        "nv = 5\n",
        "hidden_layers = [10, 10]\n",
        "L = len(hidden_layers)\n",
        "batch_size = 8192\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def energy(v, h, weight, bias):\n",
        "    energy = -tf.reduce_sum(v * bias[0][tf.newaxis, :], axis=1)\n",
        "\n",
        "    for i in range(L):\n",
        "        logits = tf.matmul(\n",
        "            tf.cast(v if i == 0 else h[i - 1], tf.float32),\n",
        "            tf.cast(weight[i], tf.float32)\n",
        "        ) + tf.cast(bias[i + 1], tf.float32)\n",
        "\n",
        "        energy -= tf.reduce_sum(h[i] * logits, axis=1)\n",
        "\n",
        "    return energy\n",
        "\n",
        "\n",
        "total_nodes = nv + sum(hidden_layers)\n",
        "\n",
        "\n",
        "# --- Model ---\n",
        "def create_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(1,),dtype=tf.float32),\n",
        "        layers.Dense(1024, activation='gelu',dtype=tf.float32),\n",
        "        layers.Dense(1024, activation='gelu',dtype=tf.float32),\n",
        "        layers.Dense(1024, activation='gelu',dtype=tf.float32),\n",
        "        layers.Dense(total_nodes, activation='sigmoid',dtype=tf.float32)\n",
        "    ])\n",
        "    # Create Parameter Variables\n",
        "    weights1 = [tf.Variable(tf.random.normal([nv, hidden_layers[0]]), trainable=True)]\n",
        "    for i in range(len(hidden_layers)-1):\n",
        "          weights1.append(tf.Variable(tf.random.normal([hidden_layers[i], hidden_layers[i+1]]), trainable=True))\n",
        "    biases = [tf.Variable(tf.random.normal([nv]), trainable=True)]\n",
        "    for i in range(len(hidden_layers)):\n",
        "          biases.append(tf.Variable(tf.random.normal([hidden_layers[i]]), trainable=True))\n",
        "\n",
        "    def custom_forward(inputs, training=None):\n",
        "        out = model(x)  # Pass input through the Keras model (up to sigmoid)\n",
        "\n",
        "        # Extract v and h (similar to PyTorch logic)\n",
        "        v = out[:, :nv]\n",
        "        h = tf.split(out[:, nv:], hidden_layers, axis=1)\n",
        "\n",
        "        # Call energy function\n",
        "        energy1 = model.energy(v, h, model.weights1, model.biases)\n",
        "\n",
        "        return out, energy1  # Return both the output and energy\n",
        "\n",
        "    model.call = custom_forward  # Replace model's call method\n",
        "    model.energy = energy       # Assign energy function\n",
        "    model.weights1 = weights1  # Assign weights\n",
        "    model.biases = biases\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- Dataset Generation on TPU ---\n",
        "def generate_data(model, batch_size=8192):\n",
        "    # TPU-compatible data generation using tf.random.uniform\n",
        "    states = tf.random.uniform((batch_size, total_nodes), 0, 2, dtype=tf.float32)\n",
        "\n",
        "    v = states[:, :nv]\n",
        "    h = tf.split(states[:, nv:], hidden_layers, axis=1)\n",
        "\n",
        "    output = model.energy(v, h, model.weights1, model.biases)[:, tf.newaxis]\n",
        "    return output\n",
        "\n",
        "# --- Training (Within TPU Strategy Scope) ---\n",
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "    optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "\n",
        "    for epoch in range(2000000):\n",
        "        X = generate_data(model)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            result = model.call(X)\n",
        "            max1 = tf.reduce_max(result) / 1000.0  # For scaling\n",
        "            result = result / max1\n",
        "            X = X / max1\n",
        "            loss = tf.keras.losses.MSE(result, X)\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables + model.weights + model.biases)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables + model.weights + model.biases))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss {loss.numpy()}\")\n"
      ],
      "metadata": {
        "id": "ozCFj-rWs8wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2,'greater_than4.pth')"
      ],
      "metadata": {
        "id": "OqCWDRYqZYBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000000):\n",
        "  total_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  #  print(X[i])\n",
        "  result = model2(X)\n",
        "   # print(result)\n",
        "   # print(Y[i])\n",
        "  loss = nn.BCELoss()(result.squeeze(),Y)\n",
        "  total_loss+=loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 10000==0:\n",
        "    print(epoch, ': avg_loss: ',total_loss/len(X))\n",
        "\n",
        "\n",
        "torch.save(model2,'greater_than2.pth')"
      ],
      "metadata": {
        "id": "auWTnGV3C7QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2,'greater_than2.pth')"
      ],
      "metadata": {
        "id": "SLUjfruse-yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc-cLCK9_Bk_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!cp 'dbn_modelv2_deepConv.pt' /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve3egHwfzPUA"
      },
      "source": [
        "Combined VAE representation - VAE encoder for continous variables, other type of encoder for categorical variables, combined into decoder.\n",
        "\n",
        "Working with this data I see why tabular data is much harder. Training a VAE on this is much more difficult than other times I've done it. Right now I've split the data into 2, categorical and continous but I'm going to have to split them into groupings of dependencies between variables.\n",
        "\n",
        "OPTICS is clustering between datapoints but we want clustering of dependencies between variables. Even then there will be large groups of disparate variables.\n",
        "\n",
        "\n",
        "CLUSTER ACCORDING TO COMBINED ENCODED LATENT SPACE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9BxN5PUWoSW"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # Provides additional layers and functions\n",
        "\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "\n",
        "\n",
        "def train_vae(num_vae, dataloader, num_epochs, optimizer_num, device):\n",
        "    num_vae.train()\n",
        "    # Train Numerical VAE\n",
        "    for epoch in range(num_epochs):\n",
        "      num_vae.train()\n",
        "      total_loss_num = 0\n",
        "      for batch_num,_ in dataloader:  # Only iterate over numerical data\n",
        "        batch_num = batch_num.to(device)\n",
        "        optimizer_num.zero_grad()\n",
        "        recon_num, mu_num, logvar_num = num_vae(batch_num)\n",
        "        loss_num = F.mse_loss(recon_num, batch_num) + \\\n",
        "                       -0.5 * torch.sum(1 + logvar_num - mu_num.pow(2) - logvar_num.exp())\n",
        "        loss_num.backward()\n",
        "        optimizer_num.step()\n",
        "        total_loss_num += loss_num.item()\n",
        "      avg_loss_num = total_loss_num / len(dataloader)\n",
        "      print(f'Epoch {epoch + 1}: Num. VAE Loss - {avg_loss_num:.4f}')\n",
        "\n",
        "\n",
        "#--- Using the loop ---\n",
        "\n",
        "# Instantiate your VAE model\n",
        "num_vae = VAE(input_size=X_num.shape[1], hidden_size=512, latent_size=16).to(device)\n",
        "\n",
        "# Optimizers for each VAE\n",
        "optimizer_num = optim.Adam(num_vae.parameters(), lr=1e-3)\n",
        "\n",
        "# Train!\n",
        "train_vae(num_vae, dataloader, num_epochs=200, optimizer_num=optimizer_num, optimizer_cat=optimizer_cat, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xVV73zMfglg"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def run_optics_and_visualize(data):\n",
        "    # Perform OPTICS clustering using the provided metric\n",
        "    optics = OPTICS(min_samples=20)\n",
        "    optics.fit(data)\n",
        "    # Get cluster labels\n",
        "    cluster_labels = optics.labels_\n",
        "\n",
        "    # Get reachability distances (useful for understanding cluster structure)\n",
        "    reachability_distances = optics.reachability_\n",
        "    # Create a dictionary for mapping cluster labels to colors\n",
        "    color_map = plt.cm.get_cmap('tab10', max(cluster_labels) + 1)  # Adjust colormap as needed\n",
        "    colors = [color_map(label) for label in cluster_labels]\n",
        "\n",
        "    # Reduce dimensionality to 2D using PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    data_reduced = pca.fit_transform(data)\n",
        "\n",
        "\n",
        "    # Plot the reduced data with cluster labels as colors\n",
        "    plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=cluster_labels)\n",
        "    plt.title('OPTICS Clusters (PCA Visualization)')\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.show()\n",
        "\n",
        "    tsne = TSNE(n_components=2)\n",
        "    data_reduced = tsne.fit_transform(data)\n",
        "\n",
        "    # Plot the reduced data with cluster labels as colors\n",
        "    plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=cluster_labels)\n",
        "    plt.title('OPTICS Clusters (t-SNE Visualization)')\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhsENycGJX9x"
      },
      "outputs": [],
      "source": [
        "run_optics_and_visualize(np.concatenate((X_num, X_cat), axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqc99903JgRJ"
      },
      "outputs": [],
      "source": [
        "run_optics_and_visualize(X_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6SX9WqCJg_q"
      },
      "outputs": [],
      "source": [
        "run_optics_and_visualize(X_cat)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}