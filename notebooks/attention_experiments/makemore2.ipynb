{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:00.586494Z",
     "start_time": "2024-04-22T22:24:59.388252Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167739c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:00.594309Z",
     "start_time": "2024-04-22T22:25:00.587127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['en', 'anastasia', 'kayla', 'alyssa', 'juliana']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = open(\"names.txt\",\"r\").read().splitlines()\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87827397",
   "metadata": {
    "lines_to_next_cell": 2,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:00.598506Z",
     "start_time": "2024-04-22T22:25:00.595815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: '<', 27: '>'}\n"
     ]
    }
   ],
   "source": [
    "# build a lookup table for character vocabulary\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "# special tokens < start 26 and > end 27\n",
    "stoi[\"<\"] = 26\n",
    "stoi[\">\"] = 27\n",
    "# now reverse mapping . we will need this to decode and predict\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40e1d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:00.896429Z",
     "start_time": "2024-04-22T22:25:00.739269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0] 26\n",
      "[0, 0, 26] 4\n",
      "[0, 26, 4] 13\n",
      "[26, 4, 13] 27\n",
      "[0, 0, 0] 26\n",
      "[0, 0, 26] 0\n",
      "[0, 26, 0] 13\n",
      "[26, 0, 13] 0\n",
      "[0, 13, 0] 18\n",
      "[13, 0, 18] 19\n",
      "[0, 18, 19] 0\n",
      "[18, 19, 0] 18\n",
      "[19, 0, 18] 8\n",
      "[0, 18, 8] 0\n",
      "[18, 8, 0] 27\n",
      "[0, 0, 0] 26\n",
      "[0, 0, 26] 10\n",
      "[0, 26, 10] 0\n",
      "[26, 10, 0] 24\n",
      "[10, 0, 24] 11\n",
      "[0, 24, 11] 0\n",
      "[24, 11, 0] 27\n"
     ]
    }
   ],
   "source": [
    "# the first thing we have to do is to compile the dataset for the network\n",
    "block_size = 3 # context length : how many chars we would use to predict the next char.\n",
    "\n",
    "X,y = [],[]\n",
    "\n",
    "for n, w in enumerate(names):\n",
    "    context = [0]*block_size\n",
    "    w = \"<\" + w + \">\"\n",
    "    for c in w:\n",
    "        ix = stoi[c]\n",
    "        X.append(context)\n",
    "        y.append(ix)\n",
    "        if n < 3: # show 3 examples\n",
    "            print(context,ix)\n",
    "        context = context[1:] + [ix]\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "# lets just use the first 30 examples first\n",
    "sa_X = X[:5]\n",
    "sa_y = y[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80bb62f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:00.900759Z",
     "start_time": "2024-04-22T22:25:00.895502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([260179, 3]), torch.int64, torch.Size([260179]), torch.int64)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataset looks like as follows\n",
    "X.shape , X.dtype , y.shape , y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([5, 3]), torch.int64, torch.Size([5]), torch.int64)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and our sample dataset\n",
    "sa_X.shape , sa_X.dtype , sa_y.shape , sa_y.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:00.902323Z",
     "start_time": "2024-04-22T22:25:00.899721Z"
    }
   },
   "id": "38d61826de5d91de"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9861ed6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:00.965708Z",
     "start_time": "2024-04-22T22:25:00.903307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x116ffa980>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x800 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# called embedding C in joshua bengio's paper\n",
    "# now our vocabulary size is 28 chars. we can embed this in 2 dimensional space for starters.\n",
    "g=torch.Generator().manual_seed(123)\n",
    "C = torch.randn(28,2,generator=g) # as we are going to right multiply this matrix\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(C,cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3741537e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.000040Z",
     "start_time": "2024-04-22T22:25:00.965007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor([ 0.6984, -1.4097]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so lets say e which is encoded as 4 in our stoi lookup table.\n",
    "# will get a representation in R28 as it would one hot encoded with num_classes=28\n",
    "# but we can just try to squeeze this into 2 dimesions.\n",
    "e_representation_orig= F.one_hot(torch.tensor(5),num_classes=28).float()\n",
    "e_rep = e_representation_orig @ C\n",
    "e_representation_orig  , e_rep# so it just plucks out the 5th row from our C Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a9af7c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.000351Z",
     "start_time": "2024-04-22T22:25:00.971629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.6984, -1.4097])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee8d1211",
   "metadata": {
    "lines_to_next_cell": 0,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.000771Z",
     "start_time": "2024-04-22T22:25:00.976326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0, 26]])\n",
      "tensor([[ 0.3374, -0.1778],\n",
      "        [ 0.1036, -2.1996]])\n"
     ]
    }
   ],
   "source": [
    "# now we are going to go a bit crazy we can actually (lookup) or index our entire training examples all at once.\n",
    "# so for now lets print . what a null character embedding is and what a < token embedding is\n",
    "# remember our first example has no context so all 3 0s and the next example only has < token as context so two 0;s and 26 (<).\n",
    "# remind yourself of the first 2 examples\n",
    "print(X[:2])\n",
    "# print Embedding for those:\n",
    "print(C[[0,26]]) # note that i passed a list . i.e queries give me 0th and26th row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.3374, -0.1778],\n          [ 0.3374, -0.1778],\n          [ 0.3374, -0.1778]],\n \n         [[ 0.3374, -0.1778],\n          [ 0.3374, -0.1778],\n          [ 0.1036, -2.1996]]]),\n torch.Size([2, 3, 2]))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so when we pass in the first 2 example in the C embedding we expect\n",
    "# for the first example we get 3 enteries 0 gets mapped to R2 which is the first row in C. and they are repeated\n",
    "\n",
    "# and for the second example the last representation would be of 26 so we expect it to be the second row from the above output\n",
    "C[X[:2]], C[X[:2]].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.000866Z",
     "start_time": "2024-04-22T22:25:00.983846Z"
    }
   },
   "id": "6c2ce6c296396c4a"
  },
  {
   "cell_type": "markdown",
   "id": "a4045f22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([260179, 3, 2])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so now that we understand we can pass in the entire example set in the lookup\n",
    "C[X].shape # lets see what the shape is going to be like \n",
    "# in our above example we had X of shape 2,3 . 2 examples and 3 context .\n",
    "# when we passed X in through C . we get 2,3,2 . 2,3 examples with each having a R2 representation.. therefore 2,3 and then ,2 the last 2 is the representation dimension . if we change C from 28,2 to 28,5 we would get 2,3,5.\n",
    "\n",
    "# or if you want to read it as we have 39 examples as before and now the example has a representation of 3,2.\n",
    "# and the representation is 3,2 because we have 3 context and each get a R2 representation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.000935Z",
     "start_time": "2024-04-22T22:25:00.989785Z"
    }
   },
   "id": "f0b07a942c0b3386"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "emb = C[X]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.001562Z",
     "start_time": "2024-04-22T22:25:00.995133Z"
    }
   },
   "id": "96f1dc4315d50fad"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# so now lets construct our hidden layer\n",
    "W1 = torch.randn(6,100) # again like the cell above it is important to understand why it is 6.\n",
    "# it is 6 because each example can be represented in R6 . if we flatten out each example we get 39,6\n",
    "\n",
    "# so the intermediate representation is going to push from R6 -> R100 . (we are going to right multiply this W)\n",
    "# an now to add bias .\n",
    "b1 = torch.randn(100) # 100 because each column of the weight matrix is allowed to get affine transformed\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.011590Z",
     "start_time": "2024-04-22T22:25:01.003429Z"
    }
   },
   "id": "5b97a14cdf4f5387"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1889, -2.0949,  0.0823],\n",
      "        [-0.6387, -0.4974,  0.1412]])\n",
      "tensor([[-0.1889, -2.0949,  0.0823],\n",
      "        [-0.6387, -0.4974,  0.1412],\n",
      "        [-0.1889, -2.0949,  0.0823],\n",
      "        [-0.6387, -0.4974,  0.1412],\n",
      "        [-0.1889, -2.0949,  0.0823],\n",
      "        [-0.6387, -0.4974,  0.1412]])\n",
      "tensor([[-0.1889, -2.0949,  0.0823, -0.1889, -2.0949,  0.0823, -0.1889, -2.0949,\n",
      "          0.0823],\n",
      "        [-0.6387, -0.4974,  0.1412, -0.6387, -0.4974,  0.1412, -0.6387, -0.4974,\n",
      "          0.1412]])\n"
     ]
    }
   ],
   "source": [
    "# but lets see how we deal with 32,3,2\n",
    "# we will use the function torch.cat and torch.unbind to deal with this\n",
    "\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(torch.cat((x, x, x), 0)) # vstack\n",
    "print(torch.cat((x, x, x), 1)) # hstack"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.012815Z",
     "start_time": "2024-04-22T22:25:01.007290Z"
    }
   },
   "id": "7f445b9b7caa0c3a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1, 4, 7]), tensor([2, 5, 8]), tensor([3, 6, 9]))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unbind\n",
    "# so it will pluck and return everytime it sees a new row\n",
    "torch.unbind(torch.tensor([[1, 2, 3],\n",
    "                           [4, 5, 6],\n",
    "                           [7, 8, 9]]),1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.115915Z",
     "start_time": "2024-04-22T22:25:01.013173Z"
    }
   },
   "id": "3f34b6f9c25f2bd"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.3374, -0.1778],\n        [ 0.3374, -0.1778],\n        [ 0.3374, -0.1778],\n        ...,\n        [ 0.4965, -1.5723],\n        [ 0.7671, -1.1925],\n        [-2.1338,  1.0524]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so the stratergy is that we want to unbind and hstack them on individual example\n",
    "\n",
    "emb[:,0,:] # give me all the examples , the first context in R2\n",
    "# similarly we can query for the rest of the 2 context and stack them"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.125668Z",
     "start_time": "2024-04-22T22:25:01.019328Z"
    }
   },
   "id": "e4961b6996ec1824"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.3374, -0.1778,  0.3374, -0.1778,  0.3374, -0.1778],\n        [ 0.3374, -0.1778,  0.3374, -0.1778,  0.1036, -2.1996],\n        [ 0.3374, -0.1778,  0.1036, -2.1996,  0.7671, -1.1925],\n        ...,\n        [ 0.4965, -1.5723,  0.7671, -1.1925, -2.1338,  1.0524],\n        [ 0.7671, -1.1925, -2.1338,  1.0524, -0.3885, -0.9343],\n        [-2.1338,  1.0524, -0.3885, -0.9343,  0.6177, -0.2876]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat( [emb[:,0,:] , emb[:,1,:] , emb[:,2,:] ] , axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.125983Z",
     "start_time": "2024-04-22T22:25:01.026127Z"
    }
   },
   "id": "26cc4f9607e38bc"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.3374, -0.1778],\n         [ 0.3374, -0.1778],\n         [ 0.3374, -0.1778],\n         ...,\n         [ 0.4965, -1.5723],\n         [ 0.7671, -1.1925],\n         [-2.1338,  1.0524]]),\n tensor([[ 0.3374, -0.1778],\n         [ 0.3374, -0.1778],\n         [ 0.1036, -2.1996],\n         ...,\n         [ 0.7671, -1.1925],\n         [-2.1338,  1.0524],\n         [-0.3885, -0.9343]]),\n tensor([[ 0.3374, -0.1778],\n         [ 0.1036, -2.1996],\n         [ 0.7671, -1.1925],\n         ...,\n         [-2.1338,  1.0524],\n         [-0.3885, -0.9343],\n         [ 0.6177, -0.2876]]))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets use unbind \n",
    "torch.unbind(emb,1) # that is we get a tuple of 3 . the first has the first 2 col the second has the next 2 and third has the last 2 col\n",
    "# it splits in 2 columns each because we gave axis as 1 . and 1 index was for context. 3 context so 3 tuples returned\n",
    "# so now we can just hstack these 3 tuples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.126193Z",
     "start_time": "2024-04-22T22:25:01.033341Z"
    }
   },
   "id": "ce77a6cfa41bbef3"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.3374, -0.1778,  0.3374, -0.1778,  0.3374, -0.1778],\n        [ 0.3374, -0.1778,  0.3374, -0.1778,  0.1036, -2.1996],\n        [ 0.3374, -0.1778,  0.1036, -2.1996,  0.7671, -1.1925],\n        ...,\n        [ 0.4965, -1.5723,  0.7671, -1.1925, -2.1338,  1.0524],\n        [ 0.7671, -1.1925, -2.1338,  1.0524, -0.3885, -0.9343],\n        [-2.1338,  1.0524, -0.3885, -0.9343,  0.6177, -0.2876]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat( torch.unbind(emb,1),1) # hstack them ."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.126407Z",
     "start_time": "2024-04-22T22:25:01.040533Z"
    }
   },
   "id": "6fb5f157ed3e7a7d"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.3374, -0.1778,  0.3374, -0.1778,  0.3374, -0.1778],\n        [ 0.3374, -0.1778,  0.3374, -0.1778,  0.1036, -2.1996],\n        [ 0.3374, -0.1778,  0.1036, -2.1996,  0.7671, -1.1925],\n        ...,\n        [ 0.4965, -1.5723,  0.7671, -1.1925, -2.1338,  1.0524],\n        [ 0.7671, -1.1925, -2.1338,  1.0524, -0.3885, -0.9343],\n        [-2.1338,  1.0524, -0.3885, -0.9343,  0.6177, -0.2876]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an even better way .. but understand the above very very clearly\n",
    "# view.. view is extremely efficient. cat takes whole new storage . view does not\n",
    "emb.view(emb.shape[0],6)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.126593Z",
     "start_time": "2024-04-22T22:25:01.045300Z"
    }
   },
   "id": "7078a263a6359eee"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([260179, 100])\n",
      "torch.Size([260179, 100])\n"
     ]
    }
   ],
   "source": [
    "# so now we can start pasing it through\n",
    "h1= emb.view(emb.shape[0],6) @ W1 + b1\n",
    "print(h1.shape)\n",
    "a1 = torch.tanh(h1) # element wise operation\n",
    "print(a1.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.128401Z",
     "start_time": "2024-04-22T22:25:01.051602Z"
    }
   },
   "id": "1d9010a9e89750c8"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([6, 100]), torch.Size([100]))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but lets just double check how it was broadcasted\n",
    "W1.shape , b1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.128569Z",
     "start_time": "2024-04-22T22:25:01.111742Z"
    }
   },
   "id": "977da277394b01e8"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 39,100\n",
    "#   100 -> first you right align them\n",
    "\n",
    "# the axis to be broadcasted needs to be 1 or empty\n",
    "# here its empty\n",
    "\n",
    "# that is to broadcast them we would vstack bias 39 times to make it 39,100\n",
    "# that is the same row is being appended .. each example get the same bias .\n",
    "# so its ok!"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.138438Z",
     "start_time": "2024-04-22T22:25:01.116113Z"
    }
   },
   "id": "809e601dfc675e82"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "W2 = torch.randn(100,28)\n",
    "b2 = torch.randn(28)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.157607Z",
     "start_time": "2024-04-22T22:25:01.121457Z"
    }
   },
   "id": "5484b48645c10621"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "logits = h1 @ W2 + b2 # note the final is a linear layer . most of the time it is\n",
    "count = logits.exp() # so now its all positive and we can interpret them as counts \n",
    "prob = count/count.sum(axis=1,keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.234218Z",
     "start_time": "2024-04-22T22:25:01.130483Z"
    }
   },
   "id": "b7e0a4874e418c2a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([260179, 28])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.259984Z",
     "start_time": "2024-04-22T22:25:01.160863Z"
    }
   },
   "id": "a572adfa278fe2a3"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.261380Z",
     "start_time": "2024-04-22T22:25:01.170404Z"
    }
   },
   "id": "aeee56031f5997d3"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([4.1152e-06, 2.3604e-17, 2.5049e-40,  ..., 7.6959e-22, 2.0009e-37,\n        0.0000e+00])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilty of right guess for the examples\n",
    "prob[torch.arange(emb.shape[0]),y]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.267746Z",
     "start_time": "2024-04-22T22:25:01.177139Z"
    }
   },
   "id": "30056b19041bb594"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-inf)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.tensor(0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.268132Z",
     "start_time": "2024-04-22T22:25:01.186559Z"
    }
   },
   "id": "97fdc5bd4cc14620"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(44.3379)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make negative log likelihood\n",
    "-1 * prob[torch.arange(emb.shape[0]),y].log().mean()\n",
    "# -1 * prob[torch.arange(5),y[:5]].log().mean()\n",
    "# inf is unfortunate 😬\n",
    "F.cross_entropy(logits,y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.268445Z",
     "start_time": "2024-04-22T22:25:01.192320Z"
    }
   },
   "id": "612fbf0e7cebd36"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# lets just make all of this a bit respeectable\n",
    "g = torch.Generator().manual_seed(123)\n",
    "C = torch.randn(28,2,generator=g)\n",
    "W1 = torch.randn(6,100)\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn(100,28)\n",
    "b2 = torch.randn(28)\n",
    "parameters = [C, W1, W2, b1, b2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.268511Z",
     "start_time": "2024-04-22T22:25:01.204973Z"
    }
   },
   "id": "3db1d0d4bd0c20ee"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "3584"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.268709Z",
     "start_time": "2024-04-22T22:25:01.209423Z"
    }
   },
   "id": "72addf76839d0034"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:25:01.268746Z",
     "start_time": "2024-04-22T22:25:01.212894Z"
    }
   },
   "id": "a88a626317b723d7"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "emb = C[X]\n",
    "h = torch.tanh( emb.view(emb.shape[0],6) @ W1 + b1)\n",
    "logit = h @ W2 + b2\n",
    "# count = logits.exp()\n",
    "# prob = count/count.sum(1,keepdim=True)\n",
    "# loss = -1 * prob[torch.arange(emb.shape[0]),y].log().mean()\n",
    "loss = F.cross_entropy(logit,y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:27:50.499563Z",
     "start_time": "2024-04-22T22:27:50.422060Z"
    }
   },
   "id": "9ee81db0f53630bd"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(7.8013, grad_fn=<NllLossBackward0>)"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:27:50.849928Z",
     "start_time": "2024-04-22T22:27:50.844398Z"
    }
   },
   "id": "80223b17d1b39a31"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.grad = None # flush\n",
    "loss.backward()\n",
    "for p in parameters:\n",
    "    p.data = p.data - (0.1 * p.grad )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:27:51.538626Z",
     "start_time": "2024-04-22T22:27:51.377907Z"
    }
   },
   "id": "50e67cdf63e5a086"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b2f0b597e9e301be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
